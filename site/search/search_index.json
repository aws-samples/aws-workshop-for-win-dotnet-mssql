{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Windows on AWS Workshops Welcome to the AWS Windows workshops portal! Here you will find a collection of workshops and other hands-on content aimed at helping you gain an understanding of the AWS service ecosystem supporting Windows, SQL, and .Net workloads, and introduce you to a variety of best practices that can be applied to operating your Windows environments on AWS. The workshops and other hands-on content contained in this portal are prepared scenarios that represent common use cases and operational tasks on Amazon Web Services (AWS). The workshops will provide a deep dive into a variety of AWS services in support of running, operating, and modernizing Windows workloads on AWS. Regardless of the workloads you run on AWS, Security and Compliance is a shared responsibility between AWS and the customer. AWS is responsible for protecting the infrastructure which runs all of the services offered and this responsibility is known as the Security of the Cloud . AWS customers benefit from a data center and network architecture built to satisfy the requirements of the most security-sensitive organizations. Customers responsibility, known as the Security in the Cloud , is determined by which services the customer chooses to use. License Summary This sample code is made available under the MIT-0 license. See the LICENSE file.","title":"Home"},{"location":"#license-summary","text":"This sample code is made available under the MIT-0 license. See the LICENSE file.","title":"License Summary"},{"location":"contribute/","text":"Contributing Guidelines Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Reporting Bugs/Feature Requests We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment Contributing via Pull Requests Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the master branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request . Finding contributions to work on Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels ((enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start. Code of Conduct This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments. Security issue notifications If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue. Licensing See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a Contributor License Agreement (CLA) for larger changes.","title":"Contributing"},{"location":"contribute/#contributing-guidelines","text":"Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.","title":"Contributing Guidelines"},{"location":"contribute/#reporting-bugsfeature-requests","text":"We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment","title":"Reporting Bugs/Feature Requests"},{"location":"contribute/#contributing-via-pull-requests","text":"Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the master branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request .","title":"Contributing via Pull Requests"},{"location":"contribute/#finding-contributions-to-work-on","text":"Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels ((enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.","title":"Finding contributions to work on"},{"location":"contribute/#code-of-conduct","text":"This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.","title":"Code of Conduct"},{"location":"contribute/#security-issue-notifications","text":"If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue.","title":"Security issue notifications"},{"location":"contribute/#licensing","text":"See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a Contributor License Agreement (CLA) for larger changes.","title":"Licensing"},{"location":"getting-started/","text":"Getting Started Create an AWS account In order to complete the hands-on content on this site, you'll need an AWS Account . We strongly recommend that you use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for unless they provide sandbox accounts just for this purpose. Create an IAM user (with admin permissions) If you don't already have an AWS IAM user with admin permissions, please use the following instructions to create one: Browse to the AWS IAM console. Click Users on the left navigation and then click Add User . Enter a User Name , check the checkbox for AWS Management Console access , enter a Custom Password , and click Next:Permissions . Click Attach existing policies directly , click the checkbox next to the AdministratorAccess , and click Next:review . Click Create User Click Dashboard on the left navigation and use the IAM users sign-in link to login as the admin user you just created. Add credits (optional) If you are doing this workshop as part of an AWS sponsored event that provides AWS credits to cover the costs, use the below instructions for entering the credits: Browse to the AWS Account Settings console. Enter the Promo Code you received (these will be handed out at the beginning of the workshop). Enter the Security Check and click Redeem . Click here if the workshop you are doing requires a Cloud9 instance If the workshop you are doing requires you to run commands or scripts you will need to launch a an AWS Cloud9 instance which will provide you with a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. The workshop instructions will specify if this needed. Below are the instructions for launching an instance: Browse to the AWS Cloud9 console. Click Create environment on the right side. Enter a Name (msft-workshop-ide) and click Next step . Leave all the defaults and click Next step . Click Create environment . The environment will open automatically after it has been provisioned. Browse back to the AWS Cloud9 console and you can click Open IDE on the environment you created to access it at anytime. You are now setup for the workshops!","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#create-an-aws-account","text":"In order to complete the hands-on content on this site, you'll need an AWS Account . We strongly recommend that you use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for unless they provide sandbox accounts just for this purpose.","title":"Create an AWS account"},{"location":"getting-started/#create-an-iam-user-with-admin-permissions","text":"If you don't already have an AWS IAM user with admin permissions, please use the following instructions to create one: Browse to the AWS IAM console. Click Users on the left navigation and then click Add User . Enter a User Name , check the checkbox for AWS Management Console access , enter a Custom Password , and click Next:Permissions . Click Attach existing policies directly , click the checkbox next to the AdministratorAccess , and click Next:review . Click Create User Click Dashboard on the left navigation and use the IAM users sign-in link to login as the admin user you just created.","title":"Create an IAM user  (with admin permissions) "},{"location":"getting-started/#add-credits-optional","text":"If you are doing this workshop as part of an AWS sponsored event that provides AWS credits to cover the costs, use the below instructions for entering the credits: Browse to the AWS Account Settings console. Enter the Promo Code you received (these will be handed out at the beginning of the workshop). Enter the Security Check and click Redeem . Click here if the workshop you are doing requires a Cloud9 instance If the workshop you are doing requires you to run commands or scripts you will need to launch a an AWS Cloud9 instance which will provide you with a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. The workshop instructions will specify if this needed. Below are the instructions for launching an instance: Browse to the AWS Cloud9 console. Click Create environment on the right side. Enter a Name (msft-workshop-ide) and click Next step . Leave all the defaults and click Next step . Click Create environment . The environment will open automatically after it has been provisioned. Browse back to the AWS Cloud9 console and you can click Open IDE on the environment you created to access it at anytime. You are now setup for the workshops!","title":"Add credits  (optional) "},{"location":"license/","text":"License MIT License Copyright (c) 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"MIT License Copyright (c) 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"builder-sessions/","text":"Builder Sessions Title Description Migrating a SQL Server Database to Aurora (MySQL) TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Using Visual Studio to Deploy an ASP.NET Web App to AWS with Elastic Beanstalk TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD API Gateway and ASP.NET Web API with Elastic Beanstalk and CloudWatch Logs TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Implement DynamoDB for ASP.NET Core Session State TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Modernize Your First Windows Application with Windows Containers TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Migrating from SQL Server to Aurora using AWS Database Migration Service TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD NET Core Serverless for resize images TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Deploying a .NET Core App with CodeStar TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Using AWS Lambda Layers with .NET Core TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Creating Lambda Layers using .NET TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD CreatingTraceable, Decoupled architecture using AWS Services TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD AWS Federated Authentication with ADFS TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Centralized Log Management for Windows using Amazon Elasticsearch Service and Kibana TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Migrating Azure VMs to AWS using AWS SMS Connector for Azure TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD","title":"Directory"},{"location":"builder-sessions/#builder-sessions","text":"Title Description Migrating a SQL Server Database to Aurora (MySQL) TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Using Visual Studio to Deploy an ASP.NET Web App to AWS with Elastic Beanstalk TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD API Gateway and ASP.NET Web API with Elastic Beanstalk and CloudWatch Logs TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Implement DynamoDB for ASP.NET Core Session State TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Modernize Your First Windows Application with Windows Containers TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Migrating from SQL Server to Aurora using AWS Database Migration Service TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD NET Core Serverless for resize images TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Deploying a .NET Core App with CodeStar TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Using AWS Lambda Layers with .NET Core TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Creating Lambda Layers using .NET TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD CreatingTraceable, Decoupled architecture using AWS Services TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD AWS Federated Authentication with ADFS TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Centralized Log Management for Windows using Amazon Elasticsearch Service and Kibana TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Migrating Azure VMs to AWS using AWS SMS Connector for Azure TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD","title":"Builder Sessions"},{"location":"reinvent2018/index2018/","text":"re:Invent 2018 Title Description Active Directory Domain Trusts and AWS SSO Jumpstart Join us for this builders session where you and a small group work with an AWS subject matter expert to build a domain trust between on-premises Active Directory and AWS Managed AD in AWS. As a bonus, learn how to connect AWS Managed AD to AWS SSO. Level : Intermediate Duration : 1 hour Service : AWS Managed AD & SSO Focus : Trusts and Single Sign On Building a .NET CI/CD Pipeline with AWS & AWS Tools for Visual Studio Team Services You will learn how to target AWS resources from your Azure DevOps build and release pipelines, deployment options for targeting different types of AWS compute services, and tricks and tips for enhancing your CI/CD pipelines. Level : Intermediate Duration : 1 hour Service : EC2, Lambda, ECS Fargate Focus : Deploying from Azure DevOps to EC2, Lambda, or ECS Fargate Hands-On: Automating AWS Infrastructure with Powershell TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Architecting Active Directory in AWS In this workshop, we are going to show you how easy it is to securely implement Active Directory in AWS, as well as discuss the different deployment options. The structure of this Workshop involves showing you two Active Directory patterns commonly observed in the field, discussing their use cases, and then guiding you through implementing them inside a fully functional AWS account. Level : Intermediate Duration : 1 hour Service : Managed Active Directory Focus : Active Directory Migration Modernize Your First Windows Application with Windows Containers Our builder session will begin with a short presentation on how to containerize .NET applications on AWS. You will then have the opportunity to build a Docker container for a sample .NET application and deploy that container/application to Amazon Elastic Container Service (ECS). Level : Intermediate Duration : 1 hour Service : ECS Focus : .NET on ECS Hands-on building a migration strategy for SQL Server on AWS In this workshop you will work in teams to develop a strategy to migrate a SQL Server 2008R2 environment to AWS. Level : Intermediate Duration : 1 hour Service : DMS Focus : SQL Server Migration Storage Spaces Direct for Windows 2016 Failover Cluster TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Migrate your first SQL Server Database to Amazon RDS for SQL Server TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Building a Serverless .Net App on AWS using the AWS toolkit for Visual Studio TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD DEV331 - Hands-On Building and Deploying .NET Applications on AWS TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD AWS Tools for PowerShell In this lab you will learn how to use AWS Tools for Powershell Core to create AWS resources and host a simple HTML page. Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD","title":"Directory"},{"location":"reinvent2018/index2018/#reinvent-2018","text":"Title Description Active Directory Domain Trusts and AWS SSO Jumpstart Join us for this builders session where you and a small group work with an AWS subject matter expert to build a domain trust between on-premises Active Directory and AWS Managed AD in AWS. As a bonus, learn how to connect AWS Managed AD to AWS SSO. Level : Intermediate Duration : 1 hour Service : AWS Managed AD & SSO Focus : Trusts and Single Sign On Building a .NET CI/CD Pipeline with AWS & AWS Tools for Visual Studio Team Services You will learn how to target AWS resources from your Azure DevOps build and release pipelines, deployment options for targeting different types of AWS compute services, and tricks and tips for enhancing your CI/CD pipelines. Level : Intermediate Duration : 1 hour Service : EC2, Lambda, ECS Fargate Focus : Deploying from Azure DevOps to EC2, Lambda, or ECS Fargate Hands-On: Automating AWS Infrastructure with Powershell TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Architecting Active Directory in AWS In this workshop, we are going to show you how easy it is to securely implement Active Directory in AWS, as well as discuss the different deployment options. The structure of this Workshop involves showing you two Active Directory patterns commonly observed in the field, discussing their use cases, and then guiding you through implementing them inside a fully functional AWS account. Level : Intermediate Duration : 1 hour Service : Managed Active Directory Focus : Active Directory Migration Modernize Your First Windows Application with Windows Containers Our builder session will begin with a short presentation on how to containerize .NET applications on AWS. You will then have the opportunity to build a Docker container for a sample .NET application and deploy that container/application to Amazon Elastic Container Service (ECS). Level : Intermediate Duration : 1 hour Service : ECS Focus : .NET on ECS Hands-on building a migration strategy for SQL Server on AWS In this workshop you will work in teams to develop a strategy to migrate a SQL Server 2008R2 environment to AWS. Level : Intermediate Duration : 1 hour Service : DMS Focus : SQL Server Migration Storage Spaces Direct for Windows 2016 Failover Cluster TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Migrate your first SQL Server Database to Amazon RDS for SQL Server TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD Building a Serverless .Net App on AWS using the AWS toolkit for Visual Studio TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD DEV331 - Hands-On Building and Deploying .NET Applications on AWS TBD Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD AWS Tools for PowerShell In this lab you will learn how to use AWS Tools for Powershell Core to create AWS resources and host a simple HTML page. Level : Intermediate Duration : 1 hour Service : TBD Focus : TBD","title":"re:Invent 2018"},{"location":"reinvent2018/dev334/challenge1/","text":"Create AWS resources using Powershell Creating new VPC with a specific CIDR block $newvpc = New-EC2Vpc -CidrBlock 10 . 233 . 0 . 0 / 20 -InstanceTenancy default $vpcid = $newvpc . vpcid Tag the VPC we just created $vpctag = New-Object Amazon . EC2 . Model . Tag $vpctag . Key = \"Name\" $vpctag . Value = \"DEV334\" New-EC2Tag -resource $newvpc . VpcId -Tag $vpctag Creating an Internet Gateway for internet access $igw = New-EC2InternetGateway | Add-EC2InternetGateway -VpcId $newvpc . VpcId Check if Internet Gateway was created Get-EC2InternetGateway -Filter @{ Name = \"attachment.vpc-id\" ; Values = \"$vpcid\" } Find the Internet Gateway associated with newly created VPC and store it as a variable $igwid = ( Get-EC2InternetGateway -filter @{ Name = \"attachment.vpc-id\" ; Values = \"$vpcid\" }). InternetGatewayId Find main Route Table that got created with VPC Get-EC2RouteTable -Filter @{ Name = \"vpc-id\" ; Value = $newvpc . vpcid } Store main Route Table in a variable $mainroutetable = Get-EC2RouteTable -Filter @{ Name = \"vpc-id\" ; Value = $newvpc . vpcid } No need to create a new route table since we can use $mainroutetable as public route table Tag Route Table as Public $pubrttag = New-Object Amazon . EC2 . Model . Tag $pubrttag . Key = \"Name\" $pubrttag . Value = \"DEV334-PubRoute-1\" New-EC2Tag -Resource $mainroutetable . RouteTableId -Tag $pubrttag Add Routes New-EC2Route -RouteTableId $mainroutetable . RouteTableId -DestinationCidrBlock 0 . 0 . 0 . 0 / 0 -GatewayId $igwid Create Public Subnet $pubsub = New-EC2Subnet -VpcId $newvpc . vpcid -AvailabilityZone us-west - 2a -CidrBlock 10 . 233 . 0 . 0 / 24 Register Public Subnet to Public Route Table Register-EC2RouteTable -RouteTableId $mainroutetable . RouteTableId -SubnetId $pubsub . SubnetId Tag Public Subnet $pubsubtag = New-Object Amazon . EC2 . Model . Tag $pubsubtag . Key = \"Name\" $pubsubtag . Value = \"DEV334-PubSub-1\" New-EC2Tag -Resource $pubsub . subnetid -Tag $pubsubtag Create NAT Gateway But first, create an Elastic IP for it $eipallocation = New-EC2Address -Domain Vpc Check Elastic IP $eipallocation . AllocationId Now Create NAT Gateway $ngw = New-EC2NatGateway -SubnetId $pubsub . SubnetId -AllocationId $eipallocation . AllocationId Check status of NAT Gateway creation Wait for about 3 minutes for the NAT Gateway to be created ( Get-EC2NatGateway -Filter @{ Name = \"vpc-id\" ; Value = $newvpc . vpcid }). State Wait till the result of this cmdlet shows NAT Gateway as Available $ngwid = ( Get-EC2NatGateway | Where-Object { $_ . State -eq 'available' -and $_ . VpcId -eq $newvpc . VpcId }). NatGatewayId Create Private Route Table $prirt = New-EC2RouteTable -VpcId $newvpc . VpcId Tag Private Route Table $prirttag = New-Object Amazon . EC2 . Model . Tag $prirttag . Key = \"Name\" $prirttag . Value = \"DEV334-PriRoute-1\" New-EC2Tag -Resource $prirt . RouteTableId -Tag $prirttag Route internet traffic through NAT Gateway for Private Subnet New-EC2Route -RouteTableId $prirt . RouteTableId -DestinationCidrBlock 0 . 0 . 0 . 0 / 0 -GatewayId $ngwid Create Private Subnet $prisub = New-EC2Subnet -VpcId $newvpc . VpcId -AvailabilityZone us-west - 2a -CidrBlock 10 . 233 . 11 . 0 / 24 Tag Private Subnet $prisubtag = New-Object Amazon . EC2 . Model . Tag $prisubtag . Key = \"Name\" $prisubtag . Value = \"DEV334-PriSub-1\" New-EC2Tag -Resource $prisub . subnetid -Tag $prisubtag Register Private Subnet to Private Route table Register-EC2RouteTable -RouteTableId $prirt . RouteTableId -SubnetId $prisub . SubnetId Next, we are going to create the Security Group for EC2 Define Ports, Ingress, IP Range etc for SG $sgrdp1 = @{ IpProtocol = \"tcp\" ; FromPort = \"3389\" ; ToPort = \"3389\" ; IpRanges = \"0.0.0.0/0\" } $sghttp = @{ IpProtocol = \"tcp\" ; FromPort = \"80\" ; ToPort = \"80\" ; IpRanges = \"0.0.0.0/0\" } More Ports for Security Group if needed $sghttps = @{ IpProtocol = \"tcp\" ; FromPort = \"443\" ; ToPort = \"443\" ; IpRanges = \"0.0.0.0/0\" } Create SG New-EC2SecurityGroup -GroupName DEV334-win-sg - 1 -Description \"SG for Builder Session\" -VpcId $newvpc . VpcId Make sure Security Group has been created If the following cmdlet doesn't return a Group ID, SG was not created as expected. Ask for help. ( Get-EC2SecurityGroup -Filter @{ Name = \"group-name\" ; Value = \"DEV334-win-sg-1\" }). GroupID Store Security Group ID to a variable $newsg = ( Get-EC2SecurityGroup -Filter @{ Name = \"group-name\" ; Value = \"DEV334-win-sg-1\" }). GroupID Grant ingress access in Security Group Grant-EC2SecurityGroupIngress -GroupId $newsg -IpPermission @( $sghttp , $sgrdp1 ) Grant egress access in SG (allow-all to keep it stateful) When SG is created, it automatically allows all egress access. This is what makes a security group stateful. You can edit it to make it more restrictive or stateful for fewer ports and IP ranges, if needed. Create PEM key for EC2 Mac users, change the .pem file path to an appropriate location ( New-EC2KeyPair -KeyName \"DEV334-builder-session-key\" ). KeyMaterial | Out-File C : \\ DEV334-builder-session-key . pem Create Windows 2016 EC2 instance Use this AMI for EC2 $ami = Get-EC2ImageByName WINDOWS_2016_BASE Check if you have an AMI with above name. If the result is empty, AMI with the specified name doesn't exist. Ask for help. $ami . ImageId Create Tag for EC2 $ec2tag1 = @{ Key = \"Name\" ; Value = \"Win-2016-With-IIS\" } $ec2tagspec = New-Object Amazon . EC2 . Model . TagSpecification $ec2tagspec . ResourceType = \"instance\" $ec2tagspec . Tags . Add ( $ec2tag1 ) New-EC2Tag -Resource $ami . ImageId -Tag $ec2tag1 Auto assign IPv4 since we need a public IP to log into EC2 Edit-EC2SubnetAttribute -SubnetId $pubsub . SubnetId -MapPublicIpOnLaunch $true Setup IIS and the sample website Copy the following script to a text file on your laptop and save it as IISConfig.txt < powershell > Set-ExecutionPolicy Bypass -Scope Process -Force # Save machine name for messages [string] $vm = $Env:Computername # Check for IIS presence and installo, if needed if (( Get-WindowsFeature Web-Server ). InstallState -ne \"Installed\" ) { Write-Host \"IIS is not installed on $vm - installing\" Install-WindowsFeature web-server -IncludeManagementTools } Write-Host \"IIS is installed on $vm\" # Remove files of the default web site Remove-Item \"C:/inetpub/wwwroot/*.*\" # New HTML file defined as hash table inside the script # (in real life we would probably copy site from some bucket or share) $Page = @( ' <html> <h1 style=\"font-size: 75px;font-family: Arial, Helvetica, sans-serif; text-align: center;\" >Congratulations!</h1> <h2 style=\"font-size: 25px;font-family: Arial, Helvetica, sans-serif; text-align: center\" >You have successfully completed the exercise DEV334 - Powershell tools for AWS</h2> <svg style=\"width:100%;height:700px;\"> <path class=\"path\" fill=\"none\" stroke=\"orange\" stroke-width=\"5\" stroke-miterlimit=\"10\" d=\"M989,595H712v-35c0,0,4.5-1.8,8-3 c5.2-1.8,12.5,5.3,22-4c3.4-3.4-0.9-7.8-0.4-10.1c0.7-3.1,4.4-6.8,1.6-11.4c-2.6-4.2-6.9-3.6-8.2-5.5c-1.7-2.3-2.2-6.3-7-9 c-5.6-3.1-9.9,0.2-13-1c-2.5-0.9-2.3-5-9-5c-4.8,0-7.2,4.8-10,5c-2.3,0.2-4.9-4.5-10-2c-5.5,2.8-4,7.3-6,9c-1.4,1.2-3.1,3.2-6,4.2 c-2.6,0.9-4.9,3-4,8.8c0.6,3.7,6.8,4.1,7.5,6c1,2.9-6.4,6.2-2.5,12c3,4.5,9,1.6,12,1c1.9-0.4,7.1-0.7,8,0c3.4,2.5,9,5,9,5v35 l-506-1.5L182.3,444l0.3-0.8v-69.6l-0.2-0.2l12.3-72.3c10.9-2.6,16.5-6.5,16.5-6.5l-5.2-2.9l1.1-5.8l15-3.3l-10.1-3.9l1.2-4.6 c-15.8-8.3-32.2-11-32.2-11v-3.6l4.1-6.5h-4.9v-4.4h-8.3V244h-1.2l-1.7-31.2l-1-1.3l-1,1.3l-1.7,31.2h-1.2v4.8h-8.3v4.4h-4.9 l4.1,6.5v3.6c0,0-16.5,2.7-32.2,11l1.2,4.6l-10.1,3.9l15,3.3l1.1,5.8l-5.1,2.9c0,0,5.5,3.9,16.4,6.5l12.3,72.4l-0.1,0.1v69.6 l0.3,0.8l-14.6,149.4h-1.2H-11\" /> <path class=\"path\" fill=\"grey\" stroke=\"orange\" stroke-width=\"2\" stroke-miterlimit=\"10\" d=\"M8,393.7c0-13.5,12.1-10.8,15.6-14.7 c2.8-3.2-1-8.8,9-13.9c7.9-4.1,9.7,1,13.1,0.8c4.5-0.3,3.3-6.7,14.7-6.6c12.7,0.2,11.2,8.4,14.7,10.6c3,1.9,7.9-2.1,13.9,4.1 c3.8,4,1.3,7.4,2.5,9.8c2.2,4.4,14.7,0.9,14.7,13.9c0,12.2-13.5,8.3-17.2,10.6c-3.6,2.3-4.4,9.3-13.1,11.5c-8,2-9.5-4-13.9-4.1 c-5-0.1-5.5,8.6-18.8,6.6c-12.3-1.9-12.3-9.1-16.4-12.3C21.9,406.3,8,408.6,8,393.7z\" /> <path class=\"path\" fill=\"lightgrey\" stroke=\"orange\" stroke-width=\"2\" stroke-miterlimit=\"10\" d=\"M325.1,313.9c-3.9,3-3.9,10-15.7,11.8 c-12.8,2-13.3-6.4-18-6.3c-4.3,0.1-5.7,5.8-13.3,3.9c-8.4-2.1-9.1-8.8-12.5-11c-3.6-2.3-16.5,1.5-16.5-10.2c0-12.4,12-9.1,14.1-13.3 c1.1-2.3-1.3-5.6,2.4-9.4c5.8-6,10.4-2.1,13.3-3.9c3.4-2.1,2-10,14.1-10.2c11-0.2,9.9,6,14.1,6.3c3.3,0.2,5-4.7,12.5-0.8 c9.6,4.9,6,10.3,8.6,13.3c3.3,3.7,14.9,1.2,14.9,14.1C343.1,312.5,329.8,310.3,325.1,313.9z\" /> <path class=\"path\" fill=\"lightgrey\" stroke=\"orange\" stroke-width=\"2\" stroke-miterlimit=\"10\" d=\"M18.4,229.5c0-13.5,12.1-10.8,15.6-14.7 c2.8-3.2-1-8.8,9-13.9c7.9-4.1,9.7,1,13.1,0.8c4.5-0.3,3.3-6.7,14.7-6.6c12.7,0.2,11.2,8.4,14.7,10.6c3,1.9,7.9-2.1,13.9,4.1 c3.8,4,1.3,7.4,2.5,9.8c2.2,4.4,14.7,0.9,14.7,13.9c0,12.2-13.5,8.3-17.2,10.6c-3.6,2.3-4.4,9.3-13.1,11.5c-8,2-9.5-4-13.9-4.1 c-5-0.1-5.5,8.6-18.8,6.6c-12.3-1.9-12.3-9.1-16.4-12.3C32.2,242.1,18.4,244.4,18.4,229.5z\" /> <path class=\"path\" fill=\"grey\" stroke=\"orange\" stroke-width=\"2\" stroke-miterlimit=\"10\" d=\"M215.8,398.8c0-13.5,12.1-10.8,15.6-14.7 c2.8-3.2-1-8.8,9-13.9c7.9-4.1,9.7,1,13.1,0.8c4.5-0.3,3.3-6.7,14.7-6.6c12.7,0.2,11.2,8.4,14.7,10.6c3,1.9,7.9-2.1,13.9,4.1 c3.8,4,1.3,7.4,2.5,9.8c2.2,4.4,14.7,0.9,14.7,13.9c0,12.2-13.5,8.3-17.2,10.6c-3.6,2.3-4.4,9.3-13.1,11.5c-8,2-9.5-4-13.9-4.1 c-5-0.1-5.5,8.6-18.8,6.6c-12.3-1.9-12.3-9.1-16.4-12.3C229.7,411.3,215.8,413.6,215.8,398.8z\" /> </svg> </html>' ) Set-Content \"C:/inetpub/wwwroot/Index.html\" -Value $Page </ powershell > Create Windows EC2 instance Make sure you give the correct local path of the IISConfig.txt file you created in the above step $script = Get-Content -Raw C : \\< Path >\\ IISConfig . txt $userdata = [System.Convert] :: ToBase64String ( [System.Text.Encoding] :: ASCII . GetBytes ( $Script )) Create EC2 instance and pass PS Userdata file to Install IIS New-EC2Instance -ImageId $ami . ImageId -InstanceType t2 . large -SubnetId $pubsub . SubnetId -KeyName DEV334-builder-session-key -SecurityGroupId $newsg -UserData $userdata -Tagspecification $ec2tagspec Get IP address of the newly created EC2 instance ( Get-EC2Instance -Filter @{ Name = \"vpc-id\" ; Value = $vpcid }). Instances | Select-Object -Property PublicIpAddress Test the website It takes about 5 minutes for the EC2 Windows instance to be created. Open your browser on the local machine and navigate to http://","title":"Powershell exercise"},{"location":"reinvent2018/dev334/challenge1/#create-aws-resources-using-powershell","text":"","title":"Create AWS resources using Powershell"},{"location":"reinvent2018/dev334/challenge1/#creating-new-vpc-with-a-specific-cidr-block","text":"$newvpc = New-EC2Vpc -CidrBlock 10 . 233 . 0 . 0 / 20 -InstanceTenancy default $vpcid = $newvpc . vpcid","title":"Creating new VPC with a specific CIDR block"},{"location":"reinvent2018/dev334/challenge1/#tag-the-vpc-we-just-created","text":"$vpctag = New-Object Amazon . EC2 . Model . Tag $vpctag . Key = \"Name\" $vpctag . Value = \"DEV334\" New-EC2Tag -resource $newvpc . VpcId -Tag $vpctag","title":"Tag the VPC we just created"},{"location":"reinvent2018/dev334/challenge1/#creating-an-internet-gateway-for-internet-access","text":"$igw = New-EC2InternetGateway | Add-EC2InternetGateway -VpcId $newvpc . VpcId","title":"Creating an Internet Gateway for internet access"},{"location":"reinvent2018/dev334/challenge1/#check-if-internet-gateway-was-created","text":"Get-EC2InternetGateway -Filter @{ Name = \"attachment.vpc-id\" ; Values = \"$vpcid\" }","title":"Check if Internet Gateway was created"},{"location":"reinvent2018/dev334/challenge1/#find-the-internet-gateway-associated-with-newly-created-vpc-and-store-it-as-a-variable","text":"$igwid = ( Get-EC2InternetGateway -filter @{ Name = \"attachment.vpc-id\" ; Values = \"$vpcid\" }). InternetGatewayId","title":"Find the Internet Gateway associated with newly created VPC and store it as a variable"},{"location":"reinvent2018/dev334/challenge1/#find-main-route-table-that-got-created-with-vpc","text":"Get-EC2RouteTable -Filter @{ Name = \"vpc-id\" ; Value = $newvpc . vpcid }","title":"Find main Route Table that got created with VPC"},{"location":"reinvent2018/dev334/challenge1/#store-main-route-table-in-a-variable","text":"$mainroutetable = Get-EC2RouteTable -Filter @{ Name = \"vpc-id\" ; Value = $newvpc . vpcid } No need to create a new route table since we can use $mainroutetable as public route table","title":"Store main Route Table in a variable"},{"location":"reinvent2018/dev334/challenge1/#tag-route-table-as-public","text":"$pubrttag = New-Object Amazon . EC2 . Model . Tag $pubrttag . Key = \"Name\" $pubrttag . Value = \"DEV334-PubRoute-1\" New-EC2Tag -Resource $mainroutetable . RouteTableId -Tag $pubrttag","title":"Tag Route Table as Public"},{"location":"reinvent2018/dev334/challenge1/#add-routes","text":"New-EC2Route -RouteTableId $mainroutetable . RouteTableId -DestinationCidrBlock 0 . 0 . 0 . 0 / 0 -GatewayId $igwid","title":"Add Routes"},{"location":"reinvent2018/dev334/challenge1/#create-public-subnet","text":"$pubsub = New-EC2Subnet -VpcId $newvpc . vpcid -AvailabilityZone us-west - 2a -CidrBlock 10 . 233 . 0 . 0 / 24","title":"Create Public Subnet"},{"location":"reinvent2018/dev334/challenge1/#register-public-subnet-to-public-route-table","text":"Register-EC2RouteTable -RouteTableId $mainroutetable . RouteTableId -SubnetId $pubsub . SubnetId","title":"Register Public Subnet to Public Route Table"},{"location":"reinvent2018/dev334/challenge1/#tag-public-subnet","text":"$pubsubtag = New-Object Amazon . EC2 . Model . Tag $pubsubtag . Key = \"Name\" $pubsubtag . Value = \"DEV334-PubSub-1\" New-EC2Tag -Resource $pubsub . subnetid -Tag $pubsubtag","title":"Tag Public Subnet"},{"location":"reinvent2018/dev334/challenge1/#create-nat-gateway","text":"But first, create an Elastic IP for it $eipallocation = New-EC2Address -Domain Vpc","title":"Create NAT Gateway"},{"location":"reinvent2018/dev334/challenge1/#check-elastic-ip","text":"$eipallocation . AllocationId","title":"Check Elastic IP"},{"location":"reinvent2018/dev334/challenge1/#now-create-nat-gateway","text":"$ngw = New-EC2NatGateway -SubnetId $pubsub . SubnetId -AllocationId $eipallocation . AllocationId","title":"Now Create NAT Gateway"},{"location":"reinvent2018/dev334/challenge1/#check-status-of-nat-gateway-creation","text":"Wait for about 3 minutes for the NAT Gateway to be created ( Get-EC2NatGateway -Filter @{ Name = \"vpc-id\" ; Value = $newvpc . vpcid }). State","title":"Check status of NAT Gateway creation"},{"location":"reinvent2018/dev334/challenge1/#wait-till-the-result-of-this-cmdlet-shows-nat-gateway-as-available","text":"$ngwid = ( Get-EC2NatGateway | Where-Object { $_ . State -eq 'available' -and $_ . VpcId -eq $newvpc . VpcId }). NatGatewayId","title":"Wait till the result of this cmdlet shows NAT Gateway as Available"},{"location":"reinvent2018/dev334/challenge1/#create-private-route-table","text":"$prirt = New-EC2RouteTable -VpcId $newvpc . VpcId","title":"Create Private Route Table"},{"location":"reinvent2018/dev334/challenge1/#tag-private-route-table","text":"$prirttag = New-Object Amazon . EC2 . Model . Tag $prirttag . Key = \"Name\" $prirttag . Value = \"DEV334-PriRoute-1\" New-EC2Tag -Resource $prirt . RouteTableId -Tag $prirttag","title":"Tag Private Route Table"},{"location":"reinvent2018/dev334/challenge1/#route-internet-traffic-through-nat-gateway-for-private-subnet","text":"New-EC2Route -RouteTableId $prirt . RouteTableId -DestinationCidrBlock 0 . 0 . 0 . 0 / 0 -GatewayId $ngwid","title":"Route internet traffic through NAT Gateway for Private Subnet"},{"location":"reinvent2018/dev334/challenge1/#create-private-subnet","text":"$prisub = New-EC2Subnet -VpcId $newvpc . VpcId -AvailabilityZone us-west - 2a -CidrBlock 10 . 233 . 11 . 0 / 24","title":"Create Private Subnet"},{"location":"reinvent2018/dev334/challenge1/#tag-private-subnet","text":"$prisubtag = New-Object Amazon . EC2 . Model . Tag $prisubtag . Key = \"Name\" $prisubtag . Value = \"DEV334-PriSub-1\" New-EC2Tag -Resource $prisub . subnetid -Tag $prisubtag","title":"Tag Private Subnet"},{"location":"reinvent2018/dev334/challenge1/#register-private-subnet-to-private-route-table","text":"Register-EC2RouteTable -RouteTableId $prirt . RouteTableId -SubnetId $prisub . SubnetId Next, we are going to create the Security Group for EC2","title":"Register Private Subnet to Private Route table"},{"location":"reinvent2018/dev334/challenge1/#define-ports-ingress-ip-range-etc-for-sg","text":"$sgrdp1 = @{ IpProtocol = \"tcp\" ; FromPort = \"3389\" ; ToPort = \"3389\" ; IpRanges = \"0.0.0.0/0\" } $sghttp = @{ IpProtocol = \"tcp\" ; FromPort = \"80\" ; ToPort = \"80\" ; IpRanges = \"0.0.0.0/0\" }","title":"Define Ports, Ingress, IP Range etc for SG"},{"location":"reinvent2018/dev334/challenge1/#more-ports-for-security-group-if-needed","text":"$sghttps = @{ IpProtocol = \"tcp\" ; FromPort = \"443\" ; ToPort = \"443\" ; IpRanges = \"0.0.0.0/0\" }","title":"More Ports for Security Group if needed"},{"location":"reinvent2018/dev334/challenge1/#create-sg","text":"New-EC2SecurityGroup -GroupName DEV334-win-sg - 1 -Description \"SG for Builder Session\" -VpcId $newvpc . VpcId","title":"Create SG"},{"location":"reinvent2018/dev334/challenge1/#make-sure-security-group-has-been-created","text":"If the following cmdlet doesn't return a Group ID, SG was not created as expected. Ask for help. ( Get-EC2SecurityGroup -Filter @{ Name = \"group-name\" ; Value = \"DEV334-win-sg-1\" }). GroupID Store Security Group ID to a variable $newsg = ( Get-EC2SecurityGroup -Filter @{ Name = \"group-name\" ; Value = \"DEV334-win-sg-1\" }). GroupID","title":"Make sure Security Group has been created"},{"location":"reinvent2018/dev334/challenge1/#grant-ingress-access-in-security-group","text":"Grant-EC2SecurityGroupIngress -GroupId $newsg -IpPermission @( $sghttp , $sgrdp1 )","title":"Grant ingress access in Security Group"},{"location":"reinvent2018/dev334/challenge1/#grant-egress-access-in-sg-allow-all-to-keep-it-stateful","text":"When SG is created, it automatically allows all egress access. This is what makes a security group stateful. You can edit it to make it more restrictive or stateful for fewer ports and IP ranges, if needed.","title":"Grant egress access in SG (allow-all to keep it stateful)"},{"location":"reinvent2018/dev334/challenge1/#create-pem-key-for-ec2","text":"Mac users, change the .pem file path to an appropriate location ( New-EC2KeyPair -KeyName \"DEV334-builder-session-key\" ). KeyMaterial | Out-File C : \\ DEV334-builder-session-key . pem","title":"Create PEM key for EC2"},{"location":"reinvent2018/dev334/challenge1/#create-windows-2016-ec2-instance","text":"Use this AMI for EC2 $ami = Get-EC2ImageByName WINDOWS_2016_BASE Check if you have an AMI with above name. If the result is empty, AMI with the specified name doesn't exist. Ask for help. $ami . ImageId","title":"Create Windows 2016 EC2 instance"},{"location":"reinvent2018/dev334/challenge1/#create-tag-for-ec2","text":"$ec2tag1 = @{ Key = \"Name\" ; Value = \"Win-2016-With-IIS\" } $ec2tagspec = New-Object Amazon . EC2 . Model . TagSpecification $ec2tagspec . ResourceType = \"instance\" $ec2tagspec . Tags . Add ( $ec2tag1 ) New-EC2Tag -Resource $ami . ImageId -Tag $ec2tag1","title":"Create Tag for EC2"},{"location":"reinvent2018/dev334/challenge1/#auto-assign-ipv4-since-we-need-a-public-ip-to-log-into-ec2","text":"Edit-EC2SubnetAttribute -SubnetId $pubsub . SubnetId -MapPublicIpOnLaunch $true","title":"Auto assign IPv4 since we need a public IP to log into EC2"},{"location":"reinvent2018/dev334/challenge1/#setup-iis-and-the-sample-website","text":"Copy the following script to a text file on your laptop and save it as IISConfig.txt < powershell > Set-ExecutionPolicy Bypass -Scope Process -Force # Save machine name for messages [string] $vm = $Env:Computername # Check for IIS presence and installo, if needed if (( Get-WindowsFeature Web-Server ). InstallState -ne \"Installed\" ) { Write-Host \"IIS is not installed on $vm - installing\" Install-WindowsFeature web-server -IncludeManagementTools } Write-Host \"IIS is installed on $vm\" # Remove files of the default web site Remove-Item \"C:/inetpub/wwwroot/*.*\" # New HTML file defined as hash table inside the script # (in real life we would probably copy site from some bucket or share) $Page = @( ' <html> <h1 style=\"font-size: 75px;font-family: Arial, Helvetica, sans-serif; text-align: center;\" >Congratulations!</h1> <h2 style=\"font-size: 25px;font-family: Arial, Helvetica, sans-serif; text-align: center\" >You have successfully completed the exercise DEV334 - Powershell tools for AWS</h2> <svg style=\"width:100%;height:700px;\"> <path class=\"path\" fill=\"none\" stroke=\"orange\" stroke-width=\"5\" stroke-miterlimit=\"10\" d=\"M989,595H712v-35c0,0,4.5-1.8,8-3 c5.2-1.8,12.5,5.3,22-4c3.4-3.4-0.9-7.8-0.4-10.1c0.7-3.1,4.4-6.8,1.6-11.4c-2.6-4.2-6.9-3.6-8.2-5.5c-1.7-2.3-2.2-6.3-7-9 c-5.6-3.1-9.9,0.2-13-1c-2.5-0.9-2.3-5-9-5c-4.8,0-7.2,4.8-10,5c-2.3,0.2-4.9-4.5-10-2c-5.5,2.8-4,7.3-6,9c-1.4,1.2-3.1,3.2-6,4.2 c-2.6,0.9-4.9,3-4,8.8c0.6,3.7,6.8,4.1,7.5,6c1,2.9-6.4,6.2-2.5,12c3,4.5,9,1.6,12,1c1.9-0.4,7.1-0.7,8,0c3.4,2.5,9,5,9,5v35 l-506-1.5L182.3,444l0.3-0.8v-69.6l-0.2-0.2l12.3-72.3c10.9-2.6,16.5-6.5,16.5-6.5l-5.2-2.9l1.1-5.8l15-3.3l-10.1-3.9l1.2-4.6 c-15.8-8.3-32.2-11-32.2-11v-3.6l4.1-6.5h-4.9v-4.4h-8.3V244h-1.2l-1.7-31.2l-1-1.3l-1,1.3l-1.7,31.2h-1.2v4.8h-8.3v4.4h-4.9 l4.1,6.5v3.6c0,0-16.5,2.7-32.2,11l1.2,4.6l-10.1,3.9l15,3.3l1.1,5.8l-5.1,2.9c0,0,5.5,3.9,16.4,6.5l12.3,72.4l-0.1,0.1v69.6 l0.3,0.8l-14.6,149.4h-1.2H-11\" /> <path class=\"path\" fill=\"grey\" stroke=\"orange\" stroke-width=\"2\" stroke-miterlimit=\"10\" d=\"M8,393.7c0-13.5,12.1-10.8,15.6-14.7 c2.8-3.2-1-8.8,9-13.9c7.9-4.1,9.7,1,13.1,0.8c4.5-0.3,3.3-6.7,14.7-6.6c12.7,0.2,11.2,8.4,14.7,10.6c3,1.9,7.9-2.1,13.9,4.1 c3.8,4,1.3,7.4,2.5,9.8c2.2,4.4,14.7,0.9,14.7,13.9c0,12.2-13.5,8.3-17.2,10.6c-3.6,2.3-4.4,9.3-13.1,11.5c-8,2-9.5-4-13.9-4.1 c-5-0.1-5.5,8.6-18.8,6.6c-12.3-1.9-12.3-9.1-16.4-12.3C21.9,406.3,8,408.6,8,393.7z\" /> <path class=\"path\" fill=\"lightgrey\" stroke=\"orange\" stroke-width=\"2\" stroke-miterlimit=\"10\" d=\"M325.1,313.9c-3.9,3-3.9,10-15.7,11.8 c-12.8,2-13.3-6.4-18-6.3c-4.3,0.1-5.7,5.8-13.3,3.9c-8.4-2.1-9.1-8.8-12.5-11c-3.6-2.3-16.5,1.5-16.5-10.2c0-12.4,12-9.1,14.1-13.3 c1.1-2.3-1.3-5.6,2.4-9.4c5.8-6,10.4-2.1,13.3-3.9c3.4-2.1,2-10,14.1-10.2c11-0.2,9.9,6,14.1,6.3c3.3,0.2,5-4.7,12.5-0.8 c9.6,4.9,6,10.3,8.6,13.3c3.3,3.7,14.9,1.2,14.9,14.1C343.1,312.5,329.8,310.3,325.1,313.9z\" /> <path class=\"path\" fill=\"lightgrey\" stroke=\"orange\" stroke-width=\"2\" stroke-miterlimit=\"10\" d=\"M18.4,229.5c0-13.5,12.1-10.8,15.6-14.7 c2.8-3.2-1-8.8,9-13.9c7.9-4.1,9.7,1,13.1,0.8c4.5-0.3,3.3-6.7,14.7-6.6c12.7,0.2,11.2,8.4,14.7,10.6c3,1.9,7.9-2.1,13.9,4.1 c3.8,4,1.3,7.4,2.5,9.8c2.2,4.4,14.7,0.9,14.7,13.9c0,12.2-13.5,8.3-17.2,10.6c-3.6,2.3-4.4,9.3-13.1,11.5c-8,2-9.5-4-13.9-4.1 c-5-0.1-5.5,8.6-18.8,6.6c-12.3-1.9-12.3-9.1-16.4-12.3C32.2,242.1,18.4,244.4,18.4,229.5z\" /> <path class=\"path\" fill=\"grey\" stroke=\"orange\" stroke-width=\"2\" stroke-miterlimit=\"10\" d=\"M215.8,398.8c0-13.5,12.1-10.8,15.6-14.7 c2.8-3.2-1-8.8,9-13.9c7.9-4.1,9.7,1,13.1,0.8c4.5-0.3,3.3-6.7,14.7-6.6c12.7,0.2,11.2,8.4,14.7,10.6c3,1.9,7.9-2.1,13.9,4.1 c3.8,4,1.3,7.4,2.5,9.8c2.2,4.4,14.7,0.9,14.7,13.9c0,12.2-13.5,8.3-17.2,10.6c-3.6,2.3-4.4,9.3-13.1,11.5c-8,2-9.5-4-13.9-4.1 c-5-0.1-5.5,8.6-18.8,6.6c-12.3-1.9-12.3-9.1-16.4-12.3C229.7,411.3,215.8,413.6,215.8,398.8z\" /> </svg> </html>' ) Set-Content \"C:/inetpub/wwwroot/Index.html\" -Value $Page </ powershell >","title":"Setup IIS and the sample website"},{"location":"reinvent2018/dev334/challenge1/#create-windows-ec2-instance","text":"Make sure you give the correct local path of the IISConfig.txt file you created in the above step $script = Get-Content -Raw C : \\< Path >\\ IISConfig . txt $userdata = [System.Convert] :: ToBase64String ( [System.Text.Encoding] :: ASCII . GetBytes ( $Script ))","title":"Create Windows EC2 instance"},{"location":"reinvent2018/dev334/challenge1/#create-ec2-instance-and-pass-ps-userdata-file-to-install-iis","text":"New-EC2Instance -ImageId $ami . ImageId -InstanceType t2 . large -SubnetId $pubsub . SubnetId -KeyName DEV334-builder-session-key -SecurityGroupId $newsg -UserData $userdata -Tagspecification $ec2tagspec","title":"Create EC2 instance and pass PS Userdata file to Install IIS"},{"location":"reinvent2018/dev334/challenge1/#get-ip-address-of-the-newly-created-ec2-instance","text":"( Get-EC2Instance -Filter @{ Name = \"vpc-id\" ; Value = $vpcid }). Instances | Select-Object -Property PublicIpAddress","title":"Get IP address of the newly created EC2 instance"},{"location":"reinvent2018/dev334/challenge1/#test-the-website","text":"It takes about 5 minutes for the EC2 Windows instance to be created. Open your browser on the local machine and navigate to http://","title":"Test the website"},{"location":"reinvent2018/dev334/dev334index/","text":"DEV334 - AWS Tools for PowerShell Introduction In this lab you will learn how to use AWS Tools for Powershell Core to create AWS resources and host a simple HTML page. Step 1 - Prerequisites Make sure you visit the Preprequisites page and prepare your environment before proceeding with the exercise Step 2 - Exercise Once your prereqs are setup visit the exercise page to start on the powershell exercise Resources Visit the Resources page for additional info about Powershell Core","title":"Home"},{"location":"reinvent2018/dev334/dev334index/#dev334-aws-tools-for-powershell","text":"","title":"DEV334 - AWS Tools for PowerShell"},{"location":"reinvent2018/dev334/dev334index/#introduction","text":"In this lab you will learn how to use AWS Tools for Powershell Core to create AWS resources and host a simple HTML page.","title":"Introduction"},{"location":"reinvent2018/dev334/dev334index/#step-1-prerequisites","text":"Make sure you visit the Preprequisites page and prepare your environment before proceeding with the exercise","title":"Step 1 - Prerequisites"},{"location":"reinvent2018/dev334/dev334index/#step-2-exercise","text":"Once your prereqs are setup visit the exercise page to start on the powershell exercise","title":"Step 2 - Exercise"},{"location":"reinvent2018/dev334/dev334index/#resources","text":"Visit the Resources page for additional info about Powershell Core","title":"Resources"},{"location":"reinvent2018/dev334/prereq/","text":"Requirements For this session you will need a laptop with Remote Desktop Services client . If you are running Windows, you will already have one installed, but if you are on a Mac, you will need to install a client. You can download the Microsoft Remote Desktop Services client by clicking here (or navigate to https://itunes.apple.com/us/app/microsoft-remote-desktop-10/id1295203466). Downloading and installing PowerShell Core AWS Tools for Powershell Core depend on presence of Powershell Core on the machine. PowerShell Core is a distinct set of tools and components that is shipped, installed, and configured separately from Windows PowerShell and does not come pre-installed. Thus the first thing we need to do is to Install Powershell Core. To install PowerShell on a Windows client or Windows Server (works on Windows 7 SP1, Server 2008 R2, and later), download the MSI package from Microsoft GitHub releases page (or navigate to URL https://github.com/PowerShell/PowerShell/releases/download/v6.1.0/PowerShell-6.1.0-win-x64.msi). The Save As dialog will pop-up - navigate to some folder (usually, Downloads ) And click Save button: Navigate to the downloaded MSI file, right-click the PowerShell-6.1.0-win-x64.msi file and select Install from the context menu. Click Next in the Powershell setup dialog: and proceed with the installation. Following successful installation, a new icon will appear in the Windows menu - righ-click this icon and select If you have Mac laptop, please follow installation instructions at this link To verify Powershell Core installation, click on the button and in the command window enter the following command: $psversiontable - if installation is successful, you should get the following reply in the window: Installing AWS Tools for Powershell Core To install open Powershell Core console in Administrative mode (right-click PowershAWS Tools for Powershell Coreell Core icon and select Run as Administrator from the context menu) and execute the following script Start your powershell as Administrator. MacOS and Linux don't support Set-ExecutionPolicy cmdlet, they run \"unrestricted\" by default, so please ignore respective line in the script below Set-ExecutionPolicy Bypass -Scope Process -Force Set-PSRepository PSGallery -InstallationPolicy Trusted Install-Module -Name AWSPowerShell . NetCore To verify installation of AWS Powwershel Core, run the following command Get-InstalledModule AWSPowerShell . NetCore If installation is successful, you will see the following screen: Configuring credentials for AWS Tools for Powershell Core All AWSPowerShell.NetCore cmdlets accept AWS Access and Secret keys or the names of credential profiles when they run. When running on Windows, AWSPowerShell.NetCore module have access to the AWS SDK for .NET credential store file (stored per-user in AppDataLocalAWSToolkitRegisteredAccounts.json file). This file stores credentials in encrypted format by using Windows crypto APIs and cannot be used on a different machine. This file is the first to be inspected by the AWSPowerShell.NetCore module when looking for a credential profile, and is also where it stores new credential profiles. As Windows crypto API is not available on other platforms, a different mechanism for managing AWSPowerShell.NetCore credentials is used on MAC and Unix platforms. For details on how to set up credentials for AWSPowerShell.NetCore module on other platforms please check this link To obtain AWS Access and Secret keys to establish AWSPowerShell.NetCore credentials, you will have to connect to AWS account and create a user that will provide credentials for AWSPowerShell.NetCore module. For this session, we have provisioned an AWS account, which you will be using to create various AWS resources using AWS Tools for Powershell Core. You will have been handed account details by the Builder Session staff; this information contains an AWS account ID, as well as a username and password to connect to the account. Locate this information; if you do not have this information, indicate to the event staff that you need assistance. Navigate your Web browser to https://console.aws.amazon.com. If you have an existing account, you will need to log out of this account. In the \u201cSign in\u201d dialog that appears, enter the account ID and click Next You will then be prompted for an IAM user name and password. Enter this information and click \u201cSign In.\u201d In the AWS console navigate to Identity and Access Management (IAM) service: In the IAM Console select Users and then click Add user Enter user name (e.g. PSUser ), check the checkbox near the Programmatic access , and then click Next: Permissions button: On the next screen select Attach existing policies directly , check the checkbox next to AdministratorAccess policy ( NOTE: In real life you should select or configure policies specific to the tasks that this user will be allowed to perform, but for this Builder session we will not go into these details ), and then click Next: Review button: Review your settings and click Create User button: It is very important that on the following screen you click Download .csv button - this is your only chance to view or save Access key ID and Secret access key for just created user: In the pop-up dialog navigate to some folder (e.g. Downloads ) and click Save button: Setup AWS Powershell environment Import Powershell Core into your session Through the rest of the session you will be working with AWS cmdlets; to make them available, they need to be imported into your Powershell session using the following command: Import-Module AWSPowerShell . NetCore Set AWS Credentials on your Powershell console. Insert your own AccessKey and SecretKey The .csv file downloaded in step 10 above contains Access key ID and Secret access key that will be used to provide credentials for AWSPowerShell.NetCore module. Open this file and copy your Access key ID and Secret access key into the following script: Set-AWSCredential -AccessKey < access-key > -SecretKey < secret-key > -StoreAs default After you execute this script, your credentials will be saved as default AWS Powershell Core credentials and can be used in subsequent scripts. Set Profile, Region and Execution Policy Initialize-AWSDefaults -region us-west - 2 Set-DefaultAWSRegion -Region us-west - 2","title":"Prerequisites"},{"location":"reinvent2018/dev334/prereq/#requirements","text":"For this session you will need a laptop with Remote Desktop Services client . If you are running Windows, you will already have one installed, but if you are on a Mac, you will need to install a client. You can download the Microsoft Remote Desktop Services client by clicking here (or navigate to https://itunes.apple.com/us/app/microsoft-remote-desktop-10/id1295203466).","title":"Requirements"},{"location":"reinvent2018/dev334/prereq/#downloading-and-installing-powershell-core","text":"AWS Tools for Powershell Core depend on presence of Powershell Core on the machine. PowerShell Core is a distinct set of tools and components that is shipped, installed, and configured separately from Windows PowerShell and does not come pre-installed. Thus the first thing we need to do is to Install Powershell Core. To install PowerShell on a Windows client or Windows Server (works on Windows 7 SP1, Server 2008 R2, and later), download the MSI package from Microsoft GitHub releases page (or navigate to URL https://github.com/PowerShell/PowerShell/releases/download/v6.1.0/PowerShell-6.1.0-win-x64.msi). The Save As dialog will pop-up - navigate to some folder (usually, Downloads ) And click Save button: Navigate to the downloaded MSI file, right-click the PowerShell-6.1.0-win-x64.msi file and select Install from the context menu. Click Next in the Powershell setup dialog: and proceed with the installation. Following successful installation, a new icon will appear in the Windows menu - righ-click this icon and select If you have Mac laptop, please follow installation instructions at this link To verify Powershell Core installation, click on the button and in the command window enter the following command: $psversiontable - if installation is successful, you should get the following reply in the window:","title":"Downloading and installing PowerShell Core"},{"location":"reinvent2018/dev334/prereq/#installing-aws-tools-for-powershell-core","text":"To install open Powershell Core console in Administrative mode (right-click PowershAWS Tools for Powershell Coreell Core icon and select Run as Administrator from the context menu) and execute the following script Start your powershell as Administrator. MacOS and Linux don't support Set-ExecutionPolicy cmdlet, they run \"unrestricted\" by default, so please ignore respective line in the script below Set-ExecutionPolicy Bypass -Scope Process -Force Set-PSRepository PSGallery -InstallationPolicy Trusted Install-Module -Name AWSPowerShell . NetCore To verify installation of AWS Powwershel Core, run the following command Get-InstalledModule AWSPowerShell . NetCore If installation is successful, you will see the following screen:","title":"Installing AWS Tools for Powershell Core"},{"location":"reinvent2018/dev334/prereq/#configuring-credentials-for-aws-tools-for-powershell-core","text":"All AWSPowerShell.NetCore cmdlets accept AWS Access and Secret keys or the names of credential profiles when they run. When running on Windows, AWSPowerShell.NetCore module have access to the AWS SDK for .NET credential store file (stored per-user in AppDataLocalAWSToolkitRegisteredAccounts.json file). This file stores credentials in encrypted format by using Windows crypto APIs and cannot be used on a different machine. This file is the first to be inspected by the AWSPowerShell.NetCore module when looking for a credential profile, and is also where it stores new credential profiles. As Windows crypto API is not available on other platforms, a different mechanism for managing AWSPowerShell.NetCore credentials is used on MAC and Unix platforms. For details on how to set up credentials for AWSPowerShell.NetCore module on other platforms please check this link To obtain AWS Access and Secret keys to establish AWSPowerShell.NetCore credentials, you will have to connect to AWS account and create a user that will provide credentials for AWSPowerShell.NetCore module. For this session, we have provisioned an AWS account, which you will be using to create various AWS resources using AWS Tools for Powershell Core. You will have been handed account details by the Builder Session staff; this information contains an AWS account ID, as well as a username and password to connect to the account. Locate this information; if you do not have this information, indicate to the event staff that you need assistance. Navigate your Web browser to https://console.aws.amazon.com. If you have an existing account, you will need to log out of this account. In the \u201cSign in\u201d dialog that appears, enter the account ID and click Next You will then be prompted for an IAM user name and password. Enter this information and click \u201cSign In.\u201d In the AWS console navigate to Identity and Access Management (IAM) service: In the IAM Console select Users and then click Add user Enter user name (e.g. PSUser ), check the checkbox near the Programmatic access , and then click Next: Permissions button: On the next screen select Attach existing policies directly , check the checkbox next to AdministratorAccess policy ( NOTE: In real life you should select or configure policies specific to the tasks that this user will be allowed to perform, but for this Builder session we will not go into these details ), and then click Next: Review button: Review your settings and click Create User button: It is very important that on the following screen you click Download .csv button - this is your only chance to view or save Access key ID and Secret access key for just created user: In the pop-up dialog navigate to some folder (e.g. Downloads ) and click Save button:","title":"Configuring credentials for AWS Tools for Powershell Core"},{"location":"reinvent2018/dev334/prereq/#setup-aws-powershell-environment","text":"","title":"Setup AWS Powershell environment"},{"location":"reinvent2018/dev334/prereq/#import-powershell-core-into-your-session","text":"Through the rest of the session you will be working with AWS cmdlets; to make them available, they need to be imported into your Powershell session using the following command: Import-Module AWSPowerShell . NetCore","title":"Import Powershell Core into your session"},{"location":"reinvent2018/dev334/prereq/#set-aws-credentials-on-your-powershell-console-insert-your-own-accesskey-and-secretkey","text":"The .csv file downloaded in step 10 above contains Access key ID and Secret access key that will be used to provide credentials for AWSPowerShell.NetCore module. Open this file and copy your Access key ID and Secret access key into the following script: Set-AWSCredential -AccessKey < access-key > -SecretKey < secret-key > -StoreAs default After you execute this script, your credentials will be saved as default AWS Powershell Core credentials and can be used in subsequent scripts.","title":"Set AWS Credentials on your Powershell console. Insert your own AccessKey and SecretKey"},{"location":"reinvent2018/dev334/prereq/#set-profile-region-and-execution-policy","text":"Initialize-AWSDefaults -region us-west - 2 Set-DefaultAWSRegion -Region us-west - 2","title":"Set Profile, Region and Execution Policy"},{"location":"reinvent2018/dev334/resources/","text":"About Windows Powershell and Powershell Core Windows PowerShell is an automation and configuration tool/framework that works well with your existing tools and is optimized for dealing with structured data (e.g. JSON, CSV, XML, etc.), REST APIs, and object models. It includes a command-line shell, an associated scripting language and a framework for processing cmdlets. The first version of PowerShell was released in November 2006 for Windows XP, Windows Server 2003 and Windows Vista. The latest version of PowerShell is Windows PowerShell 5.1, which is delivered as part of Windows 10 Anniversary Update and Windows Server 2016. It also can be installed and works on Windows Server 2008 R2, Windows Server 2012 and Windows Server 2012 R2, Windows 7 Service Pack 1 and Windows 8.1 (Pro and Enterprise editions). Windows Powershell is based on the .NET framework. Due to Powershell dependence on .Net Framework, Windows Powershell, up to the latest release 5.1, is limited to Windows platform. Considering a multitude of existing Operating Systems, especially in the Cloud environment, and understanding the limitations of Powershell, in 2018 Microsoft released new version, Powershell Core , which is based upon multiplatform .Net Core Framework. PowerShell Core is an open source project , and it can be installed on Windows, macOS, and Linux platforms. The cross-platform nature of PowerShell Core means that scripts that you write will run on any supported operating system. You can write PowerShell Core scripts on Windows, and use them on supported Mac OS X or Linux devices. AWS embraced Powershell functionality early on as a set of tools complementary to AWS SDK, Command-Line Interface (CLI) and AWS Console to manage AWS resources. The AWS Tools for Windows PowerShell and AWS Tools for PowerShell Core are PowerShell modules that are built on the functionality exposed by the AWS SDK for .NET. The AWS PowerShell Tools enable you to script operations on your AWS resources from the PowerShell command line. Although the cmdlets are implemented using the service clients and methods from the SDK, the cmdlets provide an idiomatic PowerShell experience for specifying parameters and handling results. For example, the cmdlets for the Tools for Windows PowerShell support PowerShell pipelining \u2014 that is, you can pipeline PowerShell objects both into and out of the cmdlets. The AWS Tools for Windows PowerShell and AWS Tools for PowerShell Core are flexible in how they enable you to handle credentials including support for the AWS Identity and Access Management (IAM) infrastructure; you can use the tools with IAM user credentials, temporary security tokens, and IAM roles. The AWS Tools for Windows PowerShell support the same set of services and regions as supported by the SDK. The first version, AWS Tools for Windows PowerShell 1.0.0.0 , was released in 2012. AWS continuously works on further improvements and extensions of the tool \u2014 there were multiple releases of this tool since its inception. The latest release, AWS Tools for PowerShell 3.3.343.0 , introduced AWS Tools for Powershell Core , which, besides traditional scripts, allows to build AWS Lambda functions. In this Builder Session we will focus on AWS Tools for Powershell Core as this scripts can be executed from multiple platforms.","title":"Resources"},{"location":"reinvent2018/dev334/resources/#about-windows-powershell-and-powershell-core","text":"Windows PowerShell is an automation and configuration tool/framework that works well with your existing tools and is optimized for dealing with structured data (e.g. JSON, CSV, XML, etc.), REST APIs, and object models. It includes a command-line shell, an associated scripting language and a framework for processing cmdlets. The first version of PowerShell was released in November 2006 for Windows XP, Windows Server 2003 and Windows Vista. The latest version of PowerShell is Windows PowerShell 5.1, which is delivered as part of Windows 10 Anniversary Update and Windows Server 2016. It also can be installed and works on Windows Server 2008 R2, Windows Server 2012 and Windows Server 2012 R2, Windows 7 Service Pack 1 and Windows 8.1 (Pro and Enterprise editions). Windows Powershell is based on the .NET framework. Due to Powershell dependence on .Net Framework, Windows Powershell, up to the latest release 5.1, is limited to Windows platform. Considering a multitude of existing Operating Systems, especially in the Cloud environment, and understanding the limitations of Powershell, in 2018 Microsoft released new version, Powershell Core , which is based upon multiplatform .Net Core Framework. PowerShell Core is an open source project , and it can be installed on Windows, macOS, and Linux platforms. The cross-platform nature of PowerShell Core means that scripts that you write will run on any supported operating system. You can write PowerShell Core scripts on Windows, and use them on supported Mac OS X or Linux devices. AWS embraced Powershell functionality early on as a set of tools complementary to AWS SDK, Command-Line Interface (CLI) and AWS Console to manage AWS resources. The AWS Tools for Windows PowerShell and AWS Tools for PowerShell Core are PowerShell modules that are built on the functionality exposed by the AWS SDK for .NET. The AWS PowerShell Tools enable you to script operations on your AWS resources from the PowerShell command line. Although the cmdlets are implemented using the service clients and methods from the SDK, the cmdlets provide an idiomatic PowerShell experience for specifying parameters and handling results. For example, the cmdlets for the Tools for Windows PowerShell support PowerShell pipelining \u2014 that is, you can pipeline PowerShell objects both into and out of the cmdlets. The AWS Tools for Windows PowerShell and AWS Tools for PowerShell Core are flexible in how they enable you to handle credentials including support for the AWS Identity and Access Management (IAM) infrastructure; you can use the tools with IAM user credentials, temporary security tokens, and IAM roles. The AWS Tools for Windows PowerShell support the same set of services and regions as supported by the SDK. The first version, AWS Tools for Windows PowerShell 1.0.0.0 , was released in 2012. AWS continuously works on further improvements and extensions of the tool \u2014 there were multiple releases of this tool since its inception. The latest release, AWS Tools for PowerShell 3.3.343.0 , introduced AWS Tools for Powershell Core , which, besides traditional scripts, allows to build AWS Lambda functions. In this Builder Session we will focus on AWS Tools for Powershell Core as this scripts can be executed from multiple platforms.","title":"About Windows Powershell and Powershell Core"},{"location":"reinvent2018/win308/challenge1/","text":"Challenge 1: Configuring resources using AWS Tools for Windows PowerShell You are the IT administrator of your company and you manage an important ec2 fleet of windows and linux instances. You have identified multiple misconfigurations inside your fleet from your compliance reports and defined the following required tasks for remediation: Task 1: All instances are missing the tag \"Cost Center : IT\" Task 2: All instances without the tag \"Tier : WebServer\" should be protected against termination You are required to provide individual powershell scripts using AWS Tools for Powershell to automate these tasks. To get started you can either RDP to the machine provided to you as part of the prerequisites of this workshop or you can discover the new Session Manager feature from System Manager to get access to your management instance directly form the AWS Console. If you want to discover session manager, follow the steps below. Login to AWS management console Log to AWS console using your provided credential. Ensure to select the Norther Virginia (us-east-1) region Click on service and select Systems Manager (ssm) Connect to your workshop instance using Session Manager. From the SSM console, click on Session Manager then Start Session Select the \"Visual Studio Workshop\" instance and click on start session. If you see a message telling you that your SSM agent is not up to date, click on \u201cupdate SSM agent\u201d to update the agent running on the instance. You are now connected into a powershell shell into your running instance through session manager. Start by setting your default aws region to ca-central-1 and run the command (get-ec2instance).instances to list your existing ec2 instances. Set-DefaultAWSRegion -Region us-east - 1 ( Get-EC2Instance ). Instances Tools Description AWS Tools for Powershell : The AWS Tools for PowerShell let developers and administrators manage their AWS services and resources in the PowerShell scripting environment. Now you can manage your AWS resources with the same PowerShell tools you use to manage your Windows, Linux, and MacOS environments. AWS Tools are available also for Powershell Core which can then be used on MacOS and Linux operating system as well. AWS System Manager - Session Manager : Session Manager is a fully managed AWS Systems Manager capability that lets you manage your Amazon EC2 instances through an interactive one-click browser-based shell or through the AWS CLI. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. Session Manager also makes it easy to comply with corporate policies that require controlled access to instances, strict security practices, and fully auditable logs with instance access details, while still providing end users with simple one-click cross-platform access to your Amazon EC2 instances. Hints Task 1 : Create a list that contains all instances which don't have the required tag. Create a variable which contains the desired tag to set then call your instance list to set the tag through a pipe. Task 2 : Create a list for all instances missing the tag \"Tier : Webserver\" then use a pipe to pass the terminate protection cmdlet. All AWS tools for powershell commands required for this challenge rely on the Amazon Elastic Compute service commands of the tools, you can see a reference of these commands here or by running the command: powershell Get-AWSCmdletName -Service \"Amazon Elastic Compute Cloud\" References Getting started with session manager AWS Toolkit for Powershell","title":"Challenge 1 Configuring resources using AWS Tools for Windows PowerShell"},{"location":"reinvent2018/win308/challenge1/#challenge-1-configuring-resources-using-aws-tools-for-windows-powershell","text":"You are the IT administrator of your company and you manage an important ec2 fleet of windows and linux instances. You have identified multiple misconfigurations inside your fleet from your compliance reports and defined the following required tasks for remediation: Task 1: All instances are missing the tag \"Cost Center : IT\" Task 2: All instances without the tag \"Tier : WebServer\" should be protected against termination You are required to provide individual powershell scripts using AWS Tools for Powershell to automate these tasks. To get started you can either RDP to the machine provided to you as part of the prerequisites of this workshop or you can discover the new Session Manager feature from System Manager to get access to your management instance directly form the AWS Console. If you want to discover session manager, follow the steps below. Login to AWS management console Log to AWS console using your provided credential. Ensure to select the Norther Virginia (us-east-1) region Click on service and select Systems Manager (ssm) Connect to your workshop instance using Session Manager. From the SSM console, click on Session Manager then Start Session Select the \"Visual Studio Workshop\" instance and click on start session. If you see a message telling you that your SSM agent is not up to date, click on \u201cupdate SSM agent\u201d to update the agent running on the instance. You are now connected into a powershell shell into your running instance through session manager. Start by setting your default aws region to ca-central-1 and run the command (get-ec2instance).instances to list your existing ec2 instances. Set-DefaultAWSRegion -Region us-east - 1 ( Get-EC2Instance ). Instances","title":"Challenge  1: Configuring resources using AWS Tools for Windows PowerShell"},{"location":"reinvent2018/win308/challenge1/#tools-description","text":"AWS Tools for Powershell : The AWS Tools for PowerShell let developers and administrators manage their AWS services and resources in the PowerShell scripting environment. Now you can manage your AWS resources with the same PowerShell tools you use to manage your Windows, Linux, and MacOS environments. AWS Tools are available also for Powershell Core which can then be used on MacOS and Linux operating system as well. AWS System Manager - Session Manager : Session Manager is a fully managed AWS Systems Manager capability that lets you manage your Amazon EC2 instances through an interactive one-click browser-based shell or through the AWS CLI. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. Session Manager also makes it easy to comply with corporate policies that require controlled access to instances, strict security practices, and fully auditable logs with instance access details, while still providing end users with simple one-click cross-platform access to your Amazon EC2 instances.","title":"Tools Description"},{"location":"reinvent2018/win308/challenge1/#hints","text":"Task 1 : Create a list that contains all instances which don't have the required tag. Create a variable which contains the desired tag to set then call your instance list to set the tag through a pipe. Task 2 : Create a list for all instances missing the tag \"Tier : Webserver\" then use a pipe to pass the terminate protection cmdlet. All AWS tools for powershell commands required for this challenge rely on the Amazon Elastic Compute service commands of the tools, you can see a reference of these commands here or by running the command: powershell Get-AWSCmdletName -Service \"Amazon Elastic Compute Cloud\"","title":"Hints"},{"location":"reinvent2018/win308/challenge1/#references","text":"Getting started with session manager AWS Toolkit for Powershell","title":"References"},{"location":"reinvent2018/win308/challenge2/","text":"Challenge 2 You work for a company that hosts all their servers in EC2. They want to make sure that IIS is installed on all the Instances that have tage named 'Role' with value of 'Frontend' For this they want to use AWS SSM run command. Tools description AWS Systems Manager Run Command : AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. Use Run Command to perform on-demand changes like updating applications or running Linux shell scripts and Windows PowerShell commands on a target set of dozens or hundreds of instances. Hints Use AWS Systems Manager Run Command console and find the document that allows you to run Powershell scripts on your Windows instances. The Powershell command to install IIS is: Install-WindowsFeature -Name Web-Server -IncludeManagementTools References Running Commands Using Systems Manager Run Command AWS Systems Manager State Manager Creating Associations that Execute MOF Files AWS Systems Manager Configuration Compliance Write-SSMComplianceItem Cmdlet","title":"Challenge 2 Configure EC2 instances using Powershell and SSM"},{"location":"reinvent2018/win308/challenge2/#challenge-2","text":"You work for a company that hosts all their servers in EC2. They want to make sure that IIS is installed on all the Instances that have tage named 'Role' with value of 'Frontend' For this they want to use AWS SSM run command.","title":"Challenge 2"},{"location":"reinvent2018/win308/challenge2/#tools-description","text":"AWS Systems Manager Run Command : AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. Use Run Command to perform on-demand changes like updating applications or running Linux shell scripts and Windows PowerShell commands on a target set of dozens or hundreds of instances.","title":"Tools description"},{"location":"reinvent2018/win308/challenge2/#hints","text":"Use AWS Systems Manager Run Command console and find the document that allows you to run Powershell scripts on your Windows instances. The Powershell command to install IIS is: Install-WindowsFeature -Name Web-Server -IncludeManagementTools","title":"Hints"},{"location":"reinvent2018/win308/challenge2/#references","text":"Running Commands Using Systems Manager Run Command AWS Systems Manager State Manager Creating Associations that Execute MOF Files AWS Systems Manager Configuration Compliance Write-SSMComplianceItem Cmdlet","title":"References"},{"location":"reinvent2018/win308/challenge3/","text":"Challenge 3: Create a Lambda Using PowerShell We have identified that our development staff tend to work between the hours of 8AM-6PM and do not work on weekends. We can save 70% on our compute costs by not paying to run these instances during off hours. Create a PowerShell-based Lambda function that stops EC2 instances that are tagged as \"Purpose: Development\" Create a PowerShell-based Lambda function that starts EC2 instances that are tagged as \"Purpose: Development\" Configure these Lambdas to be triggered at 6PM and 8AM M-F, respectively Demonstrate your results using the Test feature of these lambdas to stop/start these instances. Solving the Problem Opening a PowerShell Core session Let's open a PowerShell Core prompt; press and hold the Start button and press the 'R' key to open a \"Run\" dialog, type \"Pwsh\", and hit enter. This will open a PowerShell Core window. In order to interact with AWS resources, we'll need to load the AWS PowerShell for .NET Core module as well as the AWS Tools for Powershell Lambda: Install-Module AWSPowerShell.NetCore -Force Install-Module AWSLambdaPSCore -Force Import-Module AWSPowerShell.NetCore Import-Module AWSLambdaPSCore Next, let's make sure our commands will run in the correct region using the Set-DefaultAWSRegion commandlet: Set-DefaultAWSRegion -Region us-east-1 Designing and Testing Our Lambda Since Powershell Core Commandlets can run locally as well as in the Lambda execution environment, we can write and test the script before we deploy it to Lambda. Our first task becomes identifying the EC2 instances that match our 'Production' tag. This can be accomplished using the Get-EC2Instance Commandlet. Let's test it out: Get-EC2Instance -Filter @( @ { name=' tag : Purpose ';value=' Development ' } ) Example Results GroupNames : {} Groups : {} Instances : {xxxx} OwnerId : 999999999999 RequesterId : ReservationId : r-0011223344556677 GroupNames : {} Groups : {} Instances : {yyyy} OwnerId : 999999999999 RequesterId : ReservationId : r-0011223344556677 GroupNames : {} Groups : {} Instances : {zzzz} OwnerId : 999999999999 RequesterId : ReservationId : r-0011223344556677 Writing the scripts Once we have our permissions defined, we need to create our Lambda functions. We will write two: one to list and stop instances, and another to list and stop instances. In order to make our Lambda more flexible, we will use an environment variable named 'PURPOSE' to supply the value of the \"Purpose\" tag we're filtering on. StartInstances.ps1 # Requires -Modules @ { ModuleName='AWSPowerShell.NetCore' ; ModuleVersion='3.3.390.0' } $ instances = Get-EC2Instance -Filter @( @ { name=' tag : Purpose ' ; value=\"$ env : PURPOSE_TAG \"} ) | Select-Object -ExpandProperty RunningInstance Write-Host \" Starting $ ( $ instances . Length ) instances ... \" Start-EC2Instance - InstanceId $ instances StopInstances.ps1 # Requires -Modules @ { ModuleName='AWSPowerShell.NetCore' ; ModuleVersion='3.3.390.0' } $ instances = Get-EC2Instance -Filter @( @ { name=' tag : Purpose ' ; value=\"$ env : PURPOSE_TAG \"} )|Select-Object -ExpandProperty RunningInstance Write-Host \" Stopping $ ( $ instances . Length ) instances ... \" Stop-EC2Instance - InstanceId $ instances Publishing the Lambda Finally, we will publish our Lambda's scripts, setting the PURPOSE_TAG environment variable's value to \"Development\": Publish-AWSPowerShellLambda -ScriptPath ./StartInstances.ps1 -IAMRoleArn StartStopInstancesRole -Name StartInstances -EnvironmentVariable @{\"PURPOSE_TAG\"=\"Development\"} Publish-AWSPowerShellLambda -ScriptPath ./StopInstances.ps1 -IAMRoleArn StartStopInstancesRole -Name StopInstances -EnvironmentVariable @{\"PURPOSE_TAG\"=\"Development\"} Scheduling our Lambdas Now we need to schedule our Lambdas. To do this, we will need to create a CloudWatch Event rule that fires the event on a schedule. You can do this via PowerShell using CloudWatch's support for Cron syntax: Write-CWERule -Name StartOfDay -ScheduleExpression \"cron(0 12 * * ? *)\" Write-CWERule -Name EndOfDay -ScheduleExpression \"cron(0 21 * * ? *)\" After the rule has been created, each of these commands will write an ARN for the new event to the console. Next, we will need to grant permission for these new events to invoke our Lambda functions. For each of the Lambdas, we will need to grant the corresponding event permission: Add-LMPermission -FunctionName StartInstances -StatementId MyStatement -Action 'lambda:InvokeFunction' -Principal events.amazonaws.com -SourceArn $(Get-CWERule StartOfDay -region us-east-1).Arn Add-LMPermission -FunctionName StopInstances -StatementId MyStatement -Action 'lambda:InvokeFunction' -Principal events.amazonaws.com -SourceArn $(Get-CWERule EndOfDay -region us-east-1).Arn Finally, we need to associate the event with our Lambda by creating a target for the events, using the name of the rule and the ARN of the function. To get the ARN of your lambda, you can use the Get-LMFunctionConfiguration Cmdlet: Get-LMFunctionConfiguration <Function name> Now that we know the ARN, let's wire up the target: Write-CWETarget -Rule <Rule name> -Target @{ ID=1;Arn =\"<Lambda ARN>\"} Congratulations! You have now created a Lambda that uses PowerShell Core to automate the management of your EC2 instances. We hope you now have a better understanding of what you can accomplish with PowerShell on AWS. Experiment with the available Commandlets to explore the options available to you (remember to adjust permissions!) or change the scheduling of function execution to suit your use case. References Using the AWS Tools for Windows PowerShell AWS Tools for PowerShell Cmdlet Reference AWS Lambda and Tools for PowerShell AWS Lambda Permissions Model IAM Actions, Resources, and Condition Keys for Amazon EC2 Tutorial: Schedule AWS Lambda Functions Using CloudWatch Events","title":"Challenge 3: Create a Lambda Using PowerShell"},{"location":"reinvent2018/win308/challenge3/#challenge-3-create-a-lambda-using-powershell","text":"We have identified that our development staff tend to work between the hours of 8AM-6PM and do not work on weekends. We can save 70% on our compute costs by not paying to run these instances during off hours. Create a PowerShell-based Lambda function that stops EC2 instances that are tagged as \"Purpose: Development\" Create a PowerShell-based Lambda function that starts EC2 instances that are tagged as \"Purpose: Development\" Configure these Lambdas to be triggered at 6PM and 8AM M-F, respectively Demonstrate your results using the Test feature of these lambdas to stop/start these instances.","title":"Challenge 3: Create a Lambda Using PowerShell"},{"location":"reinvent2018/win308/challenge3/#solving-the-problem","text":"","title":"Solving the Problem"},{"location":"reinvent2018/win308/challenge3/#opening-a-powershell-core-session","text":"Let's open a PowerShell Core prompt; press and hold the Start button and press the 'R' key to open a \"Run\" dialog, type \"Pwsh\", and hit enter. This will open a PowerShell Core window. In order to interact with AWS resources, we'll need to load the AWS PowerShell for .NET Core module as well as the AWS Tools for Powershell Lambda: Install-Module AWSPowerShell.NetCore -Force Install-Module AWSLambdaPSCore -Force Import-Module AWSPowerShell.NetCore Import-Module AWSLambdaPSCore Next, let's make sure our commands will run in the correct region using the Set-DefaultAWSRegion commandlet: Set-DefaultAWSRegion -Region us-east-1","title":"Opening a PowerShell Core session"},{"location":"reinvent2018/win308/challenge3/#designing-and-testing-our-lambda","text":"Since Powershell Core Commandlets can run locally as well as in the Lambda execution environment, we can write and test the script before we deploy it to Lambda. Our first task becomes identifying the EC2 instances that match our 'Production' tag. This can be accomplished using the Get-EC2Instance Commandlet. Let's test it out: Get-EC2Instance -Filter @( @ { name=' tag : Purpose ';value=' Development ' } ) Example Results GroupNames : {} Groups : {} Instances : {xxxx} OwnerId : 999999999999 RequesterId : ReservationId : r-0011223344556677 GroupNames : {} Groups : {} Instances : {yyyy} OwnerId : 999999999999 RequesterId : ReservationId : r-0011223344556677 GroupNames : {} Groups : {} Instances : {zzzz} OwnerId : 999999999999 RequesterId : ReservationId : r-0011223344556677","title":"Designing and Testing Our Lambda"},{"location":"reinvent2018/win308/challenge3/#writing-the-scripts","text":"Once we have our permissions defined, we need to create our Lambda functions. We will write two: one to list and stop instances, and another to list and stop instances. In order to make our Lambda more flexible, we will use an environment variable named 'PURPOSE' to supply the value of the \"Purpose\" tag we're filtering on. StartInstances.ps1 # Requires -Modules @ { ModuleName='AWSPowerShell.NetCore' ; ModuleVersion='3.3.390.0' } $ instances = Get-EC2Instance -Filter @( @ { name=' tag : Purpose ' ; value=\"$ env : PURPOSE_TAG \"} ) | Select-Object -ExpandProperty RunningInstance Write-Host \" Starting $ ( $ instances . Length ) instances ... \" Start-EC2Instance - InstanceId $ instances StopInstances.ps1 # Requires -Modules @ { ModuleName='AWSPowerShell.NetCore' ; ModuleVersion='3.3.390.0' } $ instances = Get-EC2Instance -Filter @( @ { name=' tag : Purpose ' ; value=\"$ env : PURPOSE_TAG \"} )|Select-Object -ExpandProperty RunningInstance Write-Host \" Stopping $ ( $ instances . Length ) instances ... \" Stop-EC2Instance - InstanceId $ instances","title":"Writing the scripts"},{"location":"reinvent2018/win308/challenge3/#publishing-the-lambda","text":"Finally, we will publish our Lambda's scripts, setting the PURPOSE_TAG environment variable's value to \"Development\": Publish-AWSPowerShellLambda -ScriptPath ./StartInstances.ps1 -IAMRoleArn StartStopInstancesRole -Name StartInstances -EnvironmentVariable @{\"PURPOSE_TAG\"=\"Development\"} Publish-AWSPowerShellLambda -ScriptPath ./StopInstances.ps1 -IAMRoleArn StartStopInstancesRole -Name StopInstances -EnvironmentVariable @{\"PURPOSE_TAG\"=\"Development\"}","title":"Publishing the Lambda"},{"location":"reinvent2018/win308/challenge3/#scheduling-our-lambdas","text":"Now we need to schedule our Lambdas. To do this, we will need to create a CloudWatch Event rule that fires the event on a schedule. You can do this via PowerShell using CloudWatch's support for Cron syntax: Write-CWERule -Name StartOfDay -ScheduleExpression \"cron(0 12 * * ? *)\" Write-CWERule -Name EndOfDay -ScheduleExpression \"cron(0 21 * * ? *)\" After the rule has been created, each of these commands will write an ARN for the new event to the console. Next, we will need to grant permission for these new events to invoke our Lambda functions. For each of the Lambdas, we will need to grant the corresponding event permission: Add-LMPermission -FunctionName StartInstances -StatementId MyStatement -Action 'lambda:InvokeFunction' -Principal events.amazonaws.com -SourceArn $(Get-CWERule StartOfDay -region us-east-1).Arn Add-LMPermission -FunctionName StopInstances -StatementId MyStatement -Action 'lambda:InvokeFunction' -Principal events.amazonaws.com -SourceArn $(Get-CWERule EndOfDay -region us-east-1).Arn Finally, we need to associate the event with our Lambda by creating a target for the events, using the name of the rule and the ARN of the function. To get the ARN of your lambda, you can use the Get-LMFunctionConfiguration Cmdlet: Get-LMFunctionConfiguration <Function name> Now that we know the ARN, let's wire up the target: Write-CWETarget -Rule <Rule name> -Target @{ ID=1;Arn =\"<Lambda ARN>\"}","title":"Scheduling our Lambdas"},{"location":"reinvent2018/win308/challenge3/#congratulations","text":"You have now created a Lambda that uses PowerShell Core to automate the management of your EC2 instances. We hope you now have a better understanding of what you can accomplish with PowerShell on AWS. Experiment with the available Commandlets to explore the options available to you (remember to adjust permissions!) or change the scheduling of function execution to suit your use case.","title":"Congratulations!"},{"location":"reinvent2018/win308/challenge3/#references","text":"Using the AWS Tools for Windows PowerShell AWS Tools for PowerShell Cmdlet Reference AWS Lambda and Tools for PowerShell AWS Lambda Permissions Model IAM Actions, Resources, and Condition Keys for Amazon EC2 Tutorial: Schedule AWS Lambda Functions Using CloudWatch Events","title":"References"},{"location":"reinvent2018/win308/prereq/","text":"For this workshop, we have provided a temporary AWS account that you will use to explore how you can use PowerShell to manage infrastructure in AWS. We have provisioned an Elastic Compute Cloud instance running Windows Server 2019 with the necessary tools installed as well as instances to use for testing. Prerequisites: A 12-digit code (aka \"hash\") provided to you by the workshop instructor. A laptop computer running Windows, OSX, or Linux A Remote Desktop Services client. If you are running Windows, you will already have one installed. If you have a Mac, you may need to install a client. You can download the Microsoft Remote Desktop Services client by clicking here (or navigate to https://itunes.apple.com/us/app/microsoft-remote-desktop-10/id1295203466 ). If you are running a Linux distribution, you may need to install a client using your Linux distribution's package manager. Connecting to your team dashboard Let's get started! Login to the team dashboard at the following URL: https://dashboard.eventengine.run/login You will be prompted to enter a 12-character hash code for your team; this will be on the card provided to you at the beginning of the workshop. Once you enter your hash code and click Proceed, you will be presented with your team dashboard. Click the \"AWS Console\" button to navigate to the credentials page for your account. Logging into the AWS Console You will see a section entitled \"login link\" with a button labeled \"Open Console.\" Click this button to open your account's console. This will bring you to the AWS Console for your temporary account. Take a moment to familiarize yourself with this page; from here, you can navigate to one of the over 165 services AWS offers. NOTE: For the purposes of this workshop, we have purposefully limited access to only the services needed. You may encounter error messages if you navigate to services other than those used for this workshop. Opening the CloudFormation dashboard We are using the CloudFormation service to automatically provision the AWS resources that you will be using for this workshop. CloudFormation allows you to create documents known as Templates which describe the AWS resources you would like to provision, along with any configuration data for those resources. When you supply a template to CloudFormation, it will create a Stack that contains the resources. We will be using CloudFormation again later on in this workshop, but for now, let's examine the stack we created for you. In the \"Find Services\" search bar, enter \"CloudFormation\" and click the link that appears below the search bar. This will bring you to the CloudFormation dashboard. Viewing the outputs of our stack At first, you will only see a single stack named \"module-XXXXXXX\" (Note: this is simply the name generated by our workshop's provisioning tool; you can provide the name for a stack when you create your own). To the right of the stack, you will see a tabbed details view. The tabs contain information regarding the stack, the events that occurred during the stack's lifecyle, the resources that were created by the stack, and any outputs that the stack returns. In our case, we're interested in the outputs, so click the Outputs tab. You will note this tab contains three output parameters; we will be using these for this workshop: * VisualStudioInstancePassword - The name of the Systems Manager Parameter that contains the administrator password for this instance * VisualStudioInstance - The DNS name of the EC2 instance running Visual Studio 2019 * BasicLambdaExecutionRoleArn - The Amazon Resource Name (ARN) of a role we created for your serverless application The first output parameter is the name of a Systems Manager parameter that contains the random password that was generated for our Visual Studio instance. Systems Manager is an AWS Service that allows you to manage, patch, and store configuration parameters for EC2 instances and other AWS resources. In this case, we're using the Parameter Store feature to store the generated password. View our Systems Manager parameter NOTE: For the purposes of this workshop, we are storing the pasword as plaintext. This is NOT considered to be a best practice; in a production environment, you would either store this as an encrypted (\"secure\") string with restricted permissions for decryption, use an RSA keypair, or join the machine to an Active Directory domain. Let's open the Systems Manager console to retrieve our password. Click the \"Services\" drop-down on the upper-left of your AWS Console. This will open a search pane. In the search box, enter \"systems manager\" and click the Systems Manager entry in the results. Pro tip: we will be returning to the CloudFormation Outputs tab in a moment; if you hold the Control key (Windows) or the Command key (Mac), you will open Systems Manager in a new tab so we don't lose our place. This will open the Systems Manager dashboard. In the left-hand pane, select \"Parameter Store\" to navigate to the parameters (you may need to scroll down). You should only see a single parameter in this list, with a name ending in \"VisualStudioPassword\". Click the parameter to view it, and copy the password to a temporary file for future reference. Connecting to our EC2 instance Now it's time to connect to the EC2 instance. We have provisioned an EC2 instance running Windows Server 2019 for this workshop, and we have installed Visual Studio 2019 Community edition for you. Let's return to the CloudFormation Outputs tab for our stack by either returning to that browser tab or by searching for CloudFormation service from the Services drop-down and clicking the Outputs tab again. Copy the value for the VisualStudioInstance parameter; this is the DNS name for the EC2 instance. Use your Remote Desktop Services client (see Prerequisites above) to connect to this instance at this address. The username for the instance will be \"Administrator\" and the password will be the value copied from the Systems Manager parameter above. Once you connect to your EC2 instance, you will see the machine's desktop.","title":"Pre-requisites"},{"location":"reinvent2018/win308/prereq/#prerequisites","text":"A 12-digit code (aka \"hash\") provided to you by the workshop instructor. A laptop computer running Windows, OSX, or Linux A Remote Desktop Services client. If you are running Windows, you will already have one installed. If you have a Mac, you may need to install a client. You can download the Microsoft Remote Desktop Services client by clicking here (or navigate to https://itunes.apple.com/us/app/microsoft-remote-desktop-10/id1295203466 ). If you are running a Linux distribution, you may need to install a client using your Linux distribution's package manager.","title":"Prerequisites: "},{"location":"reinvent2018/win308/prereq/#connecting-to-your-team-dashboard","text":"Let's get started! Login to the team dashboard at the following URL: https://dashboard.eventengine.run/login You will be prompted to enter a 12-character hash code for your team; this will be on the card provided to you at the beginning of the workshop. Once you enter your hash code and click Proceed, you will be presented with your team dashboard. Click the \"AWS Console\" button to navigate to the credentials page for your account.","title":"Connecting to your team dashboard"},{"location":"reinvent2018/win308/prereq/#logging-into-the-aws-console","text":"You will see a section entitled \"login link\" with a button labeled \"Open Console.\" Click this button to open your account's console. This will bring you to the AWS Console for your temporary account. Take a moment to familiarize yourself with this page; from here, you can navigate to one of the over 165 services AWS offers. NOTE: For the purposes of this workshop, we have purposefully limited access to only the services needed. You may encounter error messages if you navigate to services other than those used for this workshop.","title":"Logging into the AWS Console"},{"location":"reinvent2018/win308/prereq/#opening-the-cloudformation-dashboard","text":"We are using the CloudFormation service to automatically provision the AWS resources that you will be using for this workshop. CloudFormation allows you to create documents known as Templates which describe the AWS resources you would like to provision, along with any configuration data for those resources. When you supply a template to CloudFormation, it will create a Stack that contains the resources. We will be using CloudFormation again later on in this workshop, but for now, let's examine the stack we created for you. In the \"Find Services\" search bar, enter \"CloudFormation\" and click the link that appears below the search bar. This will bring you to the CloudFormation dashboard.","title":"Opening the CloudFormation dashboard"},{"location":"reinvent2018/win308/prereq/#viewing-the-outputs-of-our-stack","text":"At first, you will only see a single stack named \"module-XXXXXXX\" (Note: this is simply the name generated by our workshop's provisioning tool; you can provide the name for a stack when you create your own). To the right of the stack, you will see a tabbed details view. The tabs contain information regarding the stack, the events that occurred during the stack's lifecyle, the resources that were created by the stack, and any outputs that the stack returns. In our case, we're interested in the outputs, so click the Outputs tab. You will note this tab contains three output parameters; we will be using these for this workshop: * VisualStudioInstancePassword - The name of the Systems Manager Parameter that contains the administrator password for this instance * VisualStudioInstance - The DNS name of the EC2 instance running Visual Studio 2019 * BasicLambdaExecutionRoleArn - The Amazon Resource Name (ARN) of a role we created for your serverless application The first output parameter is the name of a Systems Manager parameter that contains the random password that was generated for our Visual Studio instance. Systems Manager is an AWS Service that allows you to manage, patch, and store configuration parameters for EC2 instances and other AWS resources. In this case, we're using the Parameter Store feature to store the generated password.","title":"Viewing the outputs of our stack"},{"location":"reinvent2018/win308/prereq/#view-our-systems-manager-parameter","text":"NOTE: For the purposes of this workshop, we are storing the pasword as plaintext. This is NOT considered to be a best practice; in a production environment, you would either store this as an encrypted (\"secure\") string with restricted permissions for decryption, use an RSA keypair, or join the machine to an Active Directory domain. Let's open the Systems Manager console to retrieve our password. Click the \"Services\" drop-down on the upper-left of your AWS Console. This will open a search pane. In the search box, enter \"systems manager\" and click the Systems Manager entry in the results. Pro tip: we will be returning to the CloudFormation Outputs tab in a moment; if you hold the Control key (Windows) or the Command key (Mac), you will open Systems Manager in a new tab so we don't lose our place. This will open the Systems Manager dashboard. In the left-hand pane, select \"Parameter Store\" to navigate to the parameters (you may need to scroll down). You should only see a single parameter in this list, with a name ending in \"VisualStudioPassword\". Click the parameter to view it, and copy the password to a temporary file for future reference.","title":"View our Systems Manager parameter"},{"location":"reinvent2018/win308/prereq/#connecting-to-our-ec2-instance","text":"Now it's time to connect to the EC2 instance. We have provisioned an EC2 instance running Windows Server 2019 for this workshop, and we have installed Visual Studio 2019 Community edition for you. Let's return to the CloudFormation Outputs tab for our stack by either returning to that browser tab or by searching for CloudFormation service from the Services drop-down and clicking the Outputs tab again. Copy the value for the VisualStudioInstance parameter; this is the DNS name for the EC2 instance. Use your Remote Desktop Services client (see Prerequisites above) to connect to this instance at this address. The username for the instance will be \"Administrator\" and the password will be the value copied from the Systems Manager parameter above. Once you connect to your EC2 instance, you will see the machine's desktop.","title":"Connecting to our EC2 instance"},{"location":"reinvent2018/win308/resources/","text":"","title":"Resources"},{"location":"reinvent2018/win308/win308index/","text":"WIN308 - Hands-On: Automating AWS Infrastructure with PowerShell Welcome! In this Workshop, we will introduce you to the AWS Tools for Windows PowerShell/PowerShell Core and discuss the fundamentals for automating infrastructure on AWS using PowerShell. We have provided a set of challenges that you can choose from as a way to help you learn how you can use PowerShell to automate infrastructure on AWS in a variety of real-world scenarios. You can see these challenges in the table of contents to the left of this page. Please review the Prequisites section for this workshop by clicking the Next navigation button below.","title":"Index"},{"location":"reinvent2018/win308/win308index/#win308-hands-on-automating-aws-infrastructure-with-powershell","text":"","title":"WIN308 - Hands-On: Automating AWS Infrastructure with PowerShell"},{"location":"reinvent2018/win308/win308index/#welcome","text":"In this Workshop, we will introduce you to the AWS Tools for Windows PowerShell/PowerShell Core and discuss the fundamentals for automating infrastructure on AWS using PowerShell. We have provided a set of challenges that you can choose from as a way to help you learn how you can use PowerShell to automate infrastructure on AWS in a variety of real-world scenarios. You can see these challenges in the table of contents to the left of this page. Please review the Prequisites section for this workshop by clicking the Next navigation button below.","title":"Welcome!"},{"location":"reinvent2018/win308/solutions/challenge1/","text":"Challenge 1: Configuring resources using AWS Tools for Windows PowerShell Task 1 : All instances are missing the tag \"Cost Center : IT\" In order to make this change we need to create a list of instances which don\u2019t have the tag \u201cCost Center\u201d, create the tag \u201cCost Center : IT\u201d and then assign this tag to our group. # list all instances without the tag \"Cost Center\" $instancelist = ( Get-EC2Instance ). Instances | ? { $_ . Tag . key -ne \"Cost Center\" } # Create a new tag \"Cost Center : IT\" $tag = New-Object Amazon . EC2 . Model . Tag $tag . Key = \"Cost Center\" $tag . Value = \"IT\" # assign the tag \"Cost Center : IT\" to instances $instancelist | % { New-EC2Tag -Resource $_ . InstanceId -Tag $tag } We can now verify our change by listing all instances which this time has the tag \u201cCost Center : IT\u201d This can be confirmed from the tag tab on any of the instances: Task 2: All instances without the tag \"Tier : WebServer\" should be protected against termination #list all instances without the tag \"Tier : WebServer\" $instancelist = ( Get-EC2Instance ). Instances | ? {!( $_ . Tag . key -eq \"Tier\" -and $_ . Tag . Value -eq \"WebServer\" )} #protecting against termination $instancelist | % { Edit-EC2InstanceAttribute -InstanceId $_ . InstanceId -DisableApiTermination $true } You can verify the termination protection from the ec2 console","title":"Challenge  1: Configuring resources using AWS Tools for Windows PowerShell"},{"location":"reinvent2018/win308/solutions/challenge1/#challenge-1-configuring-resources-using-aws-tools-for-windows-powershell","text":"","title":"Challenge  1: Configuring resources using AWS Tools for Windows PowerShell"},{"location":"reinvent2018/win308/solutions/challenge1/#task-1-all-instances-are-missing-the-tag-cost-center-it","text":"In order to make this change we need to create a list of instances which don\u2019t have the tag \u201cCost Center\u201d, create the tag \u201cCost Center : IT\u201d and then assign this tag to our group. # list all instances without the tag \"Cost Center\" $instancelist = ( Get-EC2Instance ). Instances | ? { $_ . Tag . key -ne \"Cost Center\" } # Create a new tag \"Cost Center : IT\" $tag = New-Object Amazon . EC2 . Model . Tag $tag . Key = \"Cost Center\" $tag . Value = \"IT\" # assign the tag \"Cost Center : IT\" to instances $instancelist | % { New-EC2Tag -Resource $_ . InstanceId -Tag $tag } We can now verify our change by listing all instances which this time has the tag \u201cCost Center : IT\u201d This can be confirmed from the tag tab on any of the instances:","title":"Task 1 : All instances are missing the tag \"Cost Center : IT\""},{"location":"reinvent2018/win308/solutions/challenge1/#task-2-all-instances-without-the-tag-tier-webserver-should-be-protected-against-termination","text":"#list all instances without the tag \"Tier : WebServer\" $instancelist = ( Get-EC2Instance ). Instances | ? {!( $_ . Tag . key -eq \"Tier\" -and $_ . Tag . Value -eq \"WebServer\" )} #protecting against termination $instancelist | % { Edit-EC2InstanceAttribute -InstanceId $_ . InstanceId -DisableApiTermination $true } You can verify the termination protection from the ec2 console","title":"Task 2: All instances without the tag \"Tier : WebServer\" should be protected against termination"},{"location":"reinvent2018/win308/solutions/challenge2/","text":"Challenge 2 They want to make sure that IIS is installed on all the Instances that have Role tag as with value of Frontend . For this they want to use AWS SSM run command. Make sure all the instances are meeting AWS SSM requirements . From AWS Systems Manager console, click on Run command. Look for AWS-RunPowerShellScript in the Document list. Under command Parameters, enter the following Powershell code to install IIS. Install-WindowsFeature -Name Web-Server -IncludeManagementTools For Targets select Specifying a tag and then for the tag key put 'Role' and for the value specify 'Frontend' and hit Add. Click on Run Wait for the command to finish and view the command output to make sure IIS was insatlled successfully. Alternate method: Send-SSMCommand -DocumentName \"AWS-RunPowerShellScript\" -Parameter @ { commands = \"Install-WindowsFeature -Name Web-Server -IncludeManagementTools\" } -Target @( @ { Key=\" tag : Role \"; values=\" Frontend \" } )","title":"Challenge2"},{"location":"reinvent2018/win308/solutions/challenge2/#challenge-2","text":"They want to make sure that IIS is installed on all the Instances that have Role tag as with value of Frontend . For this they want to use AWS SSM run command. Make sure all the instances are meeting AWS SSM requirements . From AWS Systems Manager console, click on Run command. Look for AWS-RunPowerShellScript in the Document list. Under command Parameters, enter the following Powershell code to install IIS. Install-WindowsFeature -Name Web-Server -IncludeManagementTools For Targets select Specifying a tag and then for the tag key put 'Role' and for the value specify 'Frontend' and hit Add. Click on Run Wait for the command to finish and view the command output to make sure IIS was insatlled successfully. Alternate method: Send-SSMCommand -DocumentName \"AWS-RunPowerShellScript\" -Parameter @ { commands = \"Install-WindowsFeature -Name Web-Server -IncludeManagementTools\" } -Target @( @ { Key=\" tag : Role \"; values=\" Frontend \" } )","title":"Challenge 2"},{"location":"reinvent2018/win308/solutions/challenge3/","text":"Challenge 3: A Solution (unless you know a better one!) Testing the approach Since Powershell Core Commandlets can run locally as well as in the Lambda execution environment, we can write and test the script before we deploy it to Lambda. Our first task becomes identifying the EC2 instances that match our 'Production' tag. This can be accomplished using the Get-EC2Instance Commandlet. Let's test it out: Get-EC2Instance -Filter @( @ { name=' tag : Purpose ';value=' Development ' } ) Example Results GroupNames : {} Groups : {} Instances : {xxxx} OwnerId : 999999999999 RequesterId : ReservationId : r-0011223344556677 GroupNames : {} Groups : {} Instances : {yyyy} OwnerId : 999999999999 RequesterId : ReservationId : r-0011223344556677 GroupNames : {} Groups : {} Instances : {zzzz} OwnerId : 999999999999 RequesterId : ReservationId : r-0011223344556677 Defining Permissions Once we know our command works, we can deploy it as a Lambda. But first, we need to define a set of permissions that grants the ability to list EC2 instances, as well as start and stop instances. This requires that we create a role with an attached policy. Ideally, we'd create two roles: one that has the ability to list & start EC2 instances and another with permission to list & stop instances, but for simplicity's sake we'll define it as a single policy and use it for both Lambdas. In the example below, we are defining a policy that grants access to write logs to CloudWatch, the ability to start and stop EC2 instances, and the ability to list EC2 instances: StartStopInstancesRole { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"LambdaLogging\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:CreateLogGroup\" , \"logs:CreateLogStream\" , \"logs:PutLogEvents\" ], \"Resource\" : \"arn:aws:logs:*:*:*\" }, { \"Sid\" : \"StartStopInstances\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:StartInstances\" , \"ec2:StopInstances\" ], \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" }, { \"Sid\" : \"DescribeInstances\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DescribeInstances\" , \"Resource\" : \"*\" } ] } Writing the scripts Once we have our permissions defined, we need to create our Lambda functions. We will write two: one to list and stop instances, and another to list and stop instances. In order to make our Lambda more flexible, we will use an environment variable named 'PURPOSE' to supply the value of the \"Purpose\" tag we're filtering on. StartInstances.ps1 # Requires -Modules @ { ModuleName='AWSPowerShell.NetCore' ; ModuleVersion='3.3.390.0' } $ instances = Get-EC2Instance -Filter @( @ { name=' tag : Purpose ' ; value=\"$ env : PURPOSE_TAG \"} ) | Select-Object -ExpandProperty RunningInstance Write-Host \" Stopping $ ( $ instances . Length ) instances ... \" Start-EC2Instance - InstanceId $ instances StopInstances.ps1 # Requires -Modules @ { ModuleName='AWSPowerShell.NetCore' ; ModuleVersion='3.3.390.0' } $ instances = Get-EC2Instance -Filter @( @ { name=' tag : Purpose ' ; value=\"$ env : PURPOSE_TAG \"} )|Select-Object -ExpandProperty RunningInstance Write-Host \" Stopping $ ( $ instances . Length ) instances ... \" Stop-EC2Instance - InstanceId $ instances Publishing the Lambda Finally, we will publish our Lambdas: Publish-AWSPowerShellLambda -ScriptPath ./StartInstances.ps1 -IAMRoleArn <Your role's name> -Name StartInstances -EnvironmentVariable @{\"PURPOSE\"=\"Development\"} Publish-AWSPowerShellLambda -ScriptPath ./StopInstances.ps1 -IAMRoleArn <Your role's name> -Name StopInstances -EnvironmentVariable @{\"PURPOSE\"=\"Development\"} Scheduling our Lambdas Now we need to schedule our Lambdas. To do this, we will need to create a CloudWatch Event rule that fires the event on a schedule. You can do this via the console or PowerShell using CloudWatch's support for Cron syntax: Write-CWERule -Name StartOfDay -ScheduleExpression \"cron(0 12 * * ? *)\" Write-CWERule -Name EndOfDay -ScheduleExpression \"cron(0 21 * * ? *)\" Each of these will write an ARN for the new event to the console. Next, we will need to grant permission for these new events to invoke our Lambda functions. For each of the Lambdas, we will need to grant the corresponding event permission: Add-LMPermission -FunctionName <Lambda name> -StatementId MyStatement -Action 'lambda:InvokeFunction' -Principal events.amazonaws.com -SourceArn <your event ARN> Finally, we need to associate the event with our Lambda by creating a target for the events, using the name of the rule and the ARN of the function. To get the ARN of your lambda, you can use the Get-LMFunctionConfiguration Cmdlet: Get-LMFunctionConfiguration <Function name> Now that we know the ARN, let's wire up the target: Write-CWETarget -Rule <Rule name> -Target @{ ID=1;Arn =\"<Lambda ARN>\"}","title":"Challenge 3: A Solution (unless you know a better one!)"},{"location":"reinvent2018/win308/solutions/challenge3/#challenge-3-a-solution-unless-you-know-a-better-one","text":"","title":"Challenge 3: A Solution (unless you know a better one!)"},{"location":"reinvent2018/win308/solutions/challenge3/#testing-the-approach","text":"Since Powershell Core Commandlets can run locally as well as in the Lambda execution environment, we can write and test the script before we deploy it to Lambda. Our first task becomes identifying the EC2 instances that match our 'Production' tag. This can be accomplished using the Get-EC2Instance Commandlet. Let's test it out: Get-EC2Instance -Filter @( @ { name=' tag : Purpose ';value=' Development ' } ) Example Results GroupNames : {} Groups : {} Instances : {xxxx} OwnerId : 999999999999 RequesterId : ReservationId : r-0011223344556677 GroupNames : {} Groups : {} Instances : {yyyy} OwnerId : 999999999999 RequesterId : ReservationId : r-0011223344556677 GroupNames : {} Groups : {} Instances : {zzzz} OwnerId : 999999999999 RequesterId : ReservationId : r-0011223344556677","title":"Testing the approach"},{"location":"reinvent2018/win308/solutions/challenge3/#defining-permissions","text":"Once we know our command works, we can deploy it as a Lambda. But first, we need to define a set of permissions that grants the ability to list EC2 instances, as well as start and stop instances. This requires that we create a role with an attached policy. Ideally, we'd create two roles: one that has the ability to list & start EC2 instances and another with permission to list & stop instances, but for simplicity's sake we'll define it as a single policy and use it for both Lambdas. In the example below, we are defining a policy that grants access to write logs to CloudWatch, the ability to start and stop EC2 instances, and the ability to list EC2 instances: StartStopInstancesRole { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"LambdaLogging\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:CreateLogGroup\" , \"logs:CreateLogStream\" , \"logs:PutLogEvents\" ], \"Resource\" : \"arn:aws:logs:*:*:*\" }, { \"Sid\" : \"StartStopInstances\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:StartInstances\" , \"ec2:StopInstances\" ], \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" }, { \"Sid\" : \"DescribeInstances\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DescribeInstances\" , \"Resource\" : \"*\" } ] }","title":"Defining Permissions"},{"location":"reinvent2018/win308/solutions/challenge3/#writing-the-scripts","text":"Once we have our permissions defined, we need to create our Lambda functions. We will write two: one to list and stop instances, and another to list and stop instances. In order to make our Lambda more flexible, we will use an environment variable named 'PURPOSE' to supply the value of the \"Purpose\" tag we're filtering on. StartInstances.ps1 # Requires -Modules @ { ModuleName='AWSPowerShell.NetCore' ; ModuleVersion='3.3.390.0' } $ instances = Get-EC2Instance -Filter @( @ { name=' tag : Purpose ' ; value=\"$ env : PURPOSE_TAG \"} ) | Select-Object -ExpandProperty RunningInstance Write-Host \" Stopping $ ( $ instances . Length ) instances ... \" Start-EC2Instance - InstanceId $ instances StopInstances.ps1 # Requires -Modules @ { ModuleName='AWSPowerShell.NetCore' ; ModuleVersion='3.3.390.0' } $ instances = Get-EC2Instance -Filter @( @ { name=' tag : Purpose ' ; value=\"$ env : PURPOSE_TAG \"} )|Select-Object -ExpandProperty RunningInstance Write-Host \" Stopping $ ( $ instances . Length ) instances ... \" Stop-EC2Instance - InstanceId $ instances","title":"Writing the scripts"},{"location":"reinvent2018/win308/solutions/challenge3/#publishing-the-lambda","text":"Finally, we will publish our Lambdas: Publish-AWSPowerShellLambda -ScriptPath ./StartInstances.ps1 -IAMRoleArn <Your role's name> -Name StartInstances -EnvironmentVariable @{\"PURPOSE\"=\"Development\"} Publish-AWSPowerShellLambda -ScriptPath ./StopInstances.ps1 -IAMRoleArn <Your role's name> -Name StopInstances -EnvironmentVariable @{\"PURPOSE\"=\"Development\"}","title":"Publishing the Lambda"},{"location":"reinvent2018/win308/solutions/challenge3/#scheduling-our-lambdas","text":"Now we need to schedule our Lambdas. To do this, we will need to create a CloudWatch Event rule that fires the event on a schedule. You can do this via the console or PowerShell using CloudWatch's support for Cron syntax: Write-CWERule -Name StartOfDay -ScheduleExpression \"cron(0 12 * * ? *)\" Write-CWERule -Name EndOfDay -ScheduleExpression \"cron(0 21 * * ? *)\" Each of these will write an ARN for the new event to the console. Next, we will need to grant permission for these new events to invoke our Lambda functions. For each of the Lambdas, we will need to grant the corresponding event permission: Add-LMPermission -FunctionName <Lambda name> -StatementId MyStatement -Action 'lambda:InvokeFunction' -Principal events.amazonaws.com -SourceArn <your event ARN> Finally, we need to associate the event with our Lambda by creating a target for the events, using the name of the rule and the ARN of the function. To get the ARN of your lambda, you can use the Get-LMFunctionConfiguration Cmdlet: Get-LMFunctionConfiguration <Function name> Now that we know the ARN, let's wire up the target: Write-CWETarget -Rule <Rule name> -Target @{ ID=1;Arn =\"<Lambda ARN>\"}","title":"Scheduling our Lambdas"},{"location":"reinvent2018/win309/challenge1/","text":"","title":"Challenge1"},{"location":"reinvent2018/win309/prereq/","text":"WORKSHOP PREREQUISITES In order to run this Workshop, you will need the following: Laptop/tablet that is connected to the reinvent 2018 Wi-Fi network. Microsoft Remote Desktop Client which you can find via this link Remote Desktop clients Let\u2019s get you logged into the AWS Account which you will use throughout the workshop. At your desk you will find a sticker with an account number on. Enter the following address into a browser now. This will bring you to the standard AWS login page. You will need to enter the following information before clicking Login. Account Name - Username \u2013 win309 Password - reInvent2@18! The AWS region we are using for this workshop is US-EAST-2 ( OHIO ) Please make sure that US-EAST-2 is showing in the top right of the console. QUICKTASKS At the start of each lab we have a Quicktask section. If you already have experience using AWS, you may save time by performing the actions on your own. Alternatively, feel free to follow the step-by-step instructions.","title":"Prerequisites"},{"location":"reinvent2018/win309/prereq/#workshop-prerequisites","text":"In order to run this Workshop, you will need the following: Laptop/tablet that is connected to the reinvent 2018 Wi-Fi network. Microsoft Remote Desktop Client which you can find via this link Remote Desktop clients Let\u2019s get you logged into the AWS Account which you will use throughout the workshop. At your desk you will find a sticker with an account number on. Enter the following address into a browser now. This will bring you to the standard AWS login page. You will need to enter the following information before clicking Login. Account Name - Username \u2013 win309 Password - reInvent2@18!","title":"WORKSHOP PREREQUISITES"},{"location":"reinvent2018/win309/prereq/#the-aws-region-we-are-using-for-this-workshop-is-us-east-2-ohio","text":"Please make sure that US-EAST-2 is showing in the top right of the console.","title":"The AWS region we are using for this workshop is US-EAST-2        ( OHIO )"},{"location":"reinvent2018/win309/prereq/#quicktasks","text":"At the start of each lab we have a Quicktask section. If you already have experience using AWS, you may save time by performing the actions on your own. Alternatively, feel free to follow the step-by-step instructions.","title":"QUICKTASKS"},{"location":"reinvent2018/win309/resources/","text":"LAB A \u2013 Active Directory running on EC2 GOAL In this lab you are going to extend an on-premise Active Directory into AWS. You will be using EC2, AWS AD Connector and AWS Systems Manager services to complete this Lab. LAB A \u2013 TASKS 1 thru 9 \u2013 QUICKTASK Familiarize yourself with the on-premise ACME.com domain via its management server\u2019s public-IP address (named Management Server in the console). Then we would like you to create a Server 2016 instance in VPC01 Private Subnet, use the AWS AD Connector to join it to the ACME.com domain (residing in VPC02), and then promote it to a domain controller. LAB A - TASK 1 Let\u2019s familiarize ourselves with the environment. As we mentioned, VPC02 is a simulated on-premise environment. We are going to connect via Remote Desktop (RDP) to the Management Server using its external IP address. Once connected, we will verify that the settings are correct and that the AD management tools are installed. Go to the AWS Console you launched and complete the following steps: Click on EC2 In the center of the screen under Resources, click on Running Instances Select the instance called Management Server You\u2019ll find the Public IP address on the bottom detail screen. Make a note of this address. LAB A \u2013 TASK 2 Connect to the EC2 Instance from TASK 1-1 using Microsoft Remote Desktop You can either do this via launching the Microsoft Remote Desktop app and adding an entry for a new host or via the command line with the following MSTSC /v: IP address of EC2 Instance Please call out to one of the support team in the room if you are having issues. Once connected, you\u2019ll need account details to login Username = acme\\administrator Password = @Pa55w0rd159@ Let\u2019s look at Administrative Tools which you will find in the Start menu. Locate Active Directory Users and Computers app, launch it. Take a look around the ACME.COM domain, you will see a Domain Controller called ADSERVER1 but apart from that, it\u2019s a brand new domain. Please close down the remote session to this server and continue with the next task. TASK 2.5 \u2013 Create an IAM role IAM Roles are configured with permissions that allow a service, assuming the role, to perform actions permitted by that role. Amazon EC2 Roles allow Amazon EC2 instances to assume a role so that applications on the instance can make secure API calls to AWS. In this task, we will create an EC2 role which will be used by the SSM agent on our instance to perform the actions it requires with Directory Services. Back at the Home screen of the AWS Management Console, click IAM Select Roles from the left side menu and click on Create role Select AWS service and then EC2 Select EC2 Role for Simple Systems Manager as your use case and click Next:Permissions The AmazonEC2RoleforSSM should be selected Click Next: Review Enter EC2RoleforSSM as the name for the role Click Create Role LAB A \u2013 TASK 3 In this task we will create a new Windows 2016 EC2 Instance using the standard Amazon Machine Image (AMI) and then in later tasks we will configure this instance to be an AD Domain Controller. This new EC2 Instance will be on the AWS side in VPC01 . In later tasks we will extend the secondary domain into this EC2 Instance. In the AWS Console, click on EC2 Next click on Launch Instance Scroll down the list of Amazon Machine Images until you find Microsoft Windows Server 2016 Base and then click Select On the Choose an Instance Type screen select T2.Large and click Next: Configure Instance Details You will need to make sure you select the following on the Configure Instance Details screen NETWORK = VPC01-VPC SUBNET = VPC01-SNEXTPUB1A AUTO-ASSIGN PUBLIC IP = Enable IAM Role = AmazonEC2RoleforSSM You can leave all other parameters at their default. Click Next: Add Storage Click Next: Add Tags Click Next: Add Security Groups Click Add Rule Make sure that an RDP rule for port 3389 with a Source of My IP is present NOTE: We advise that you restrict access of RDP Traffic to your own network Click Review and Launch On the Review Instance Launch screen click Launch On the Select an existing key pair screen ensure that you select Create a new key pair , give it a Key Pair Name and ensure that you Download the file to your laptop. Then click Launch Instances LAB A \u2013 TASK 4 In this task we are going to use the AWS AD Connector Service to allow us to connect our newly created EC2 Instance to the ACME.COM domain that resides in VPC02 . In the AWS Console, click on Directory Service On the Directories screen click on Set up directory On the Select directory type screen select AD Connector and click Next On the Enter AD Connector information screen select Directory size of Small and click Next On the Choose VPC and subnets screen enter the following: VPC \u2013 VPC01-VPC Subnets \u2013 VPC01-SNEXTPUB1A VPC01-SNEXTPUB1B Click Next On the Active Directory Information screen enter the following details: Directory DNS name \u2013 acme.com Directory NetBIOS name \u2013 acme DNS IP Address \u2013 10.1.16.5 Service account username \u2013 administrator Service account password \u2013 @Pa55w0rd159@ Click Next Finally, on the Review and Create screen, check all settings and then click Create directory NOTE: It is best practice to reduce the permissions assigned to the Service Account used during the AWS AD Connector setup. The following link has best practice guidelines: https://docs.aws.amazon.com/directoryservice/latest/admin-guide/prereq_connector.html?icmpid=docs_ds_console_help_panel#connect_delegate_privileges TASK 4.5 \u2013 Create a new DHCP Options set DHCP Option Sets cannot be modified. If you require your VPC to use a different set of DHCP options, you must create a new set and associate with your VPC. In this section, you will create a new DHCP Option Set which will be used by EC2 instances launched into VPC01. In the AWS Management Console on the services menu, go back to the home screen and then click on VPC Choose DHCP Option Sets from the side menu Choose Create DHCP options set Enter these values in to the relevant fields: Name Tag = dopt-acme Domain name = acme.com Domain name servers = 10.1.16.5 NTP servers = (leave blank) NetBIOS servers = (leave blank) NetBIOS node type = (leave blank) Choose Yes, Create TASK 4.6 \u2013 Associate the new DHCP Options set with your VPC The next step is to associate our newly created DHCP Option Set with our VPC. After you associate a new set of DHCP options with a VPC, any existing instances and all new instances that you launch in the VPC use these options. You don't need to restart or relaunch the instances. Go back to the VPC services menu Select Your VPCs from the left menu and then select the VPC labelled VPC01-VPC Click on Actions and Select Edit DHCP Options Set From the list, select the DHCP Option Set with the name given in Task 4.5 dopt-acme Click Save LAB A \u2013 TASK 5 You can download the Port Verification Tool to troubleshoot AD Connector or Microsoft AD Trust setup issues from here: Directory Service Port Test The AD Connector Port Verification Tool does two things: Determines if the necessary ports are open from the VPC to your domain Verifies the minimum forest and domain functional levels. Usage: DirectoryServicePortTest.exe -d <domain_name> -ip <server_IP_address> -tcp \"53,88,135,389,445,464,3268,3269,5722,9389\" -udp \"53,88,123,138,389,445\" Start a Remote Desktop session with your newly created EC2 Instance. Start Internet Explorer and browse to the following link. https://docs.aws.amazon.com/directoryservice/latest/admin-guide/samples/DirectoryServicePortTest.zip If Internet Explorer Enhanced Security Configuration is enabled Click OFF for Administrators option. Click ok. f. Restart Internet Explorer. Download the tool, and extract the .zip file to the C:\\TEMP folder. Start a Command Prompt prompt via the Start Menu. Change to the C:\\TEMP folder via CD C:\\TEMP Enter the following \u2013 directoryserviceporttest -d acme.com \u2013 ip \u201c10.1.16.5\u201d You should see the following confirmation LAB A \u2013 TASK 6 In this task we will perform the simple task of joining our newly created EC2 Instance to the ACME.COM domain. There are numerous ways of doing this such as - Adding a Powershell script to the USERDATA section when launching the EC2 Instance - Using the AWS Systems Manager \u2018Run Command\u2019 service to run a Powershell script - Use the new AWS Systems Manager \u2018Session Manager\u2019 to create a session with the instance Since you may not have had the chance to use Session Manager yet, lets choose this option. In the AWS Console search for Systems Manager under Management Tools and select. AWS Systems Manager gives you a lot of functionality that will help you manage your long-running AWS EC2 Instances. On the left hand side menu choose Session Manager This screen gives you details of previous used sessions and also you can configure session-logging in the Preferences tab. Click Start Session There should be three EC2 instances viable in the Target Instances screen. Select the Instance ID that corresponds to your newly created instance and select using the radio-button on the left and then click the Start Session button. A Powershell Prompt will appear in your browser. Type or paste the following \u2013 please enter each line individually Set-DNSClientServerAddress -InterfaceAlias \u201cEthernet\u201d -ServerAddresses (\u201c10.1.16.5\u201d) $domain = \"acme.com\" $password = \"@Pa55w0rd159@\u201d | ConvertTo-SecureString -asPlainText -Force $username = \"$domain\\administrator\" $credential = New-Object System.Management.Automation.PSCredential($username,$password) Add-Computer -DomainName $domain -Credential $credential -restart You should get the following response The EC2 Instance restarts and joins the domain ACME.COM. After the restart see if you can connect it using the ACME.COM credentials. LAB A \u2013 TASK 7 In this task we will promote our newly created instance to become a Domain Controller in the ACME.COM domain. We will use AWS Systems Manager\u2019s Session Manager service once again. Back to the AWS Console, find and select AWS Systems Manager In the AWS Systems Manager screen, select Session Manager on the left. Click Start Session Select the EC2 we created in TASK 3 and click Start Session Type or cut and paste the following Powershell into the Session. Please enter the commands Line by Line Add-WindowsFeature AD-Domain-Services, RSAT-AD-AdminCenter,RSAT-ADDS-Tools $domain = \"acme.com\" $password = \"@Pa55w0rd159@\u201d | ConvertTo-SecureString -asPlainText -Force $username = \"$domain\\administrator\" $credential = New-Object System.Management.Automation.PSCredential($username,$password) install-addsdomaincontroller -installdns -Credential $credential -domainname acme.com You\u2019ll be asked to provide a SafeMode Administrator Password. Type the following ReInvent2018 You\u2019ll receive the following message \u2013 enter Y (for Yes) Congratulations ! We\u2019ve used AWS AD Connector to extend on on-premise Active Directory into AWS and then created a domain controller in the ACME.COM domain LAB B \u2013 Using the AWS Directory Service for Active Directory GOAL In this lab you are going to configure the instance of AWS Directory Services that has already been created for you. You will then create EC2 Instances which are automatically domain joined when they start. STRETCH GOAL You will configure federated Single Sign on to the AWS Console for Active Directory users. We will also Share our AWS Directory Services domain into a separate VPC. QUICKTASK In this next lab you will complete the configuration of AWS Directory Services for Active Directory This includes setting DHCP Option Sets in VPC01 and configuring a AWS Systems Manager role that will enable an EC2 Instance to Auto-Join the corp.example.com domain when it launches. After that is complete we will configure a Domain Trust with ACME.COM Gathering information Before we begin, we need to gather some information that will be required throughout the tasks ahead. In the AWS Console, select Directory Service from the Security, Identity & Compliance section of services. Or just type Directory in the Console search bar and select the Directory Service. You will see a single Directory already created for you. Click on the Directory ID link. On the next screen, you'll find details of the Directory. Make a note of the following items... Directory ID DNS Address (x2) The DNS addresses (x2) are the individual IP addresses of each domain controller pre-created in separate Availability Zones inside a single VPC. TASK 1 - Configuring a VPC to use your DNS servers The Dynamic Host Configuration Protocol (DHCP) provides a standard for passing configuration information to hosts on a TCP/IP network. The options field of a DHCP message contains the configuration parameters. Some of those parameters are the domain name, domain name server, and the NetBIOS-node-type. DHCP Option Sets are associated with your virtual private cloud (VPC). The default DHCP Option Set resolves DNS requests via Route 53. With Windows we need our instances to resolve their DNS via our Domain Controllers. In this lab we will create a new DHCP Option Set and redirect DNS requests to our AWS Directory Service. TASK 2 \u2013 Create a new DHCP Options set DHCP Option Sets cannot be modified. If you require your VPC to use a different set of DHCP options, you must create a new set and associate with your VPC. In this section, you will create a new DHCP Option Set which will be used by EC2 instances launched into VPC01. In the AWS Management Console on the services menu, go back to the home screen and then click on VPC Choose DHCP Option Sets from the side menu Choose Create DHCP options set Enter these values in to the relevant fields: Name Tag = dopt-corp Domain name = corp.example.com Domain name servers = (The two IP addresses for your AWS Directory Services DNS Servers that you noted earlier. Use a comma to separate them) NTP servers = (leave blank) NetBIOS servers = (leave blank) NetBIOS node type = (leave blank) Choose Yes, Create TASK 3 \u2013 Associate the new DHCP Options set with your VPC The next step is to associate our newly created DHCP Option Set with our VPC. After you associate a new set of DHCP options with a VPC, any existing instances and all new instances that you launch in the VPC use these options. You don't need to restart or relaunch the instances. Go back to the VPC services menu Select Your VPCs from the left menu and then select the VPC labelled VPC01-VPC Click on Actions and Select Edit DHCP Options Set From the list, select the DHCP Option Set with the name given in Task 1 dopt-corp Click Save TASK 4 - Domain Join Part 1 Overview For our second task, we are going to configure AD domain auto-join. This requires the creation of an SSM Document and IAM role. The process is simplified as AWS will automatically create the SSM document for you when you first launch an instance. The steps below take you through the creation of the IAM role, and then finally we launch an Amazon EC2 instance that will automatically join the corp.example.com domain running on AWS Directory Services. TASK 5 \u2013 Create an IAM role If you worked through LAB A first then you will have already created an IAM role called EC2RoleforSSM. You can then skip this task. If not, please follow the instructions below. IAM Roles are configured with permissions that allow a service, assuming the role, to perform actions permitted by that role. Amazon EC2 Roles allow Amazon EC2 instances to assume a role so that applications on the instance can make secure API calls to AWS. In this task, we will create an EC2 role which will be used by the SSM agent on our instance to perform the actions it requires with Directory Services. Back at the Home screen of the AWS Management Console, click IAM Select Roles from the left side menu and click on Create role Select AWS service and then EC2 Select EC2 Role for Simple Systems Manager as your use case and click Next:Permissions The AmazonEC2RoleforSSM should be selected Click Next: Review Enter EC2RoleforSSM as the name for the role Click Create Role TASK 6 \u2013 Launch an Instance In this task, we will launch a new instance into VPC01 and attach the IAM role that we created in the previous task. On the Services menu, click EC2 Select Launch Instance Select Microsoft Windows Server 2016 Base as the AMI Choose t2. medium as the instance type Click Next:Configure Instance Details In the Configure Instance Details Screen, ensure the following settings: Network VPC, select VPC01 Subnet, select VPC01-SNEXTPUB1A Auto-Assign Public-IP should be set to ENABLE Domain join directory should be set to corp.example.com I AM role should be set to EC2RoleforSSM (which we previously created in TASK 5) Click Advanced Details at the bottom of the screen which will show the Userdata section Copy and Paste the following PowerShell code into the Userdata section which will install the required management tools <powershell> Import-Module Servermanager Install-WindowsFeature RSAT, RSAT-DNS-Server </powershell> Select Review and Launch You'll receive a security warning that we are not using Free Usage Tier. Click Launch . Select the existing Key Pair from LAB A. Acknowledge via the check-box and click Launch Instances NOTE: Make a note of the Instance-ID of the new EC2 instance you've just launched Whilst our Amazon EC2 instance is launching, let's complete another task before returning to the instance. TASK 7 \u2013 AWS Directory Service Event Notification As a Windows server administrator, understanding when you have an issue with a domain controller is vital. Windows Server has a built-in logging system via Event Viewer, but to be notified when the AWS Directory Service itself has a problem we create a notification using SNS. Configure SNS 1. Go to the Home screen of the AWS console Select Directory Service On the Directories page, choose the directory ID for the corp.example.com domain Choose Monitoring and then Create Notification Choose Create a new notification Choose Recipient Type as Email and enter an email address that is accessible to you during the workshop Choose Advanced Options and ensure your SNS topic starts with DirectoryMonitoring Choose Add TASK 8 \u2013 Confirm Subscription You will receive an email asking you to confirm your subscription to the SNS notification. Click the link to confirm the subscription. TASK 9 \u2013 Domain Join Part 2 Verify your instance is joined to the domain Switching back to the Amazon EC2 instance we previously launched in Task 2. Let's find the Instance-ID in Amazon EC2 Services and note the Public IP of the instance. Using the Remote Desktop client for your machine, connect to the EC2 instance Public IP address NOTE: You will receive a Certificate error when connecting to the instance, select continue Login to the server using the corp.example.com domain account using the details below: Username: corp.example.com\\admin Password: @Pa55w0rd159@ (case-sensitive) This will demonstrate that your EC2 instance is joined to the corp.example.com domain NOTE: If you fail to connect, check the status of the EC2 instance in the console to ensure it has finished launching. TASK 10 \u2013 Configuring DNS resolution In the first task, we created a new DHCP options set and associated it with VPC01. This option set will allow instances launched into VPC01 to resolve DNS hostnames for corp.example.com . In this task, we will configure DNS to allow our AWS resources to resolve DNS hostnames for acme.com domain as well as vice versa. We need to make changes to the existing security group attached to the Directory Services domain controllers, this will allow the outbound connectivity required between the Directory Services located in VPC01 and resources in VPC02. On the Services menu, click EC2 On the sidebar, select Security Groups You should see a number of security groups already created. One will have the Group Name that will have a suffix of controllers. This is the security group automatically created when we launched AWS Directory Services Select this Security Group and select the Outbound tab Click Edit and add a new Security group rule as per the following: Type = ALL Traffic Protocol = ALL Destination = 10.1.0.0/16 Click Save TASK 11 - Continuing DNS Configuration If you have closed your existing remote desktop connection, using the Remote Desktop client for your machine, connect to the EC2 instance public IP address you previously created in task 2.2 Login to the server using the corp.example.com domain account using the details below: Username: corp.example.com\\admin Password: @Pa55w0rd159@ (case-sensitive) Once successfully logged in, go to Windows Administrative Tools on the Start Menu and select DNS to launch the Windows DNS Manager NOTE: Ensure that the DNS server you connect to is one of the AWS Directory Services instances which you noted at the start of the lab. In the left-hand side navigation window make sure that you have selected the DNS Server We are going to add an entry for a Conditional Forwarder that will point to the VPC02 Domain Controller and DNS server Right-click Conditional Forwarders, and select New Conditional Forwarder Enter a domain of acme.com and the IP address of the master server 10.1.16.5 NOTE: Ignore any errors in the DNS console reporting \"cannot resolve FQDN\" TASK 12 Setting a DNS Conditional Forwarder We now add a Conditional Forwarder on the acme.com Domain Controller located in VPC02. Using the Remote Desktop client for your machine, connect to the VPC02 Management Server using the IP address shown in the Qwiklabs Connection Settings on the Left Hand Side of the screen Login to the server using the acme.com domain account using the details below: Username: acme.com\\Administrator Password: @Pa55w0rd159@ (case-sensitive) Once successfully logged in, go to Windows Administrative Tools on the Start Menu and select DNS to launch the Windows DNS Manager NOTE: Ensure that the DNS server you are connected to is ADSERVER1 In the left-hand side navigation window make sure that you have selected DNS We are going to add an entry for a Conditional Forwarder that will point to the AWS Directory Services instances in VPC01 Right-click Conditional Forwarders, and select New Conditional Forwarder Enter a domain of corp.example.com and the IP address of both AWS Directory Services instances you noted at the start of the lab. Ignore the 'Unable to resolve' message and click OK TASK 13 \u2013 Configuring a Domain Trust One of the most common configurations for AWS Directory Services is adding AWS resources to AWS Directory Services and configuring a trust between AWS Directory Services and an existing Active Directory domain. This configuration isolates AWS resources into a separate, AWS hosted domain, while allowing Single Sign On from your on premise domain. In this task, we will create a Domain Trust to support this configuration. NOTE: Trusts can either be one of three things. Incoming, Outgoing and Bi-directional. It is important to understand that permissions are granted in the opposite direction to where the trust is. corp.example.com creates an outgoing trust to acme.com This allows acme.com access to corp.example.com resources Think of it as, corp.example.com trusts acme.com to access its resources. TASK 14 \u2013 Creating a Trust - VPC02 side Using the Remote Desktop client for your machine, connect to the VPC02 Management Server using an IP address that you can find via the EC2 Console by selecting the EC2 Instance called VPC02 Management Server Open Active Directory Domains and Trusts which you will find in the Start Menu under Windows Administrative Tools In the left-hand navigation pane right-click our domain acme.com and select Properties In the acme.com Properties screen, select the middle tab Trusts At the bottom of the screen click New Trust and the New Trust Wizard appears Click Next on the Welcome Screen Next, we are going to give our trust a name, name it corp.example.com and click Next In the Trust Type screen select Forest trust and click Next In the Direction of Trust screen select One-Way Incoming and click Next In the Sides of Trust screen select This domain only and click Next In the Trust Password screen provide a password of @Pa55w0rd159@ and click Next Click Next, Next Select No, do not confirm the incoming trust Click Next, Finish TASK 15 - Creating a Trust - AWS side Go to the AWS Management Console and select Directory Service With the Directories tab selected click on the directoryID for the corp.example.com domain Select the Trust Relationship tab in the middle of the page Click on the Add trust relationship button The Add a trust relationship detail page appears. Enter the following Remote domain name = acme.com Trust password = @Pa55w0rd159@ Trust direction = One-Way: Outgoing Conditional forwarder = 10.1.16.5 (The IP address of acme.com Domain Controller) Click Add Back at the Directory page you'll see a message stating that the One-Way: Outgoing trust is being created The Status of the trust relationship should be Verified in a few minutes TASK 16 - STRETCH GOAL - Share the CORP.EXAMPLE.COM domain to another Account In this task we will show one of the very recent additions to AWS Directory Services functionality. It is now possible to share an AWS Directory Service domain across VPC and also across different accounts. Go to the AWS Console, find and select Directory Services You will see a number of directories already created including one for corp.example.co Click on the Directory ID hyperlink of the Corp.Example.com domain. d-######## Scroll down the page where you can see a new tab called Scale & Share click that tab Click on Create new shared directory You have two options which are Share this directory with AWS Accounts inside your organization and Option B which is Share this directory with other AWS Accounts You need to be running AWS Organizations in order to do the first, but we can test out the second option by asking the student next to you for their account ID and adding it during this task. You then have the option of adding a note as part of the Directory sharing process. Add a note if you like and then click SHARE at the bottom of the page. Work with your partner to check that Directory Sharing works In your partners account, make sure you are using the correct region and then go to the Directory Services screen. In the Directories Shared with me link on the left hand side you see a notification. Select the Directory and click Review The Pending Shared Directory Invitation screen appears. Select the agreement and click Accept You can test this out by launching an EC2 Instance and adding it to the Corp.Example.com domain Congratulations you have completed this lab. LAB C \u2013 Federating Single Sign On for AWS Console Overview In this final exercise, we implement federation to the AWS Console from the Active Directory domain (acme.com). We will create users, groups and configure mapping between these and IAM roles. Objectives We are going to configure the AWS Console single-sign-on (SSO) to acme.com Active Directory domain. Task 1 \u2013 Creating Users & Groups Firstly, we need to create some users and groups in the corp.example.com domain. Remote Desktop into the VPC01 Management Server, Go to the Start Menu and launch PowerShell. Copy and paste the following PowerShell commands. Create AD Groups New-ADGroup -Name AWSDevelopers -GroupScope DomainLocal New-ADGroup -Name AWSAdmins -GroupScope DomainLocal Create Users New - Aduser - Name \"Williams\" - GivenName William - Surname Gates - SamAccountName William - UserPrincipalName william @ acme . com - accountPassword ( ConvertTo - SecureString - AsPlainText \"@Passw0rd158@@\" - Force ) - PassThru - enable $ True New - Aduser - Name \"Alex\" - GivenName Alex - Surname Summer - SamAccountName Alex - UserPrincipalName alex @ acme . com - accountPassword ( ConvertTo - SecureString - AsPlainText \"@Passw0rd158@@\" - Force ) - PassThru - enable $ True Add Users to Groups Add-ADGroupMember AWSDevelopers William Add-ADGroupMember AWSAdmins Alex Task 2 \u2013 Creating an EC2 Developer IAM role In the AWS console, select the IAM service In the IAM service screen select Roles in the Navigation pane Select Create role In the Create Role screen select AWS Service , choose Directory Service and click Next: Permissions In the Attach permissions policies screen find the following policy AmazonEC2ReadOnlyAccess make sure you tick the box to select it and then choose Next:Review In the review screen enter a role name of EC2Developer and click Create role Task 3 \u2013 Creating an EC2 Administrator IAM role In the AWS console, select the IAM service In the IAM service screen select Roles in the Navigation pane Select Create role In the Create Role screen select AWS Service , choose Directory Service and click Next:Permissions In the Attach permissions policies screen find the following policy AmazonEC2FullAccess make sure you tick the box to select it and then choose Next:Review In the review screen enter a Role Name of EC2Admin and click Create role Task 4 \u2013 Creating an Access URL In the AWS Console , select the Directory Service Select the directory ID for domain corp.example.com Choose the Apps & services tab where you'll see the Access URL box Select a unique name to add to the URL and click Create Access URL. This process may take a few minutes To Confirm - the directory Access URL would be https://uniquename.awsapps.com Task 5 \u2013 Enabling AWS Management Console access In the AWS Console, select the Directory Service Further down the page you'll see a list of AWS Apps & services Select AWS Management Console and choose Enable Access You will see the console URL next to the newly Enabled AWS Management Console link Task 6 \u2013 Assigning AD Users and Groups to IAM roles - Developers From the Directory Service screen click AWS management Console on the Apps & services tab You\u2019ll be presented with a confirmation pop-up, click Continue to be redirected to the manage access screen On the Add Users and Groups to Roles screen you will see the two roles we created in Task 1.1 in this section Click on the EC2Developer role which will take you to the Role Detail: EC2Developer screen Click Add under Assigned Users and Groups Make sure that the corp.example.com domain is selected and then choose the Group radio button In the search box type AWSDevelopers and then Add Task 7 \u2013 Assigning AD Users and Groups to IAM roles \u2013 Administrators Following on from the last task. From the Directory Service screen, once again select AWS Management Console Choose the EC2Admin role which takes you to the Role Detail: EC2Admin screen Click Add Click Group and search for AWSAdmins in the search box and then click Add NOTE: You can change the length of time allowed on the console by changing the Login Session Length number at the Add Users and Groups screen Task 8 \u2013 Testing access - AWS Developers In a new browser window, browse to the Access URL you configured in Task 1.2 Username = corp.example.com\\william Password = @Passw0rd158@@ Go into EC2 and click Instances. Now click Launch Instances and attempt to create an EC2 instance \u2013 you should receive an error message because Alex only has ReadOnly access Don't forget to Sign-out of the AWS Console and close the browser window Task 9 \u2013 Testing access - AWS Admins In a new browser window, browse to the Access URL you configured in Task 1.2 Username = corp.example.com\\alex Password = @Passw0rd158@@ Go into EC2 and click Instances. Now click Launch Instances and attempt to create an EC2 instance - you should be able to successfully launch an instance because William has Full Access Sign out of the console, and close down the browser window LAB D - BONUS LAB \u2013 AWS Systems Manager Overview AWS Systems Manager provides a number of critical services that will help you manage and maintain long running EC2 instances in AWS. If you have EC2 Instances that are immutable then you\u2019ll need a way of managing those instances and AWS Systems Manager is the best way of doing that. During this Bonus Lab we will give you a quick run through of some critical AWS Systems Manager services that you might use. Objectives After completing this lab, you will be able to: Remotely execute a command using Run Command Configure and use the Inventory service to collect configuration and instance information Create a Parameter store value Use State Manager to create a task and apply to your instances Define a Maintenance Window for disruptive tasks Configure and Schedule Patch Management LAB D \u2013 TASK 1 - Running a command remotely In this task, you will use Run Command to remotely and securely execute a PowerShell command on your Windows instance to verify the instance name and domain membership. Execute the command On the Services menu, click EC2 Scroll down to SYSTEMS MANAGER SERVICES , click Run Command Click Run a command On the Command document page, click AWS-RunPowerShellScript you\u2019ll find it on the third page along. For Select Targets by , choose one of the EC2 instances that you previously created In the commands detail, type the following text: (Get-WmiObject Win32_ComputerSystem).Name (Get-WmiObject Win32_ComputerSystem).Domain Leave the fields Working Directory and Execution Timeout as defaults For Comment , add a comment to help you identify this command Click Run to execute the command LAB D \u2013 TASK 2 - View the results 1. Once the command is complete, choose View result 2. You will be returned to the Run Command windows where the job details will be displayed, Click Output from the bottom pane Click View Output to display the output from the PowerShell command You will now see the results of the PowerShell command that was executed on your instance, it should display the instance name and domain information Click Close to return to the previous screen LAB D \u2013 TASK 3 - Bonus Task 1. [1]Try running some other familiar PowerShell commands on the instance(s) and viewing the output, for example can you verify the trust relationship between the domains? LAB D \u2013 TASK 4 - Inventory In this task, you will setup the Inventory service to collect configuration and inventory information of all instances in VPC02 to identify which version of the SSM Agent is installed on your instances. Configure Inventory 1. Click Managed Instances from the SYSTEMS MANAGER SHARED RESOURCES menu Click Setup Inventory In Targets , Click Select Targets by Specifying a Tag and in the selection box choose Tag Name \u2013 Environment , Tag Value \u2013 VPC02 We will leave the Schedule to execute Every 30 Minutes We will ensure all the Parameters are Enabled Click Setup Inventory Click Close LAB D \u2013 TASK 5 - Execute Inventory 1. As the task may not execute for 30 minutes, Click State Manager from the SYSTEMS MANAGER SERVICES menu Click on the AWS-GatherSoftwareInventory Document Name Click on Instances to view the targeted instances, you should see your instance Click Apply Association Now to force the inventory task to execute immediately LAB D \u2013 TASK 6 - View Inventory Results 1. Return to Managed Instances from the SYSTEMS MANAGER SHARED RESOURCES menu 2. Click on your instance and Click Inventory 3. From Inventory Type , Select AWS:Application This will now display the installed AWS Applications including the SSM Agent version LAB D \u2013 TASK 7 - Bonus Task Using the inventory data, can you identify the following? [1]What roles are installed on the server? [2]Is IIS installed and running? LAB D \u2013 TASK 8 - Parameter Store The Parameter store provides a centralized location to store, provide access control, and easily reference your configuration data, whether plain-text data such as database strings or secrets such as passwords, encrypted through AWS Key Management Service (KMS). In this task, we will create an administrator password parameter which we will later use to apply to Windows instances. Create Parameter in the Store 1. Click Parameter Store from the SYSTEMS MANAGER SHARED RESOURCES menu Click Get Started Now For Name type WindowsAdministratorPassword For Description type Windows local administrator password For Type select String For Value type L0calAdm1nPa55word@ Click Create Parameter Click Close LAB D \u2013 TASK 9 - State Manager Systems Manager State Manager is a secure and scalable configuration management service that ensures your Amazon EC2 and hybrid infrastructure is in a desired or consistent state, which you define. In this task, we will create an association to apply to the Windows servers with a job that will regularly update the local administrator password to the value previously created in the parameter store. Create Association 1. Click State Manager from the SYSTEMS MANAGER SERVICES menu 2. Click Create Association 3. For Association Name, Type SetWindowsLocalAdminPassword 4. For Document select AWS-RunPowerShellScript 5. Click Select Targets by Specifying a Tag and in the selection box choose Tag Name \u2013 Platform, Tag Value \u2013 Windows 6. We will leave the Schedule to execute Every 30 Minutes 7. In the commands detail, use the following values: Net.exe user administrator {{ ssm : WindowsAdministratorPassword }} Leave the default values for the Working Directory and Execution Timeout and Click Create Association After the Association has been successfully created, Click Close LAB D \u2013 TASK 10 - Execute Association 1. As the task may not execute for 30 minutes, Click State Manager from the SYSTEMS MANAGER SERVICES menu 2. Click on the SetWindowsLocalAdminPassword Association 3. Click Instances to view the targeted instances and the status 4. To force the association, Click Apply Association Now LAB D \u2013 TASK 11 - Test Association 1. To test that the local administrator password has been successfully changed on your Windows instances, ensure you are logged out of any remote desktop sessions. 2. Connect using remote desktop to your existing Windows instance using the local computer administrator account with the new password: - User: .\\administrator - Password: L0calAdm1nPa55word@ LAB D \u2013 TASK 12 - Maintenance Windows In this task, we will define a maintenance window for patches to be applied at 4 PM on every Tuesday for 4 hours. Create a Maintenance Window 1. Click Maintenance Windows from the SYSTEMS MANAGER SHARED RESOURCES menu 2. Click Create a Maintenance Window 3. For Name Type MyNightlyMaintenanceWindow 4. Click CRON/Rate expression 5. For CRON/Rate expression , Type cron(0 16 ? TUE ) For Duration, Type 4 For Stop initiating tasks, Type 1 This will create a maintenance Window that runs at 4 PM on every Tuesday for 4 hours, with a 1 hour cutoff Click Create maintenance window LAB D \u2013 TASK 13 - Register targets with a Maintenance Window 1. Click Maintenance Windows from the SYSTEMS MANAGER SHARED RESOURCES menu 2. Click MyNightlyMaintenanceWindow from the maintenance windows 3. Click Actions , Register targets from the drop-down menu 4. For Target Name Type MyWindowsInstances 5. Click Select Targets by Specifying a Tag and in the selection box choose Tag Name \u2013 Platform , Tag Value \u2013 Windows Click Register targets Once target registration has been successful, Click Close LAB D \u2013 TASK 14 - Patch Manager In this task, we will configure Patch Manager to help you select and deploy operating system and software patches automatically to your instances by using the default patch baseline. Register the patch manager task with the maintenance windows 1. Click Maintenance Windows from the SYSTEMS MANAGER SHARED RESOURCES menu 2. Click MyNightlyMaintenanceWindow from the maintenance windows 3. Click Actions , Register run command task from the drop-down menu 4. For Name , Type WindowsPatching 5. In the Document section, choose AWS-RunPatchBaseline 6. In Strict targets ensure your Maintenance Window target is selected, this may be identified by a unique ID rather than by name LAB D \u2013 TASK 15 - Create the required IAM role 1. For Role , click Add new custom role 2. Click Create role 3. Select EC2 , and click Next: Permissions 4. In Policy type , Type AmazonSSMMaintenanceWindowRole Select the AmazonSSMMaintenanceWindowRole policy and click Next: Review For Role name type * ec2_AmazonSSMMaintenanceWindowRole Click Create role LAB D \u2013 TASK 16 - Complete the configuration 1. Return to your AWS Console windows and click the Refresh icon next to the Role 2. You should now be able to select the newly created role within the Role drop-down For Execute on enter 5 For Stop after enter 5 For Operation, change to Install Leave all other fields as default and click Register task Once the task has been successfully registered, click Close Select your Maintenance Window task and check the associated tasks and targets to check its setup correctly Congratulations you have now successfully configured a Maintenance Window to execute at 4 PM on every Tuesday for 4 hours that will patch all your Windows instances 5 at a time LAB D \u2013 TASK 17 - Run Patch Baseline Bonus Task As we won't be around next Tuesday at 4PM, execute a patch baseline against our Windows instances to view the patch status immediately Execute a Run command on your instances using AWS-RunPatchBaseline, Note: it can take >10 minutes to execute on the instances Once complete select your instances in Patch Compliance and view the compliance status From Managed Instances click Filter by attributes and select AWS:ComplianceItem.Classification : equals : SecurityUpdates and AWS:PatchSummary.MissingCount : greater-than : 0 Try some more advanced search filters, can you identify any missing security updates on your instances? Congratulations, you have now completed all Lab activities","title":"Lab Guide"},{"location":"reinvent2018/win309/resources/#lab-a-active-directory-running-on-ec2","text":"","title":"LAB A \u2013 Active Directory running on EC2"},{"location":"reinvent2018/win309/resources/#goal","text":"In this lab you are going to extend an on-premise Active Directory into AWS. You will be using EC2, AWS AD Connector and AWS Systems Manager services to complete this Lab. LAB A \u2013 TASKS 1 thru 9 \u2013 QUICKTASK Familiarize yourself with the on-premise ACME.com domain via its management server\u2019s public-IP address (named Management Server in the console). Then we would like you to create a Server 2016 instance in VPC01 Private Subnet, use the AWS AD Connector to join it to the ACME.com domain (residing in VPC02), and then promote it to a domain controller. LAB A - TASK 1 Let\u2019s familiarize ourselves with the environment. As we mentioned, VPC02 is a simulated on-premise environment. We are going to connect via Remote Desktop (RDP) to the Management Server using its external IP address. Once connected, we will verify that the settings are correct and that the AD management tools are installed. Go to the AWS Console you launched and complete the following steps: Click on EC2 In the center of the screen under Resources, click on Running Instances Select the instance called Management Server You\u2019ll find the Public IP address on the bottom detail screen. Make a note of this address. LAB A \u2013 TASK 2 Connect to the EC2 Instance from TASK 1-1 using Microsoft Remote Desktop You can either do this via launching the Microsoft Remote Desktop app and adding an entry for a new host or via the command line with the following MSTSC /v: IP address of EC2 Instance Please call out to one of the support team in the room if you are having issues. Once connected, you\u2019ll need account details to login Username = acme\\administrator Password = @Pa55w0rd159@ Let\u2019s look at Administrative Tools which you will find in the Start menu. Locate Active Directory Users and Computers app, launch it. Take a look around the ACME.COM domain, you will see a Domain Controller called ADSERVER1 but apart from that, it\u2019s a brand new domain. Please close down the remote session to this server and continue with the next task.","title":"GOAL"},{"location":"reinvent2018/win309/resources/#task-25-create-an-iam-role","text":"IAM Roles are configured with permissions that allow a service, assuming the role, to perform actions permitted by that role. Amazon EC2 Roles allow Amazon EC2 instances to assume a role so that applications on the instance can make secure API calls to AWS. In this task, we will create an EC2 role which will be used by the SSM agent on our instance to perform the actions it requires with Directory Services. Back at the Home screen of the AWS Management Console, click IAM Select Roles from the left side menu and click on Create role Select AWS service and then EC2 Select EC2 Role for Simple Systems Manager as your use case and click Next:Permissions The AmazonEC2RoleforSSM should be selected Click Next: Review Enter EC2RoleforSSM as the name for the role Click Create Role LAB A \u2013 TASK 3 In this task we will create a new Windows 2016 EC2 Instance using the standard Amazon Machine Image (AMI) and then in later tasks we will configure this instance to be an AD Domain Controller. This new EC2 Instance will be on the AWS side in VPC01 . In later tasks we will extend the secondary domain into this EC2 Instance. In the AWS Console, click on EC2 Next click on Launch Instance Scroll down the list of Amazon Machine Images until you find Microsoft Windows Server 2016 Base and then click Select On the Choose an Instance Type screen select T2.Large and click Next: Configure Instance Details You will need to make sure you select the following on the Configure Instance Details screen NETWORK = VPC01-VPC SUBNET = VPC01-SNEXTPUB1A AUTO-ASSIGN PUBLIC IP = Enable IAM Role = AmazonEC2RoleforSSM You can leave all other parameters at their default. Click Next: Add Storage Click Next: Add Tags Click Next: Add Security Groups Click Add Rule Make sure that an RDP rule for port 3389 with a Source of My IP is present NOTE: We advise that you restrict access of RDP Traffic to your own network Click Review and Launch On the Review Instance Launch screen click Launch On the Select an existing key pair screen ensure that you select Create a new key pair , give it a Key Pair Name and ensure that you Download the file to your laptop. Then click Launch Instances LAB A \u2013 TASK 4 In this task we are going to use the AWS AD Connector Service to allow us to connect our newly created EC2 Instance to the ACME.COM domain that resides in VPC02 . In the AWS Console, click on Directory Service On the Directories screen click on Set up directory On the Select directory type screen select AD Connector and click Next On the Enter AD Connector information screen select Directory size of Small and click Next On the Choose VPC and subnets screen enter the following: VPC \u2013 VPC01-VPC Subnets \u2013 VPC01-SNEXTPUB1A VPC01-SNEXTPUB1B Click Next On the Active Directory Information screen enter the following details: Directory DNS name \u2013 acme.com Directory NetBIOS name \u2013 acme DNS IP Address \u2013 10.1.16.5 Service account username \u2013 administrator Service account password \u2013 @Pa55w0rd159@ Click Next Finally, on the Review and Create screen, check all settings and then click Create directory NOTE: It is best practice to reduce the permissions assigned to the Service Account used during the AWS AD Connector setup. The following link has best practice guidelines: https://docs.aws.amazon.com/directoryservice/latest/admin-guide/prereq_connector.html?icmpid=docs_ds_console_help_panel#connect_delegate_privileges","title":"TASK 2.5 \u2013 Create an IAM role"},{"location":"reinvent2018/win309/resources/#task-45-create-a-new-dhcp-options-set","text":"DHCP Option Sets cannot be modified. If you require your VPC to use a different set of DHCP options, you must create a new set and associate with your VPC. In this section, you will create a new DHCP Option Set which will be used by EC2 instances launched into VPC01. In the AWS Management Console on the services menu, go back to the home screen and then click on VPC Choose DHCP Option Sets from the side menu Choose Create DHCP options set Enter these values in to the relevant fields: Name Tag = dopt-acme Domain name = acme.com Domain name servers = 10.1.16.5 NTP servers = (leave blank) NetBIOS servers = (leave blank) NetBIOS node type = (leave blank) Choose Yes, Create","title":"TASK 4.5 \u2013 Create a new DHCP Options set"},{"location":"reinvent2018/win309/resources/#task-46-associate-the-new-dhcp-options-set-with-your-vpc","text":"The next step is to associate our newly created DHCP Option Set with our VPC. After you associate a new set of DHCP options with a VPC, any existing instances and all new instances that you launch in the VPC use these options. You don't need to restart or relaunch the instances. Go back to the VPC services menu Select Your VPCs from the left menu and then select the VPC labelled VPC01-VPC Click on Actions and Select Edit DHCP Options Set From the list, select the DHCP Option Set with the name given in Task 4.5 dopt-acme Click Save LAB A \u2013 TASK 5 You can download the Port Verification Tool to troubleshoot AD Connector or Microsoft AD Trust setup issues from here: Directory Service Port Test The AD Connector Port Verification Tool does two things: Determines if the necessary ports are open from the VPC to your domain Verifies the minimum forest and domain functional levels. Usage: DirectoryServicePortTest.exe -d <domain_name> -ip <server_IP_address> -tcp \"53,88,135,389,445,464,3268,3269,5722,9389\" -udp \"53,88,123,138,389,445\" Start a Remote Desktop session with your newly created EC2 Instance. Start Internet Explorer and browse to the following link. https://docs.aws.amazon.com/directoryservice/latest/admin-guide/samples/DirectoryServicePortTest.zip If Internet Explorer Enhanced Security Configuration is enabled Click OFF for Administrators option. Click ok. f. Restart Internet Explorer. Download the tool, and extract the .zip file to the C:\\TEMP folder. Start a Command Prompt prompt via the Start Menu. Change to the C:\\TEMP folder via CD C:\\TEMP Enter the following \u2013 directoryserviceporttest -d acme.com \u2013 ip \u201c10.1.16.5\u201d You should see the following confirmation LAB A \u2013 TASK 6 In this task we will perform the simple task of joining our newly created EC2 Instance to the ACME.COM domain. There are numerous ways of doing this such as - Adding a Powershell script to the USERDATA section when launching the EC2 Instance - Using the AWS Systems Manager \u2018Run Command\u2019 service to run a Powershell script - Use the new AWS Systems Manager \u2018Session Manager\u2019 to create a session with the instance Since you may not have had the chance to use Session Manager yet, lets choose this option. In the AWS Console search for Systems Manager under Management Tools and select. AWS Systems Manager gives you a lot of functionality that will help you manage your long-running AWS EC2 Instances. On the left hand side menu choose Session Manager This screen gives you details of previous used sessions and also you can configure session-logging in the Preferences tab. Click Start Session There should be three EC2 instances viable in the Target Instances screen. Select the Instance ID that corresponds to your newly created instance and select using the radio-button on the left and then click the Start Session button. A Powershell Prompt will appear in your browser. Type or paste the following \u2013","title":"TASK 4.6 \u2013 Associate the new DHCP Options set with your VPC"},{"location":"reinvent2018/win309/resources/#please-enter-each-line-individually","text":"Set-DNSClientServerAddress -InterfaceAlias \u201cEthernet\u201d -ServerAddresses (\u201c10.1.16.5\u201d) $domain = \"acme.com\" $password = \"@Pa55w0rd159@\u201d | ConvertTo-SecureString -asPlainText -Force $username = \"$domain\\administrator\" $credential = New-Object System.Management.Automation.PSCredential($username,$password) Add-Computer -DomainName $domain -Credential $credential -restart You should get the following response The EC2 Instance restarts and joins the domain ACME.COM. After the restart see if you can connect it using the ACME.COM credentials. LAB A \u2013 TASK 7 In this task we will promote our newly created instance to become a Domain Controller in the ACME.COM domain. We will use AWS Systems Manager\u2019s Session Manager service once again. Back to the AWS Console, find and select AWS Systems Manager In the AWS Systems Manager screen, select Session Manager on the left. Click Start Session Select the EC2 we created in TASK 3 and click Start Session Type or cut and paste the following Powershell into the Session. Please enter the commands Line by Line Add-WindowsFeature AD-Domain-Services, RSAT-AD-AdminCenter,RSAT-ADDS-Tools $domain = \"acme.com\" $password = \"@Pa55w0rd159@\u201d | ConvertTo-SecureString -asPlainText -Force $username = \"$domain\\administrator\" $credential = New-Object System.Management.Automation.PSCredential($username,$password) install-addsdomaincontroller -installdns -Credential $credential -domainname acme.com You\u2019ll be asked to provide a SafeMode Administrator Password. Type the following ReInvent2018 You\u2019ll receive the following message \u2013 enter Y (for Yes) Congratulations ! We\u2019ve used AWS AD Connector to extend on on-premise Active Directory into AWS and then created a domain controller in the ACME.COM domain","title":"please enter each line individually"},{"location":"reinvent2018/win309/resources/#lab-b-using-the-aws-directory-service-for-active-directory","text":"","title":"LAB B \u2013 Using the AWS Directory Service for Active Directory"},{"location":"reinvent2018/win309/resources/#goal_1","text":"In this lab you are going to configure the instance of AWS Directory Services that has already been created for you. You will then create EC2 Instances which are automatically domain joined when they start.","title":"GOAL"},{"location":"reinvent2018/win309/resources/#stretch-goal","text":"You will configure federated Single Sign on to the AWS Console for Active Directory users. We will also Share our AWS Directory Services domain into a separate VPC.","title":"STRETCH GOAL"},{"location":"reinvent2018/win309/resources/#quicktask","text":"In this next lab you will complete the configuration of AWS Directory Services for Active Directory This includes setting DHCP Option Sets in VPC01 and configuring a AWS Systems Manager role that will enable an EC2 Instance to Auto-Join the corp.example.com domain when it launches. After that is complete we will configure a Domain Trust with ACME.COM","title":"QUICKTASK"},{"location":"reinvent2018/win309/resources/#gathering-information","text":"Before we begin, we need to gather some information that will be required throughout the tasks ahead. In the AWS Console, select Directory Service from the Security, Identity & Compliance section of services. Or just type Directory in the Console search bar and select the Directory Service. You will see a single Directory already created for you. Click on the Directory ID link. On the next screen, you'll find details of the Directory. Make a note of the following items... Directory ID DNS Address (x2) The DNS addresses (x2) are the individual IP addresses of each domain controller pre-created in separate Availability Zones inside a single VPC.","title":"Gathering information"},{"location":"reinvent2018/win309/resources/#task-1-configuring-a-vpc-to-use-your-dns-servers","text":"The Dynamic Host Configuration Protocol (DHCP) provides a standard for passing configuration information to hosts on a TCP/IP network. The options field of a DHCP message contains the configuration parameters. Some of those parameters are the domain name, domain name server, and the NetBIOS-node-type. DHCP Option Sets are associated with your virtual private cloud (VPC). The default DHCP Option Set resolves DNS requests via Route 53. With Windows we need our instances to resolve their DNS via our Domain Controllers. In this lab we will create a new DHCP Option Set and redirect DNS requests to our AWS Directory Service.","title":"TASK 1 - Configuring a VPC to use your DNS servers"},{"location":"reinvent2018/win309/resources/#task-2-create-a-new-dhcp-options-set","text":"DHCP Option Sets cannot be modified. If you require your VPC to use a different set of DHCP options, you must create a new set and associate with your VPC. In this section, you will create a new DHCP Option Set which will be used by EC2 instances launched into VPC01. In the AWS Management Console on the services menu, go back to the home screen and then click on VPC Choose DHCP Option Sets from the side menu Choose Create DHCP options set Enter these values in to the relevant fields: Name Tag = dopt-corp Domain name = corp.example.com Domain name servers = (The two IP addresses for your AWS Directory Services DNS Servers that you noted earlier. Use a comma to separate them) NTP servers = (leave blank) NetBIOS servers = (leave blank) NetBIOS node type = (leave blank) Choose Yes, Create","title":"TASK 2 \u2013 Create a new DHCP Options set"},{"location":"reinvent2018/win309/resources/#task-3-associate-the-new-dhcp-options-set-with-your-vpc","text":"The next step is to associate our newly created DHCP Option Set with our VPC. After you associate a new set of DHCP options with a VPC, any existing instances and all new instances that you launch in the VPC use these options. You don't need to restart or relaunch the instances. Go back to the VPC services menu Select Your VPCs from the left menu and then select the VPC labelled VPC01-VPC Click on Actions and Select Edit DHCP Options Set From the list, select the DHCP Option Set with the name given in Task 1 dopt-corp Click Save","title":"TASK 3 \u2013 Associate the new DHCP Options set with your VPC"},{"location":"reinvent2018/win309/resources/#task-4-domain-join-part-1","text":"Overview For our second task, we are going to configure AD domain auto-join. This requires the creation of an SSM Document and IAM role. The process is simplified as AWS will automatically create the SSM document for you when you first launch an instance. The steps below take you through the creation of the IAM role, and then finally we launch an Amazon EC2 instance that will automatically join the corp.example.com domain running on AWS Directory Services.","title":"TASK 4 - Domain Join Part 1"},{"location":"reinvent2018/win309/resources/#task-5-create-an-iam-role","text":"","title":"TASK 5 \u2013 Create an IAM role"},{"location":"reinvent2018/win309/resources/#if-you-worked-through-lab-a-first-then-you-will-have-already-created-an-iam-role-called-ec2roleforssm-you-can-then-skip-this-task-if-not-please-follow-the-instructions-below","text":"IAM Roles are configured with permissions that allow a service, assuming the role, to perform actions permitted by that role. Amazon EC2 Roles allow Amazon EC2 instances to assume a role so that applications on the instance can make secure API calls to AWS. In this task, we will create an EC2 role which will be used by the SSM agent on our instance to perform the actions it requires with Directory Services. Back at the Home screen of the AWS Management Console, click IAM Select Roles from the left side menu and click on Create role Select AWS service and then EC2 Select EC2 Role for Simple Systems Manager as your use case and click Next:Permissions The AmazonEC2RoleforSSM should be selected Click Next: Review Enter EC2RoleforSSM as the name for the role Click Create Role","title":"If you worked through LAB A first then you will have already created an IAM role called EC2RoleforSSM.  You can then skip this task.  If not, please follow the instructions below."},{"location":"reinvent2018/win309/resources/#task-6-launch-an-instance","text":"In this task, we will launch a new instance into VPC01 and attach the IAM role that we created in the previous task. On the Services menu, click EC2 Select Launch Instance Select Microsoft Windows Server 2016 Base as the AMI Choose t2. medium as the instance type Click Next:Configure Instance Details In the Configure Instance Details Screen, ensure the following settings: Network VPC, select VPC01 Subnet, select VPC01-SNEXTPUB1A Auto-Assign Public-IP should be set to ENABLE Domain join directory should be set to corp.example.com I AM role should be set to EC2RoleforSSM (which we previously created in TASK 5) Click Advanced Details at the bottom of the screen which will show the Userdata section Copy and Paste the following PowerShell code into the Userdata section which will install the required management tools <powershell> Import-Module Servermanager Install-WindowsFeature RSAT, RSAT-DNS-Server </powershell> Select Review and Launch You'll receive a security warning that we are not using Free Usage Tier. Click Launch . Select the existing Key Pair from LAB A. Acknowledge via the check-box and click Launch Instances NOTE: Make a note of the Instance-ID of the new EC2 instance you've just launched Whilst our Amazon EC2 instance is launching, let's complete another task before returning to the instance.","title":"TASK 6 \u2013 Launch an Instance"},{"location":"reinvent2018/win309/resources/#task-7-aws-directory-service-event-notification","text":"As a Windows server administrator, understanding when you have an issue with a domain controller is vital. Windows Server has a built-in logging system via Event Viewer, but to be notified when the AWS Directory Service itself has a problem we create a notification using SNS. Configure SNS 1. Go to the Home screen of the AWS console Select Directory Service On the Directories page, choose the directory ID for the corp.example.com domain Choose Monitoring and then Create Notification Choose Create a new notification Choose Recipient Type as Email and enter an email address that is accessible to you during the workshop Choose Advanced Options and ensure your SNS topic starts with DirectoryMonitoring Choose Add","title":"TASK 7 \u2013 AWS Directory Service Event Notification"},{"location":"reinvent2018/win309/resources/#task-8-confirm-subscription","text":"You will receive an email asking you to confirm your subscription to the SNS notification. Click the link to confirm the subscription.","title":"TASK 8 \u2013 Confirm Subscription"},{"location":"reinvent2018/win309/resources/#task-9-domain-join-part-2","text":"Verify your instance is joined to the domain Switching back to the Amazon EC2 instance we previously launched in Task 2. Let's find the Instance-ID in Amazon EC2 Services and note the Public IP of the instance. Using the Remote Desktop client for your machine, connect to the EC2 instance Public IP address NOTE: You will receive a Certificate error when connecting to the instance, select continue Login to the server using the corp.example.com domain account using the details below: Username: corp.example.com\\admin Password: @Pa55w0rd159@ (case-sensitive) This will demonstrate that your EC2 instance is joined to the corp.example.com domain NOTE: If you fail to connect, check the status of the EC2 instance in the console to ensure it has finished launching.","title":"TASK 9 \u2013 Domain Join Part 2"},{"location":"reinvent2018/win309/resources/#task-10-configuring-dns-resolution","text":"In the first task, we created a new DHCP options set and associated it with VPC01. This option set will allow instances launched into VPC01 to resolve DNS hostnames for corp.example.com . In this task, we will configure DNS to allow our AWS resources to resolve DNS hostnames for acme.com domain as well as vice versa. We need to make changes to the existing security group attached to the Directory Services domain controllers, this will allow the outbound connectivity required between the Directory Services located in VPC01 and resources in VPC02. On the Services menu, click EC2 On the sidebar, select Security Groups You should see a number of security groups already created. One will have the Group Name that will have a suffix of controllers. This is the security group automatically created when we launched AWS Directory Services Select this Security Group and select the Outbound tab Click Edit and add a new Security group rule as per the following: Type = ALL Traffic Protocol = ALL Destination = 10.1.0.0/16 Click Save","title":"TASK 10 \u2013 Configuring DNS resolution"},{"location":"reinvent2018/win309/resources/#task-11-continuing-dns-configuration","text":"If you have closed your existing remote desktop connection, using the Remote Desktop client for your machine, connect to the EC2 instance public IP address you previously created in task 2.2 Login to the server using the corp.example.com domain account using the details below: Username: corp.example.com\\admin Password: @Pa55w0rd159@ (case-sensitive) Once successfully logged in, go to Windows Administrative Tools on the Start Menu and select DNS to launch the Windows DNS Manager NOTE: Ensure that the DNS server you connect to is one of the AWS Directory Services instances which you noted at the start of the lab. In the left-hand side navigation window make sure that you have selected the DNS Server We are going to add an entry for a Conditional Forwarder that will point to the VPC02 Domain Controller and DNS server Right-click Conditional Forwarders, and select New Conditional Forwarder Enter a domain of acme.com and the IP address of the master server 10.1.16.5 NOTE: Ignore any errors in the DNS console reporting \"cannot resolve FQDN\"","title":"TASK 11 - Continuing DNS Configuration"},{"location":"reinvent2018/win309/resources/#task-12-setting-a-dns-conditional-forwarder","text":"We now add a Conditional Forwarder on the acme.com Domain Controller located in VPC02. Using the Remote Desktop client for your machine, connect to the VPC02 Management Server using the IP address shown in the Qwiklabs Connection Settings on the Left Hand Side of the screen Login to the server using the acme.com domain account using the details below: Username: acme.com\\Administrator Password: @Pa55w0rd159@ (case-sensitive) Once successfully logged in, go to Windows Administrative Tools on the Start Menu and select DNS to launch the Windows DNS Manager NOTE: Ensure that the DNS server you are connected to is ADSERVER1 In the left-hand side navigation window make sure that you have selected DNS We are going to add an entry for a Conditional Forwarder that will point to the AWS Directory Services instances in VPC01 Right-click Conditional Forwarders, and select New Conditional Forwarder Enter a domain of corp.example.com and the IP address of both AWS Directory Services instances you noted at the start of the lab. Ignore the 'Unable to resolve' message and click OK","title":"TASK 12 Setting a DNS Conditional Forwarder"},{"location":"reinvent2018/win309/resources/#task-13-configuring-a-domain-trust","text":"One of the most common configurations for AWS Directory Services is adding AWS resources to AWS Directory Services and configuring a trust between AWS Directory Services and an existing Active Directory domain. This configuration isolates AWS resources into a separate, AWS hosted domain, while allowing Single Sign On from your on premise domain. In this task, we will create a Domain Trust to support this configuration. NOTE: Trusts can either be one of three things. Incoming, Outgoing and Bi-directional. It is important to understand that permissions are granted in the opposite direction to where the trust is. corp.example.com creates an outgoing trust to acme.com This allows acme.com access to corp.example.com resources Think of it as, corp.example.com trusts acme.com to access its resources.","title":"TASK 13 \u2013 Configuring a Domain Trust"},{"location":"reinvent2018/win309/resources/#task-14-creating-a-trust-vpc02-side","text":"Using the Remote Desktop client for your machine, connect to the VPC02 Management Server using an IP address that you can find via the EC2 Console by selecting the EC2 Instance called VPC02 Management Server Open Active Directory Domains and Trusts which you will find in the Start Menu under Windows Administrative Tools In the left-hand navigation pane right-click our domain acme.com and select Properties In the acme.com Properties screen, select the middle tab Trusts At the bottom of the screen click New Trust and the New Trust Wizard appears Click Next on the Welcome Screen Next, we are going to give our trust a name, name it corp.example.com and click Next In the Trust Type screen select Forest trust and click Next In the Direction of Trust screen select One-Way Incoming and click Next In the Sides of Trust screen select This domain only and click Next In the Trust Password screen provide a password of @Pa55w0rd159@ and click Next Click Next, Next Select No, do not confirm the incoming trust Click Next, Finish","title":"TASK 14 \u2013 Creating a Trust - VPC02 side"},{"location":"reinvent2018/win309/resources/#task-15-creating-a-trust-aws-side","text":"Go to the AWS Management Console and select Directory Service With the Directories tab selected click on the directoryID for the corp.example.com domain Select the Trust Relationship tab in the middle of the page Click on the Add trust relationship button The Add a trust relationship detail page appears. Enter the following Remote domain name = acme.com Trust password = @Pa55w0rd159@ Trust direction = One-Way: Outgoing Conditional forwarder = 10.1.16.5 (The IP address of acme.com Domain Controller) Click Add Back at the Directory page you'll see a message stating that the One-Way: Outgoing trust is being created The Status of the trust relationship should be Verified in a few minutes","title":"TASK 15 - Creating a Trust - AWS side"},{"location":"reinvent2018/win309/resources/#task-16-stretch-goal-share-the-corpexamplecom-domain-to-another-account","text":"In this task we will show one of the very recent additions to AWS Directory Services functionality. It is now possible to share an AWS Directory Service domain across VPC and also across different accounts. Go to the AWS Console, find and select Directory Services You will see a number of directories already created including one for corp.example.co Click on the Directory ID hyperlink of the Corp.Example.com domain. d-######## Scroll down the page where you can see a new tab called Scale & Share click that tab Click on Create new shared directory You have two options which are Share this directory with AWS Accounts inside your organization and Option B which is Share this directory with other AWS Accounts You need to be running AWS Organizations in order to do the first, but we can test out the second option by asking the student next to you for their account ID and adding it during this task. You then have the option of adding a note as part of the Directory sharing process. Add a note if you like and then click SHARE at the bottom of the page. Work with your partner to check that Directory Sharing works In your partners account, make sure you are using the correct region and then go to the Directory Services screen. In the Directories Shared with me link on the left hand side you see a notification. Select the Directory and click Review The Pending Shared Directory Invitation screen appears. Select the agreement and click Accept You can test this out by launching an EC2 Instance and adding it to the Corp.Example.com domain Congratulations you have completed this lab.","title":"TASK 16 - STRETCH GOAL - Share the CORP.EXAMPLE.COM domain to another Account"},{"location":"reinvent2018/win309/resources/#lab-c-federating-single-sign-on-for-aws-console","text":"Overview In this final exercise, we implement federation to the AWS Console from the Active Directory domain (acme.com). We will create users, groups and configure mapping between these and IAM roles. Objectives We are going to configure the AWS Console single-sign-on (SSO) to acme.com Active Directory domain. Task 1 \u2013 Creating Users & Groups Firstly, we need to create some users and groups in the corp.example.com domain. Remote Desktop into the VPC01 Management Server, Go to the Start Menu and launch PowerShell. Copy and paste the following PowerShell commands. Create AD Groups New-ADGroup -Name AWSDevelopers -GroupScope DomainLocal New-ADGroup -Name AWSAdmins -GroupScope DomainLocal Create Users New - Aduser - Name \"Williams\" - GivenName William - Surname Gates - SamAccountName William - UserPrincipalName william @ acme . com - accountPassword ( ConvertTo - SecureString - AsPlainText \"@Passw0rd158@@\" - Force ) - PassThru - enable $ True New - Aduser - Name \"Alex\" - GivenName Alex - Surname Summer - SamAccountName Alex - UserPrincipalName alex @ acme . com - accountPassword ( ConvertTo - SecureString - AsPlainText \"@Passw0rd158@@\" - Force ) - PassThru - enable $ True Add Users to Groups Add-ADGroupMember AWSDevelopers William Add-ADGroupMember AWSAdmins Alex Task 2 \u2013 Creating an EC2 Developer IAM role In the AWS console, select the IAM service In the IAM service screen select Roles in the Navigation pane Select Create role In the Create Role screen select AWS Service , choose Directory Service and click Next: Permissions In the Attach permissions policies screen find the following policy AmazonEC2ReadOnlyAccess make sure you tick the box to select it and then choose Next:Review In the review screen enter a role name of EC2Developer and click Create role Task 3 \u2013 Creating an EC2 Administrator IAM role In the AWS console, select the IAM service In the IAM service screen select Roles in the Navigation pane Select Create role In the Create Role screen select AWS Service , choose Directory Service and click Next:Permissions In the Attach permissions policies screen find the following policy AmazonEC2FullAccess make sure you tick the box to select it and then choose Next:Review In the review screen enter a Role Name of EC2Admin and click Create role Task 4 \u2013 Creating an Access URL In the AWS Console , select the Directory Service Select the directory ID for domain corp.example.com Choose the Apps & services tab where you'll see the Access URL box Select a unique name to add to the URL and click Create Access URL. This process may take a few minutes To Confirm - the directory Access URL would be https://uniquename.awsapps.com Task 5 \u2013 Enabling AWS Management Console access In the AWS Console, select the Directory Service Further down the page you'll see a list of AWS Apps & services Select AWS Management Console and choose Enable Access You will see the console URL next to the newly Enabled AWS Management Console link Task 6 \u2013 Assigning AD Users and Groups to IAM roles - Developers From the Directory Service screen click AWS management Console on the Apps & services tab You\u2019ll be presented with a confirmation pop-up, click Continue to be redirected to the manage access screen On the Add Users and Groups to Roles screen you will see the two roles we created in Task 1.1 in this section Click on the EC2Developer role which will take you to the Role Detail: EC2Developer screen Click Add under Assigned Users and Groups Make sure that the corp.example.com domain is selected and then choose the Group radio button In the search box type AWSDevelopers and then Add Task 7 \u2013 Assigning AD Users and Groups to IAM roles \u2013 Administrators Following on from the last task. From the Directory Service screen, once again select AWS Management Console Choose the EC2Admin role which takes you to the Role Detail: EC2Admin screen Click Add Click Group and search for AWSAdmins in the search box and then click Add NOTE: You can change the length of time allowed on the console by changing the Login Session Length number at the Add Users and Groups screen Task 8 \u2013 Testing access - AWS Developers In a new browser window, browse to the Access URL you configured in Task 1.2 Username = corp.example.com\\william Password = @Passw0rd158@@ Go into EC2 and click Instances. Now click Launch Instances and attempt to create an EC2 instance \u2013 you should receive an error message because Alex only has ReadOnly access Don't forget to Sign-out of the AWS Console and close the browser window Task 9 \u2013 Testing access - AWS Admins In a new browser window, browse to the Access URL you configured in Task 1.2 Username = corp.example.com\\alex Password = @Passw0rd158@@ Go into EC2 and click Instances. Now click Launch Instances and attempt to create an EC2 instance - you should be able to successfully launch an instance because William has Full Access Sign out of the console, and close down the browser window","title":"LAB C \u2013 Federating Single Sign On for AWS Console"},{"location":"reinvent2018/win309/resources/#lab-d-bonus-lab-aws-systems-manager","text":"Overview AWS Systems Manager provides a number of critical services that will help you manage and maintain long running EC2 instances in AWS. If you have EC2 Instances that are immutable then you\u2019ll need a way of managing those instances and AWS Systems Manager is the best way of doing that. During this Bonus Lab we will give you a quick run through of some critical AWS Systems Manager services that you might use. Objectives After completing this lab, you will be able to: Remotely execute a command using Run Command Configure and use the Inventory service to collect configuration and instance information Create a Parameter store value Use State Manager to create a task and apply to your instances Define a Maintenance Window for disruptive tasks Configure and Schedule Patch Management LAB D \u2013 TASK 1 - Running a command remotely In this task, you will use Run Command to remotely and securely execute a PowerShell command on your Windows instance to verify the instance name and domain membership. Execute the command On the Services menu, click EC2 Scroll down to SYSTEMS MANAGER SERVICES , click Run Command Click Run a command On the Command document page, click AWS-RunPowerShellScript you\u2019ll find it on the third page along. For Select Targets by , choose one of the EC2 instances that you previously created In the commands detail, type the following text: (Get-WmiObject Win32_ComputerSystem).Name (Get-WmiObject Win32_ComputerSystem).Domain Leave the fields Working Directory and Execution Timeout as defaults For Comment , add a comment to help you identify this command Click Run to execute the command LAB D \u2013 TASK 2 - View the results 1. Once the command is complete, choose View result 2. You will be returned to the Run Command windows where the job details will be displayed, Click Output from the bottom pane Click View Output to display the output from the PowerShell command You will now see the results of the PowerShell command that was executed on your instance, it should display the instance name and domain information Click Close to return to the previous screen LAB D \u2013 TASK 3 - Bonus Task 1. [1]Try running some other familiar PowerShell commands on the instance(s) and viewing the output, for example can you verify the trust relationship between the domains? LAB D \u2013 TASK 4 - Inventory In this task, you will setup the Inventory service to collect configuration and inventory information of all instances in VPC02 to identify which version of the SSM Agent is installed on your instances. Configure Inventory 1. Click Managed Instances from the SYSTEMS MANAGER SHARED RESOURCES menu Click Setup Inventory In Targets , Click Select Targets by Specifying a Tag and in the selection box choose Tag Name \u2013 Environment , Tag Value \u2013 VPC02 We will leave the Schedule to execute Every 30 Minutes We will ensure all the Parameters are Enabled Click Setup Inventory Click Close LAB D \u2013 TASK 5 - Execute Inventory 1. As the task may not execute for 30 minutes, Click State Manager from the SYSTEMS MANAGER SERVICES menu Click on the AWS-GatherSoftwareInventory Document Name Click on Instances to view the targeted instances, you should see your instance Click Apply Association Now to force the inventory task to execute immediately LAB D \u2013 TASK 6 - View Inventory Results 1. Return to Managed Instances from the SYSTEMS MANAGER SHARED RESOURCES menu 2. Click on your instance and Click Inventory 3. From Inventory Type , Select AWS:Application This will now display the installed AWS Applications including the SSM Agent version LAB D \u2013 TASK 7 - Bonus Task Using the inventory data, can you identify the following? [1]What roles are installed on the server? [2]Is IIS installed and running? LAB D \u2013 TASK 8 - Parameter Store The Parameter store provides a centralized location to store, provide access control, and easily reference your configuration data, whether plain-text data such as database strings or secrets such as passwords, encrypted through AWS Key Management Service (KMS). In this task, we will create an administrator password parameter which we will later use to apply to Windows instances. Create Parameter in the Store 1. Click Parameter Store from the SYSTEMS MANAGER SHARED RESOURCES menu Click Get Started Now For Name type WindowsAdministratorPassword For Description type Windows local administrator password For Type select String For Value type L0calAdm1nPa55word@ Click Create Parameter Click Close LAB D \u2013 TASK 9 - State Manager Systems Manager State Manager is a secure and scalable configuration management service that ensures your Amazon EC2 and hybrid infrastructure is in a desired or consistent state, which you define. In this task, we will create an association to apply to the Windows servers with a job that will regularly update the local administrator password to the value previously created in the parameter store. Create Association 1. Click State Manager from the SYSTEMS MANAGER SERVICES menu 2. Click Create Association 3. For Association Name, Type SetWindowsLocalAdminPassword 4. For Document select AWS-RunPowerShellScript 5. Click Select Targets by Specifying a Tag and in the selection box choose Tag Name \u2013 Platform, Tag Value \u2013 Windows 6. We will leave the Schedule to execute Every 30 Minutes 7. In the commands detail, use the following values: Net.exe user administrator {{ ssm : WindowsAdministratorPassword }} Leave the default values for the Working Directory and Execution Timeout and Click Create Association After the Association has been successfully created, Click Close LAB D \u2013 TASK 10 - Execute Association 1. As the task may not execute for 30 minutes, Click State Manager from the SYSTEMS MANAGER SERVICES menu 2. Click on the SetWindowsLocalAdminPassword Association 3. Click Instances to view the targeted instances and the status 4. To force the association, Click Apply Association Now LAB D \u2013 TASK 11 - Test Association 1. To test that the local administrator password has been successfully changed on your Windows instances, ensure you are logged out of any remote desktop sessions. 2. Connect using remote desktop to your existing Windows instance using the local computer administrator account with the new password: - User: .\\administrator - Password: L0calAdm1nPa55word@ LAB D \u2013 TASK 12 - Maintenance Windows In this task, we will define a maintenance window for patches to be applied at 4 PM on every Tuesday for 4 hours. Create a Maintenance Window 1. Click Maintenance Windows from the SYSTEMS MANAGER SHARED RESOURCES menu 2. Click Create a Maintenance Window 3. For Name Type MyNightlyMaintenanceWindow 4. Click CRON/Rate expression 5. For CRON/Rate expression , Type cron(0 16 ? TUE ) For Duration, Type 4 For Stop initiating tasks, Type 1 This will create a maintenance Window that runs at 4 PM on every Tuesday for 4 hours, with a 1 hour cutoff Click Create maintenance window LAB D \u2013 TASK 13 - Register targets with a Maintenance Window 1. Click Maintenance Windows from the SYSTEMS MANAGER SHARED RESOURCES menu 2. Click MyNightlyMaintenanceWindow from the maintenance windows 3. Click Actions , Register targets from the drop-down menu 4. For Target Name Type MyWindowsInstances 5. Click Select Targets by Specifying a Tag and in the selection box choose Tag Name \u2013 Platform , Tag Value \u2013 Windows Click Register targets Once target registration has been successful, Click Close LAB D \u2013 TASK 14 - Patch Manager In this task, we will configure Patch Manager to help you select and deploy operating system and software patches automatically to your instances by using the default patch baseline. Register the patch manager task with the maintenance windows 1. Click Maintenance Windows from the SYSTEMS MANAGER SHARED RESOURCES menu 2. Click MyNightlyMaintenanceWindow from the maintenance windows 3. Click Actions , Register run command task from the drop-down menu 4. For Name , Type WindowsPatching 5. In the Document section, choose AWS-RunPatchBaseline 6. In Strict targets ensure your Maintenance Window target is selected, this may be identified by a unique ID rather than by name LAB D \u2013 TASK 15 - Create the required IAM role 1. For Role , click Add new custom role 2. Click Create role 3. Select EC2 , and click Next: Permissions 4. In Policy type , Type AmazonSSMMaintenanceWindowRole Select the AmazonSSMMaintenanceWindowRole policy and click Next: Review For Role name type * ec2_AmazonSSMMaintenanceWindowRole Click Create role LAB D \u2013 TASK 16 - Complete the configuration 1. Return to your AWS Console windows and click the Refresh icon next to the Role 2. You should now be able to select the newly created role within the Role drop-down For Execute on enter 5 For Stop after enter 5 For Operation, change to Install Leave all other fields as default and click Register task Once the task has been successfully registered, click Close Select your Maintenance Window task and check the associated tasks and targets to check its setup correctly Congratulations you have now successfully configured a Maintenance Window to execute at 4 PM on every Tuesday for 4 hours that will patch all your Windows instances 5 at a time LAB D \u2013 TASK 17 - Run Patch Baseline Bonus Task As we won't be around next Tuesday at 4PM, execute a patch baseline against our Windows instances to view the patch status immediately Execute a Run command on your instances using AWS-RunPatchBaseline, Note: it can take >10 minutes to execute on the instances Once complete select your instances in Patch Compliance and view the compliance status From Managed Instances click Filter by attributes and select AWS:ComplianceItem.Classification : equals : SecurityUpdates and AWS:PatchSummary.MissingCount : greater-than : 0 Try some more advanced search filters, can you identify any missing security updates on your instances? Congratulations, you have now completed all Lab activities","title":"LAB D - BONUS LAB \u2013 AWS Systems Manager"},{"location":"reinvent2018/win309/win309index/","text":"re:Invent 2018 - Architecting Active Directory in AWS Welcome It is estimated that over 70% of our Enterprise customer workloads are based on Microsoft Windows. In most enterprises, Microsoft Active Directory (AD) is the foundation for authentication and security. Because Active Directory touches so many facets of a company's infrastructure, it is a vital component in any cloud migration strategy. AWS Microsoft Active Directory is a full Microsoft Active Directory that provides greater compatibility with Microsoft products requiring integration. Customers can use it as a primary user directory in the AWS cloud for use with AD-aware applications such as SharePoint and Amazon RDS for SQL Server, or they use it as a resource directory by connecting it to their self-managed AD infrastructure. In this workshop, we are going to show you how easy it is to securely implement Active Directory in AWS, as well as discuss the different deployment options. The structure of this Workshop involves showing you two Active Directory patterns commonly observed in the field, discussing their use cases, and then guiding you through implementing them inside a fully functional AWS account. Scenario 1 \u2013 Extending your on-premise AD domain into AWS using EC2 Instances. Scenario 2 - Implementing AWS Directory Services for Active Directory and creating a trust relationship between AWS and your on-premise AD Domain. You can jump straight to the scenario that best fits your business, however, you should also have enough time to fully complete both scenarios in this workshop and if you complete both labs we've added a Bonus lab focusing on AWS Systems Manager. To make this possible, we have pre-configured an account with the assets you need. As illustrated above, we have created two VPC\u2019s in a single AWS Account. VPC01 represents your primary AWS VPC. It is what we would expect you to have in your own AWS VPC. We have also created an AWS Directory Services domain with a domain name of CORP.EXAMPLE.COM in this VPC. The reason for pre-creating the AWS Directory Service ahead of the workshop is to save ourselves the 20 minutes it takes to provision. Since we do not have the ability to provide you with an on-premise environment for this workshop, we have placed all the servers we need in a separate VPC. VPC02 simulates an on-premise environment with two servers already created. One server acting as an Active Directory Domain Controller for ACME.COM in a private subnet, the other with Active Directory Management Tools already installed. The on-premise to AWS communications are simulated by VPC peering between the two VPC\u2019s. We have also prepared two additional labs should you finish LABs A & B. LAB C steps you through Federation to the console using AWS Directory Services and LAB D takes a walkthrough of some key AWS Systems Manager functions that we think you'll use for your Microsoft Workloads in AWS.","title":"Home"},{"location":"reinvent2018/win309/win309index/#reinvent-2018-architecting-active-directory-in-aws","text":"","title":"re:Invent 2018 - Architecting Active Directory in AWS"},{"location":"reinvent2018/win309/win309index/#welcome","text":"It is estimated that over 70% of our Enterprise customer workloads are based on Microsoft Windows. In most enterprises, Microsoft Active Directory (AD) is the foundation for authentication and security. Because Active Directory touches so many facets of a company's infrastructure, it is a vital component in any cloud migration strategy. AWS Microsoft Active Directory is a full Microsoft Active Directory that provides greater compatibility with Microsoft products requiring integration. Customers can use it as a primary user directory in the AWS cloud for use with AD-aware applications such as SharePoint and Amazon RDS for SQL Server, or they use it as a resource directory by connecting it to their self-managed AD infrastructure. In this workshop, we are going to show you how easy it is to securely implement Active Directory in AWS, as well as discuss the different deployment options. The structure of this Workshop involves showing you two Active Directory patterns commonly observed in the field, discussing their use cases, and then guiding you through implementing them inside a fully functional AWS account. Scenario 1 \u2013 Extending your on-premise AD domain into AWS using EC2 Instances. Scenario 2 - Implementing AWS Directory Services for Active Directory and creating a trust relationship between AWS and your on-premise AD Domain. You can jump straight to the scenario that best fits your business, however, you should also have enough time to fully complete both scenarios in this workshop and if you complete both labs we've added a Bonus lab focusing on AWS Systems Manager. To make this possible, we have pre-configured an account with the assets you need. As illustrated above, we have created two VPC\u2019s in a single AWS Account. VPC01 represents your primary AWS VPC. It is what we would expect you to have in your own AWS VPC. We have also created an AWS Directory Services domain with a domain name of CORP.EXAMPLE.COM in this VPC. The reason for pre-creating the AWS Directory Service ahead of the workshop is to save ourselves the 20 minutes it takes to provision. Since we do not have the ability to provide you with an on-premise environment for this workshop, we have placed all the servers we need in a separate VPC. VPC02 simulates an on-premise environment with two servers already created. One server acting as an Active Directory Domain Controller for ACME.COM in a private subnet, the other with Active Directory Management Tools already installed. The on-premise to AWS communications are simulated by VPC peering between the two VPC\u2019s. We have also prepared two additional labs should you finish LABs A & B. LAB C steps you through Federation to the console using AWS Directory Services and LAB D takes a walkthrough of some key AWS Systems Manager functions that we think you'll use for your Microsoft Workloads in AWS.","title":"Welcome"},{"location":"reinvent2018/win310/bonus/","text":"Bonus Exercises If you have finished the other exercises and want something else to work on, here are a few ideas. There are no step-by-step instructions, but you can do it. Database Migration Service (DMS) AWS Database Migration Service (AWS DMS) is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. You might want to use DMS rather than a backup/restore minimize the outage needed during the migration. Migrate the dms_sample from the ONPREM server to RDS using DMS. Read the getting started guide . Simple Email Service (SES) Amazon Simple Email Service (Amazon SES) is a cloud-based email sending service designed to help digital marketers and application developers send marketing, notification, and transactional emails. It is a reliable, cost-effective service for businesses of all sizes that use email to keep in contact with their customers. RDS does not support SQL Mail. If we choose RDS, we need an alternative method for sending emails. One alternative is to use SES. Create a lambda function the reads from the confirmation_email_queue , send emails using SES, and updated the ticket_purchase_hist table to indicate the email has been sent. Read Building Lambda Functions with C# and Configuring a Lambda Function to Access Resources in an Amazon VPC .","title":"Bonus"},{"location":"reinvent2018/win310/bonus/#bonus-exercises","text":"If you have finished the other exercises and want something else to work on, here are a few ideas. There are no step-by-step instructions, but you can do it.","title":"Bonus Exercises"},{"location":"reinvent2018/win310/bonus/#database-migration-service-dms","text":"AWS Database Migration Service (AWS DMS) is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. You might want to use DMS rather than a backup/restore minimize the outage needed during the migration. Migrate the dms_sample from the ONPREM server to RDS using DMS. Read the getting started guide .","title":"Database Migration Service (DMS)"},{"location":"reinvent2018/win310/bonus/#simple-email-service-ses","text":"Amazon Simple Email Service (Amazon SES) is a cloud-based email sending service designed to help digital marketers and application developers send marketing, notification, and transactional emails. It is a reliable, cost-effective service for businesses of all sizes that use email to keep in contact with their customers. RDS does not support SQL Mail. If we choose RDS, we need an alternative method for sending emails. One alternative is to use SES. Create a lambda function the reads from the confirmation_email_queue , send emails using SES, and updated the ticket_purchase_hist table to indicate the email has been sent. Read Building Lambda Functions with C# and Configuring a Lambda Function to Access Resources in an Amazon VPC .","title":"Simple Email Service (SES)"},{"location":"reinvent2018/win310/links/","text":"Links Best Practices for Deploying Microsoft SQL Server on AWS SQL Server on the AWS Cloud: Quick Start Reference Deployment SQL Server Performance on AWS RDS Native Backup and Restore Amazon Aurora Database Migration Service Schema Conversion Tool 10 Years of Microsoft on AWS Slideshare DB Best - Azure DB Best - AWS Cloud Harmony RDS Docs","title":"Links"},{"location":"reinvent2018/win310/links/#links","text":"Best Practices for Deploying Microsoft SQL Server on AWS SQL Server on the AWS Cloud: Quick Start Reference Deployment SQL Server Performance on AWS RDS Native Backup and Restore Amazon Aurora Database Migration Service Schema Conversion Tool 10 Years of Microsoft on AWS Slideshare DB Best - Azure DB Best - AWS Cloud Harmony RDS Docs","title":"Links"},{"location":"reinvent2018/win310/refactor/","text":"Refactor This is an opportunity to explore other options besides SQL Server. While there are many AWS services you could choose from, we will assume Amazon Aurora running MySQL. In this case, you will need to migrate both the schema and data from SQL Server to MySQL. Proposed Solution Spoiler Alert Before you read the proposed solution spend time talking with your team members. Log into the on-prem SQL Server Log into your account using the credentials provided. Change the region to Ireland from the menu in the top left corner. Choose EC2 from the list of services, and click on Instances . Select the server named ONPREM , and click the Connect button at the top of the screen. Download the Remote Desktop File and connect as Admin@example.com and the password provided. Run AWS Schema Conversion Tool Click the start button. This is Windows 2012, so you need to find that magic pixel in the bottom left corner. Launch the AWS Schema Conversion Tool . From the File menu, choose New Project . Change the Source Database Engine to Microsoft SQL Server . Change the Destination Database Engine to Amazon Aurora (MySQL Compatible) . Compare to the diagram below and click OK . Connect to Microsoft SQL Server Choose Connect to Microsoft SQL Server from the menu bar. Fill in the dialog box as shown below and click OK . Note: If you are prompted for a driver path, use C:\\Program Files\\AWS Schema Conversion Tool\\drivers\\sqljdbc42.jar . Generate a report Select dms_sample from the tree. Right click and choose Create report . Review the report Review the report as a team. Notice that schema conversion is expected to be easy, but there is work to do on the stored procedures. Switch to the Action Items tab. Review the issues found as a team. On the right side, expand **Issue 692: MySQL doesn't support cursor variables\". Click on one of the impacted procedures to see the DDL. Based on the findings would you recommend moving to MySQL? Repeat these steps for PostgreSQL Rerun the above steps for Amazon Aurora (PostgreSQL Compatible) . Notice that the schema conversion is much easier this time. Based on the findings would you recommend moving to PostgreSQL?","title":"Refactor"},{"location":"reinvent2018/win310/refactor/#refactor","text":"This is an opportunity to explore other options besides SQL Server. While there are many AWS services you could choose from, we will assume Amazon Aurora running MySQL. In this case, you will need to migrate both the schema and data from SQL Server to MySQL.","title":"Refactor"},{"location":"reinvent2018/win310/refactor/#proposed-solution","text":"Spoiler Alert Before you read the proposed solution spend time talking with your team members.","title":"Proposed Solution"},{"location":"reinvent2018/win310/refactor/#log-into-the-on-prem-sql-server","text":"Log into your account using the credentials provided. Change the region to Ireland from the menu in the top left corner. Choose EC2 from the list of services, and click on Instances . Select the server named ONPREM , and click the Connect button at the top of the screen. Download the Remote Desktop File and connect as Admin@example.com and the password provided.","title":"Log into the on-prem SQL Server"},{"location":"reinvent2018/win310/refactor/#run-aws-schema-conversion-tool","text":"Click the start button. This is Windows 2012, so you need to find that magic pixel in the bottom left corner. Launch the AWS Schema Conversion Tool . From the File menu, choose New Project . Change the Source Database Engine to Microsoft SQL Server . Change the Destination Database Engine to Amazon Aurora (MySQL Compatible) . Compare to the diagram below and click OK .","title":"Run AWS Schema Conversion Tool"},{"location":"reinvent2018/win310/refactor/#connect-to-microsoft-sql-server","text":"Choose Connect to Microsoft SQL Server from the menu bar. Fill in the dialog box as shown below and click OK . Note: If you are prompted for a driver path, use C:\\Program Files\\AWS Schema Conversion Tool\\drivers\\sqljdbc42.jar .","title":"Connect to Microsoft SQL Server"},{"location":"reinvent2018/win310/refactor/#generate-a-report","text":"Select dms_sample from the tree. Right click and choose Create report .","title":"Generate a report"},{"location":"reinvent2018/win310/refactor/#review-the-report","text":"Review the report as a team. Notice that schema conversion is expected to be easy, but there is work to do on the stored procedures. Switch to the Action Items tab. Review the issues found as a team. On the right side, expand **Issue 692: MySQL doesn't support cursor variables\". Click on one of the impacted procedures to see the DDL. Based on the findings would you recommend moving to MySQL?","title":"Review the report"},{"location":"reinvent2018/win310/refactor/#repeat-these-steps-for-postgresql","text":"Rerun the above steps for Amazon Aurora (PostgreSQL Compatible) . Notice that the schema conversion is much easier this time. Based on the findings would you recommend moving to PostgreSQL?","title":"Repeat these steps for PostgreSQL"},{"location":"reinvent2018/win310/rehost/","text":"Rehost Rehost is essentially a lift-and-shift. In this case you will run SQL Server on Amazon Elastic Compute Cloud (EC2). This will be a relatively easy migration, but you will need to address a few issues. For example, EC2 does not allow shared storage, therefore you will need evaluate new options. Most likely Always On Availability Groups. You will also need to decide how to migrate data. Proposed Solution Spoiler Alert Before you read the proposed solution spend time talking with your team members. Log into ONPREM NOTE: we are using ONPREM as a jump box (e.g. bastion host) Log into your account using the credentials provided. Change the region to Ireland from the menu in the top left corner. Choose EC2 from the list of services, and click on Instances . Select the server named ONPREM , and click the Connect button at the top of the screen. Download the Remote Desktop File and connect as Admin@example.com and the password provided. Log into the WSFCNode1 Click the start button. This is Windows 2012, so you need to find that magic pixel in the bottom left corner. Launch Remote Desktop Connection . Enter 10.0.0.100 as the computer name. Log in as Admin@example.com and the password provided. Download the backup file Start a new PowerShell session as an administrator. Run the following command to download the file. This will take a few minutes. Read-S3Object -BucketName win310 -Key MSSQL2008R2_01.bak -File F:\\MSSQL\\Backup\\MSSQL2008R2_01.bak Set up Permissions for the Cluster Object Run Windows PowerShell as an administrator, and use the following command to install Active Directory Management Services: Add-WindowsFeature RSAT-ADDS-Tools Open Active Directory Users and Computers In the navigation bar, choose View, Advanced Features to see the advanced features for Active Directory Users and Computers. Right click on Computers (see below) and click Properties . On the Security tab, choose Advanced . In the Advanced Security Settings dialog box, choose Add . Next to Principal, choose Select a principal. Choose Object Types , select Computers , and then choose OK. Type WSFCLUSTER1 , choose Check Names, and then choose OK. Add the Create Computer objects permission to this principal, and then choose OK. In the Advanced Security Settings for Computers screen, choose OK. In the Computer Properties screen, choose OK. Connect to SQL Server Click the start button. Right click on Microsoft SQL Server Management Studio , and Run as a different user . Run as sqlsa@example.com with the password provided. On the Connect to Server dialog, leave the defaults and click Connect . Note: we are connecting as sqlsa because the domain admin does not have access to the database instance. It is generally a good idea to keep these roles separate. Restore the Backup File In Object Explorer, right click on Databases and choose Restore database . Find the file you downloaded earlier and restore it. For example: Create an Availability Group In Object Explorer, right-click AlwaysOn High Availability and launch the New Availability Group wizard. On the Introduction page, choose Next . On the Specify Availability Group Name page, type SQLAG1 , and then choose Next . On the Select Databases page, choose the database you created or attached in the previous section, and then choose Next . On the Specify Replicas page, add the second cluster node WSFCNode2 , and then choose Automatic Failover. It should look like this. On the Listener tab, choose Create an availability group listener , and provide a Listener DNS Name AG1-Listener . Then specify the TPC port used by this listener as 1433 . Add the two private subnets the cluster nodes were deployed into and a corresponding IPv4 address. The results should look like this: . Click Next On the Select Initial Data Synchronization page, choose Full database and log backup . In the shared network location box, type \\\\WSFCFileServer\\replica . Choose Next . On the Validation page, choose Next . On the Summary page, choose Finish, and then close the wizard. Simulate a Failover In Object Explorer, click Connect and then Database Engine . Leave the existing connection open. Enter AG1-Listner as the server name and click Connect . Navigate to Always on High Availability > Availability Groups . Right click on SQLAG1 and choose Failover . Click Next and Next again. On the Connect to Replica screen, click Connect . On the Connect to Server dialog, click Connect . Click Next and then Finish . Test the Replica You should still have WSFCNode1 in the object explorer. Expand WSFCNode1 > Databases > dms_sample . You should get an error because the database is now running on WSFCNode2. Expand AG1-Listner > Databases > dms_sample . You can still access the database because AG1-Listner always points to the active node.","title":"Rehost"},{"location":"reinvent2018/win310/rehost/#rehost","text":"Rehost is essentially a lift-and-shift. In this case you will run SQL Server on Amazon Elastic Compute Cloud (EC2). This will be a relatively easy migration, but you will need to address a few issues. For example, EC2 does not allow shared storage, therefore you will need evaluate new options. Most likely Always On Availability Groups. You will also need to decide how to migrate data.","title":"Rehost"},{"location":"reinvent2018/win310/rehost/#proposed-solution","text":"Spoiler Alert Before you read the proposed solution spend time talking with your team members.","title":"Proposed Solution"},{"location":"reinvent2018/win310/rehost/#log-into-onprem","text":"NOTE: we are using ONPREM as a jump box (e.g. bastion host) Log into your account using the credentials provided. Change the region to Ireland from the menu in the top left corner. Choose EC2 from the list of services, and click on Instances . Select the server named ONPREM , and click the Connect button at the top of the screen. Download the Remote Desktop File and connect as Admin@example.com and the password provided.","title":"Log into ONPREM"},{"location":"reinvent2018/win310/rehost/#log-into-the-wsfcnode1","text":"Click the start button. This is Windows 2012, so you need to find that magic pixel in the bottom left corner. Launch Remote Desktop Connection . Enter 10.0.0.100 as the computer name. Log in as Admin@example.com and the password provided.","title":"Log into the WSFCNode1"},{"location":"reinvent2018/win310/rehost/#download-the-backup-file","text":"Start a new PowerShell session as an administrator. Run the following command to download the file. This will take a few minutes. Read-S3Object -BucketName win310 -Key MSSQL2008R2_01.bak -File F:\\MSSQL\\Backup\\MSSQL2008R2_01.bak","title":"Download the backup file"},{"location":"reinvent2018/win310/rehost/#set-up-permissions-for-the-cluster-object","text":"Run Windows PowerShell as an administrator, and use the following command to install Active Directory Management Services: Add-WindowsFeature RSAT-ADDS-Tools Open Active Directory Users and Computers In the navigation bar, choose View, Advanced Features to see the advanced features for Active Directory Users and Computers. Right click on Computers (see below) and click Properties . On the Security tab, choose Advanced . In the Advanced Security Settings dialog box, choose Add . Next to Principal, choose Select a principal. Choose Object Types , select Computers , and then choose OK. Type WSFCLUSTER1 , choose Check Names, and then choose OK. Add the Create Computer objects permission to this principal, and then choose OK. In the Advanced Security Settings for Computers screen, choose OK. In the Computer Properties screen, choose OK.","title":"Set up Permissions for the Cluster Object"},{"location":"reinvent2018/win310/rehost/#connect-to-sql-server","text":"Click the start button. Right click on Microsoft SQL Server Management Studio , and Run as a different user . Run as sqlsa@example.com with the password provided. On the Connect to Server dialog, leave the defaults and click Connect . Note: we are connecting as sqlsa because the domain admin does not have access to the database instance. It is generally a good idea to keep these roles separate.","title":"Connect to SQL Server"},{"location":"reinvent2018/win310/rehost/#restore-the-backup-file","text":"In Object Explorer, right click on Databases and choose Restore database . Find the file you downloaded earlier and restore it. For example:","title":"Restore the Backup File"},{"location":"reinvent2018/win310/rehost/#create-an-availability-group","text":"In Object Explorer, right-click AlwaysOn High Availability and launch the New Availability Group wizard. On the Introduction page, choose Next . On the Specify Availability Group Name page, type SQLAG1 , and then choose Next . On the Select Databases page, choose the database you created or attached in the previous section, and then choose Next . On the Specify Replicas page, add the second cluster node WSFCNode2 , and then choose Automatic Failover. It should look like this. On the Listener tab, choose Create an availability group listener , and provide a Listener DNS Name AG1-Listener . Then specify the TPC port used by this listener as 1433 . Add the two private subnets the cluster nodes were deployed into and a corresponding IPv4 address. The results should look like this: . Click Next On the Select Initial Data Synchronization page, choose Full database and log backup . In the shared network location box, type \\\\WSFCFileServer\\replica . Choose Next . On the Validation page, choose Next . On the Summary page, choose Finish, and then close the wizard.","title":"Create an Availability Group"},{"location":"reinvent2018/win310/rehost/#simulate-a-failover","text":"In Object Explorer, click Connect and then Database Engine . Leave the existing connection open. Enter AG1-Listner as the server name and click Connect . Navigate to Always on High Availability > Availability Groups . Right click on SQLAG1 and choose Failover . Click Next and Next again. On the Connect to Replica screen, click Connect . On the Connect to Server dialog, click Connect . Click Next and then Finish .","title":"Simulate a Failover"},{"location":"reinvent2018/win310/rehost/#test-the-replica","text":"You should still have WSFCNode1 in the object explorer. Expand WSFCNode1 > Databases > dms_sample . You should get an error because the database is now running on WSFCNode2. Expand AG1-Listner > Databases > dms_sample . You can still access the database because AG1-Listner always points to the active node.","title":"Test the Replica"},{"location":"reinvent2018/win310/replatform/","text":"Replatform In this case you will migrate to Amazon Relational Database Service (RDS) running SQL Server 2017. RDS makes it easy to set up, operate, and scale a relational database in the cloud. Again, you will have a few issues to address. Most notably, RDS does not support database mail. Therefore, you will need to find another option to send email notifications. Again, you will need to decide how to migrate data. Proposed Solution Spoiler Alert Before you read the proposed solution spend time talking with your team members. Create an S3 Bucket Log into your account using the credentials provided. Change the region to Ireland from the menu in the top left corner. Choose S3 from the list of services Click on Create bucket . Name the bucket and click Create . Note: You will need the name later Configure RDS for Native Backup/Restore Choose RDS from the list of services, and click on Option groups . Click the Create Group button. Fill out the form as shown below and click Create . Click the check box next to the group you just created and click Add Option . Ensure that SQLSERVER_BACKUP_RESTORE is selected in the option dropdown. Under IAM Role , select the No radio button. Choose the SQLServerNativeBackupRestore IAM Role. Choose Yes under the Apply Immediately and click Add Option . Modify the Instance Click on Instance from the left navigation. Click radio button next to win310-rdssqlinstance . From the Instance Actions menu choose Modify . Find the Database Options section. Change the Option Group to sqlservernativebackuprestore . Scroll down and click the Continue button. Choose Apply Immediately and click Modify DB Instance . Copy the RDS Endpoint Click on Instance from the left navigation. Click the win310-rdssqlinstance link. Find the Connect section and copy the Endpoint . You will need this below. Log into the on-prem SQL Server Choose EC2 from the list of services, and click on Instances . Select the server named ONPREM , and click the Connect button at the top of the screen. Download the Remote Desktop File and connect as Admin@example.com and the password provided. Copy a backup to S3 Note: there is a recent backup in the root of the C drive. Start a new PowerShell session. Run the following command to upload the file. Replace YOUR-BUCKET with the bucket you created earlier Write-S3Object -BucketName YOUR-BUCKET -Key MSSQL2008R2_01.bak -File C:\\MSSQL2008R2_01.bak Connect to RDS Click the start button. This is Windows 2012, so you need to find that magic pixel in the bottom left corner. Launch Microsoft SQL Server Management Studio . In the Connect to Server dialog enter the endpoint you copied above. Use SQL Server Authentication . Use the username sa and the password provided. Note, we are using SQL Credentials (e.g. sa) here because our domain admin account has not been granted access to the database. However, this RDS instance has been joined to the example.com domain and you could use Windows credentials if you wanted. Just add a new login to the database instance. Restore the sample database Click \"New Query\" from the toolbar. Execute the following SQL. NOTE: replace YOUR-BUCKET with the bucket you created above. exec msdb.dbo.rds_restore_database @restore_db_name='dms_sample', @s3_arn_to_restore_from='arn:aws:s3:::YOUR-BUCKET/MSSQL2008R2_01.bak'; It will take a few minutes to restore the database. You can run the following SQL to check the status. exec msdb.dbo.rds_task_status @db_name='dms_sample';","title":"Replatform"},{"location":"reinvent2018/win310/replatform/#replatform","text":"In this case you will migrate to Amazon Relational Database Service (RDS) running SQL Server 2017. RDS makes it easy to set up, operate, and scale a relational database in the cloud. Again, you will have a few issues to address. Most notably, RDS does not support database mail. Therefore, you will need to find another option to send email notifications. Again, you will need to decide how to migrate data.","title":"Replatform"},{"location":"reinvent2018/win310/replatform/#proposed-solution","text":"Spoiler Alert Before you read the proposed solution spend time talking with your team members.","title":"Proposed Solution"},{"location":"reinvent2018/win310/replatform/#create-an-s3-bucket","text":"Log into your account using the credentials provided. Change the region to Ireland from the menu in the top left corner. Choose S3 from the list of services Click on Create bucket . Name the bucket and click Create . Note: You will need the name later","title":"Create an S3 Bucket"},{"location":"reinvent2018/win310/replatform/#configure-rds-for-native-backuprestore","text":"Choose RDS from the list of services, and click on Option groups . Click the Create Group button. Fill out the form as shown below and click Create . Click the check box next to the group you just created and click Add Option . Ensure that SQLSERVER_BACKUP_RESTORE is selected in the option dropdown. Under IAM Role , select the No radio button. Choose the SQLServerNativeBackupRestore IAM Role. Choose Yes under the Apply Immediately and click Add Option .","title":"Configure RDS for Native Backup/Restore"},{"location":"reinvent2018/win310/replatform/#modify-the-instance","text":"Click on Instance from the left navigation. Click radio button next to win310-rdssqlinstance . From the Instance Actions menu choose Modify . Find the Database Options section. Change the Option Group to sqlservernativebackuprestore . Scroll down and click the Continue button. Choose Apply Immediately and click Modify DB Instance .","title":"Modify the Instance"},{"location":"reinvent2018/win310/replatform/#copy-the-rds-endpoint","text":"Click on Instance from the left navigation. Click the win310-rdssqlinstance link. Find the Connect section and copy the Endpoint . You will need this below.","title":"Copy the RDS Endpoint"},{"location":"reinvent2018/win310/replatform/#log-into-the-on-prem-sql-server","text":"Choose EC2 from the list of services, and click on Instances . Select the server named ONPREM , and click the Connect button at the top of the screen. Download the Remote Desktop File and connect as Admin@example.com and the password provided.","title":"Log into the on-prem SQL Server"},{"location":"reinvent2018/win310/replatform/#copy-a-backup-to-s3","text":"Note: there is a recent backup in the root of the C drive. Start a new PowerShell session. Run the following command to upload the file. Replace YOUR-BUCKET with the bucket you created earlier Write-S3Object -BucketName YOUR-BUCKET -Key MSSQL2008R2_01.bak -File C:\\MSSQL2008R2_01.bak","title":"Copy a backup to S3"},{"location":"reinvent2018/win310/replatform/#connect-to-rds","text":"Click the start button. This is Windows 2012, so you need to find that magic pixel in the bottom left corner. Launch Microsoft SQL Server Management Studio . In the Connect to Server dialog enter the endpoint you copied above. Use SQL Server Authentication . Use the username sa and the password provided. Note, we are using SQL Credentials (e.g. sa) here because our domain admin account has not been granted access to the database. However, this RDS instance has been joined to the example.com domain and you could use Windows credentials if you wanted. Just add a new login to the database instance.","title":"Connect to RDS"},{"location":"reinvent2018/win310/replatform/#restore-the-sample-database","text":"Click \"New Query\" from the toolbar. Execute the following SQL. NOTE: replace YOUR-BUCKET with the bucket you created above. exec msdb.dbo.rds_restore_database @restore_db_name='dms_sample', @s3_arn_to_restore_from='arn:aws:s3:::YOUR-BUCKET/MSSQL2008R2_01.bak'; It will take a few minutes to restore the database. You can run the following SQL to check the status. exec msdb.dbo.rds_task_status @db_name='dms_sample';","title":"Restore the sample database"},{"location":"reinvent2018/win310/scenario/","text":"Workshop Scenario In this workshop you will work in teams to develop a strategy to migrate a SQL Server 2008R2 environment to AWS. There are no correct answers, but we inlclude three likely solutions. The requirements are as follows. Overview You are a contractor for an online ticket broker that sells tickets to sporting events, concerts, etc. The company stores data in a SQL Server 2008 R2. SQL Server 2008 R2 is end-of-life and the company is currently paying for extended support. The company wants to upgrade to SQL Server 2017. They see this upgrade as an opportunity to migrate to AWS. Architecture On Prem, SQL Server is running Enterprise Edition. HA is achieved using a failover cluster with shared storage on a Fibre Channel attached SAN. SQL Server is running on physical servers with dual socket, two core (i.e. 8 vCPU) and 16GB RAM in each server. The application uses SQL Mail to send order confirmations to customers. The database schema is available here. Humans The CIO is excited about moving to AWS, and is open to your recommendations. The CFO, not surprisingly, wants to minimize cost. The CISO is open to AWS, but wants to ensure you can achieve end-to-end encryption. The DBA team has some experience with MySQL but are most comfortable in SQL Server. The DEV team is comfortable in multiple languages and operating systems.","title":"Scenario"},{"location":"reinvent2018/win310/scenario/#workshop-scenario","text":"In this workshop you will work in teams to develop a strategy to migrate a SQL Server 2008R2 environment to AWS. There are no correct answers, but we inlclude three likely solutions. The requirements are as follows.","title":"Workshop Scenario"},{"location":"reinvent2018/win310/scenario/#overview","text":"You are a contractor for an online ticket broker that sells tickets to sporting events, concerts, etc. The company stores data in a SQL Server 2008 R2. SQL Server 2008 R2 is end-of-life and the company is currently paying for extended support. The company wants to upgrade to SQL Server 2017. They see this upgrade as an opportunity to migrate to AWS.","title":"Overview"},{"location":"reinvent2018/win310/scenario/#architecture","text":"On Prem, SQL Server is running Enterprise Edition. HA is achieved using a failover cluster with shared storage on a Fibre Channel attached SAN. SQL Server is running on physical servers with dual socket, two core (i.e. 8 vCPU) and 16GB RAM in each server. The application uses SQL Mail to send order confirmations to customers. The database schema is available here.","title":"Architecture"},{"location":"reinvent2018/win310/scenario/#humans","text":"The CIO is excited about moving to AWS, and is open to your recommendations. The CFO, not surprisingly, wants to minimize cost. The CISO is open to AWS, but wants to ensure you can achieve end-to-end encryption. The DBA team has some experience with MySQL but are most comfortable in SQL Server. The DEV team is comfortable in multiple languages and operating systems.","title":"Humans"},{"location":"reinvent2018/win310/win310index/","text":"Home Welcome to WIN310 - HANDS-ON: BUILDING A MIGRATION STRATEGY FOR SQL SERVER ON AWS Environment Your account has the following resources already created for you in eu-west-1 (Ireland). It was created with this template . The AMI for the ONPREM server in eu-west-1 is ami-08da693038d9ca4c4 . The on-prem server represents the source environment. We have also deployed SQL Server 2017 on both EC2 and RDS to be used as potential targets. Feel free add additional resources if needed. Notice that only the ONPREM server is accessible from the internet. Use this a jump box (i.e. bastion host). It has SQL Server Management Studio and the Schema Conversion Tool installed. Add anything else you want. All servers are joined to the example.com domain. The domain admin account is Admin@example.com and has access the SQL server instances on ONPREM. sqlsa@wxample.com is the service account for the SQL cluster WSFCNode1 and WSFCNode2. The RDS instance is domain joined, but no domain logins have been created. You can log into RDS and ONPREM using the SQL login sa. All users have the same password which will be shared during the workshop. Helpful Links Best Practices for Deploying Microsoft SQL Server on AWS SQL Server on the AWS Cloud: Quick Start Reference Deployment SQL Server Performance on AWS RDS Native Backup and Restore Amazon Aurora Database Migration Service Schema Conversion Tool Resources Sample database on GitHub A 1GB full backup without tickets A 9GB full backup with tickets","title":"Home"},{"location":"reinvent2018/win310/win310index/#home","text":"Welcome to WIN310 - HANDS-ON: BUILDING A MIGRATION STRATEGY FOR SQL SERVER ON AWS","title":"Home"},{"location":"reinvent2018/win310/win310index/#environment","text":"Your account has the following resources already created for you in eu-west-1 (Ireland). It was created with this template . The AMI for the ONPREM server in eu-west-1 is ami-08da693038d9ca4c4 . The on-prem server represents the source environment. We have also deployed SQL Server 2017 on both EC2 and RDS to be used as potential targets. Feel free add additional resources if needed. Notice that only the ONPREM server is accessible from the internet. Use this a jump box (i.e. bastion host). It has SQL Server Management Studio and the Schema Conversion Tool installed. Add anything else you want. All servers are joined to the example.com domain. The domain admin account is Admin@example.com and has access the SQL server instances on ONPREM. sqlsa@wxample.com is the service account for the SQL cluster WSFCNode1 and WSFCNode2. The RDS instance is domain joined, but no domain logins have been created. You can log into RDS and ONPREM using the SQL login sa. All users have the same password which will be shared during the workshop.","title":"Environment"},{"location":"reinvent2018/win310/win310index/#helpful-links","text":"Best Practices for Deploying Microsoft SQL Server on AWS SQL Server on the AWS Cloud: Quick Start Reference Deployment SQL Server Performance on AWS RDS Native Backup and Restore Amazon Aurora Database Migration Service Schema Conversion Tool","title":"Helpful Links"},{"location":"reinvent2018/win310/win310index/#resources","text":"Sample database on GitHub A 1GB full backup without tickets A 9GB full backup with tickets","title":"Resources"},{"location":"reinvent2018/win313/ec2/","text":"Deploy an app to EC2 using Azure DevOps (VSTS) Introduction This article will demonstrate a .Net application deployment to AWS Ec2 using AWS CodeDeploy, GitHub, and Azure DevOps. Prerequisites Complete the Lab Setup Architecture Modules Import Code into Azure DevOps Setup AWS CodeDeploy Integrate Azure DevOps and AWS CodeDeploy Test the Deployment Import code into Azure DevOps In this module we will pull a .NET application from GitHub to be used in our deployment. Go to https://dev.azure.com/ and login to your Azure DevOps account. Click New Project Fill in the name, description and take the default setting for the other options. Visibility: Private Version Control: Git Work item Process: Agile Click Create. Next, we will import the code. Select the Repos tab on the left and then click the import button under \u201cor Import a repository.\u201d Put https://github.com/StevenDavid/ASPDotNetMVC in the Clone URL input box. Click Import. Once import is complete, you will see a confirmation screen and then be refreshed to the code view. Your code is now in Azure DevOps. Create CodeDeploy IAM Role In this step we will create the role to be used by CodeDeploy. From the AWS Console search for IAM. Click on \"Roles\" in the left hand menu. Click on the \"Create role\" button. Make sure \"AWS Service\" is selected and click on CodeDeploy in the \"Choose the service that will use this role\" section. Scroll down and in the \"Select your use case\" section, select the top \"CodeDeploy\" option. Click \"Next:Permissions\" Wait for the \"AWSCodeDeployRole\" permissions to load then click \"Next:Tags\" Add whatever tags you want and then click \"Next:Review\" Name the Role something like \"CodeDeployRole\" and then click \"Create role\" Setup AWS CodeDeploy In this step we will create an application in CodeDeploy and a deployment group. From the AWS Console search for CodeDeploy. Note: It will take you to the new version of the interface. Click the \"Return to the old experience\" in the bottom left hand side of the screen. Click the Get Started Now button Note: if you are on the new version of the CodeDeploy interface. Use the link, in the blue bar, at the top of the page to revert to the old version and select the \"Sample Deployment Wizard\" from the right hand side of the page. Chose the Sample Deployment and click Next. Choose In-place deployment and click Next. Change the instance type to Windows, select or create a key pair and click Launch instances. Name your CodeDeploy Application. Click Next. Select the sample application for Windows. This will deploy a generic sample application to your servers. We will overwrite it with the code we checked into Azure DevOps. Click Next. Create a Deployment Group. Click Next. We will select the role we created earlier. Click Next. For the Deployment Configuration we will use the default. Click Next. Review the Deployment details and click Deploy. CodeDeploy will do the initial deployment and is ready to receive our application. Create S3 bucket From the AWS Console search for S3. Click Create bucket. Name the bucket some thing unique, chose the region you are using for codedeploy (Canada is our recommendation) and click next until you create the bucket. Integrate Azure DevOps and AWS CodeDeploy In this step we will create a Azure DevOps build job that will deploy our application to EC2 instances via CodeDeploy. Open our project in Azure DevOps and click on the Pipelines tab. Click New Pipeline. Azure Repos Git is our source repo and we want to publish the master branch. Click Continue. Choose the ASP.NET template and click Apply. Click the + symbol to add a new task. Search for the Copy files task and click Add. The task will go to the bottom of the list on the left. Click the Copy files task and update the parameters. Source Folder: ASPDotNetMVC Target Folder: $(Build.ArtifactStagingDirectory)\\\\publish Click the + symbol to add a new task. Search for the CodeDeploy task and click Add. The Task will go to the bottom of the list on the left. Update the parameters. Note: Your first time you will need to install the AWS Azure DevOps toolkit from the Marketplace. Configure the Code Deploy task. Note: Application name has to match the CodeDeploy Application name. Same is true for Deployment Group Name. Click Triggers and enable Continuous integration. Save the build. Test the deployment In this module, we will deploy the code to the EC2 instances. Make a change to your code and check it into Azure DevOps which initiates a build. Once the build is complete you will be able to click through the various steps in the process. You can view your deployed application by getting the public DNS or IP address for the EC2 instance and to navigate to the site in a browser. You now have a working CI/CD pipeline that deploys an ASP.NET application from source control to EC2.","title":"Amazon EC2"},{"location":"reinvent2018/win313/ec2/#deploy-an-app-to-ec2-using-azure-devops-vsts","text":"","title":"Deploy an app to EC2 using Azure DevOps (VSTS)"},{"location":"reinvent2018/win313/ec2/#introduction","text":"This article will demonstrate a .Net application deployment to AWS Ec2 using AWS CodeDeploy, GitHub, and Azure DevOps.","title":"Introduction"},{"location":"reinvent2018/win313/ec2/#prerequisites","text":"Complete the Lab Setup","title":"Prerequisites"},{"location":"reinvent2018/win313/ec2/#architecture","text":"","title":"Architecture"},{"location":"reinvent2018/win313/ec2/#modules","text":"Import Code into Azure DevOps Setup AWS CodeDeploy Integrate Azure DevOps and AWS CodeDeploy Test the Deployment","title":"Modules"},{"location":"reinvent2018/win313/ec2/#import-code-into-azure-devops","text":"In this module we will pull a .NET application from GitHub to be used in our deployment. Go to https://dev.azure.com/ and login to your Azure DevOps account. Click New Project Fill in the name, description and take the default setting for the other options. Visibility: Private Version Control: Git Work item Process: Agile Click Create. Next, we will import the code. Select the Repos tab on the left and then click the import button under \u201cor Import a repository.\u201d Put https://github.com/StevenDavid/ASPDotNetMVC in the Clone URL input box. Click Import. Once import is complete, you will see a confirmation screen and then be refreshed to the code view. Your code is now in Azure DevOps.","title":"Import code into Azure DevOps"},{"location":"reinvent2018/win313/ec2/#create-codedeploy-iam-role","text":"In this step we will create the role to be used by CodeDeploy. From the AWS Console search for IAM. Click on \"Roles\" in the left hand menu. Click on the \"Create role\" button. Make sure \"AWS Service\" is selected and click on CodeDeploy in the \"Choose the service that will use this role\" section. Scroll down and in the \"Select your use case\" section, select the top \"CodeDeploy\" option. Click \"Next:Permissions\" Wait for the \"AWSCodeDeployRole\" permissions to load then click \"Next:Tags\" Add whatever tags you want and then click \"Next:Review\" Name the Role something like \"CodeDeployRole\" and then click \"Create role\"","title":"Create CodeDeploy IAM Role"},{"location":"reinvent2018/win313/ec2/#setup-aws-codedeploy","text":"In this step we will create an application in CodeDeploy and a deployment group. From the AWS Console search for CodeDeploy. Note: It will take you to the new version of the interface. Click the \"Return to the old experience\" in the bottom left hand side of the screen. Click the Get Started Now button Note: if you are on the new version of the CodeDeploy interface. Use the link, in the blue bar, at the top of the page to revert to the old version and select the \"Sample Deployment Wizard\" from the right hand side of the page. Chose the Sample Deployment and click Next. Choose In-place deployment and click Next. Change the instance type to Windows, select or create a key pair and click Launch instances. Name your CodeDeploy Application. Click Next. Select the sample application for Windows. This will deploy a generic sample application to your servers. We will overwrite it with the code we checked into Azure DevOps. Click Next. Create a Deployment Group. Click Next. We will select the role we created earlier. Click Next. For the Deployment Configuration we will use the default. Click Next. Review the Deployment details and click Deploy. CodeDeploy will do the initial deployment and is ready to receive our application.","title":"Setup AWS CodeDeploy"},{"location":"reinvent2018/win313/ec2/#create-s3-bucket","text":"From the AWS Console search for S3. Click Create bucket. Name the bucket some thing unique, chose the region you are using for codedeploy (Canada is our recommendation) and click next until you create the bucket.","title":"Create S3 bucket"},{"location":"reinvent2018/win313/ec2/#integrate-azure-devops-and-aws-codedeploy","text":"In this step we will create a Azure DevOps build job that will deploy our application to EC2 instances via CodeDeploy. Open our project in Azure DevOps and click on the Pipelines tab. Click New Pipeline. Azure Repos Git is our source repo and we want to publish the master branch. Click Continue. Choose the ASP.NET template and click Apply. Click the + symbol to add a new task. Search for the Copy files task and click Add. The task will go to the bottom of the list on the left. Click the Copy files task and update the parameters. Source Folder: ASPDotNetMVC Target Folder: $(Build.ArtifactStagingDirectory)\\\\publish Click the + symbol to add a new task. Search for the CodeDeploy task and click Add. The Task will go to the bottom of the list on the left. Update the parameters. Note: Your first time you will need to install the AWS Azure DevOps toolkit from the Marketplace. Configure the Code Deploy task. Note: Application name has to match the CodeDeploy Application name. Same is true for Deployment Group Name. Click Triggers and enable Continuous integration. Save the build.","title":"Integrate Azure DevOps and AWS CodeDeploy"},{"location":"reinvent2018/win313/ec2/#test-the-deployment","text":"In this module, we will deploy the code to the EC2 instances. Make a change to your code and check it into Azure DevOps which initiates a build. Once the build is complete you will be able to click through the various steps in the process. You can view your deployed application by getting the public DNS or IP address for the EC2 instance and to navigate to the site in a browser. You now have a working CI/CD pipeline that deploys an ASP.NET application from source control to EC2.","title":"Test the deployment"},{"location":"reinvent2018/win313/fargate/","text":"Deploy a .Net Core application on Amazon ECS Fargate using Azure DevOps (VSTS) In this lab we will create a CICD pipeline on Azure DevOps using AWS Tools for VSTS. Prerequisites Complete the Lab Setup . Validate access to the sample source code repository in Github by browsing to this url https://github.com/awsimaya/ECSFargate.git . It is necessary to follow the instructions carefully to ensure successful completion of the lab. Environment Setup In this section we will configure the environment baseline to get started. Create a new IAM User for Azure DevOps In this section, we will create a new IAM user account that will be used by your Azure DevOps service to deploy resources in your AWS Account. If you already have Access Key ID and Secret Access Key of an IAM User account to be used with Azure DevOps then you can skip this section. Click on IAM Service In the navigation pane, choose Users and then choose Add user . Type the user name for the new user. This is the sign-in name for AWS. Select only programmatic access. This creates an access key for each new user. You can view or download the access keys when you get to the Final page. Click on Next: Permissions . Select Attach existing policies directly. Select Administrator Access. Note: This is not a best practice to provide Administrator Access. Its recommended to follow least access privilege and limit permissions to the resources that would be required from AzureDevOps account. This could vary depending upon your use case and scenarios. Example you may chose to create different IAM User account for Non-Prod Vs Prod and appropriately provide the necessary permissions following least access privilege principle. Click on Next: Review Review the settings and Click on Create User . Save the Secret Access Key and Access Key ID to be used later in this lab. Install AWS Tools for VSTS aka Azure DevOps In this step, we will install AWS Tools for Azure DevOps aka Visual Studio Team Services (VSTS) to your account. This will allow us to leverage the powerful features that this add-on provides to create CICD tasks on AWS Login to your Azure DevOps (formerly VSTS) account Navigate to Visual Studio Marketplace and click on Get it free button as shown in the image below If you're logged in to Azure DevOps already, you will be taken to your Azure DevOps account where you can select the Organization as shown below. Once you click, AWS Tools for VSTS will be installed on the organization successfully. You will screen like the below if the the process was successful. Click Proceed to organization Import Sample Application code & Create Azure DevOps project In this step, we will clone an existing sample .Net Core Application code available in a public Github repository to Azure DevOps Git repository. We will be deploying this application code to Amazon ECS Fargate environment. On your Azure DevOps home page, go ahead and create a project. Use default settings. You can create a new Azure DevOps project and/or repository to start from scratch. To create new project, go to home page in Azure DevOps and select + Create Project . To create new repository in existing project, you can select the Import repository from the Repos screen by providing the sample application code's Github repository link https://github.com/awsimaya/ECSFargate.git . Now Azure DevOps will clone the project from GitHub into its own git repo. if you already have a project created, then navigate to Repos page on the left navigation section. The screen should look similar to this below Click on Import under or Import a repository Enter the Github URL https://github.com/awsimaya/ECSFargate.git . Click Import . Now Azure DevOps will clone the project from GitHub into its own git repo. Setup AWS Service Connection for Azure DevOps project In this step, we will create a service connection to AWS on Azure DevOps. This will allow us to easily execute AWS specific tasks by simply selecting the service connection within the task itself. If you already have this configured in your Azure DevOps account then you can skip this step and go to next step. Under Azure DevOps Project Settings > Service connections, click on New Service connections and select AWS from that list You will see a window similar to the screenshot below. Give a connection name, enter the Access Key Id and Secret Access Key of the IAM User Account to be used by Azure DevOps. Click OK You will see a screen like the one below once this process is complete Create Azure DevOps Build pipeline In this section, we will configure build and release pipelines on Azure DevOps environment using tasks from AWS Tools for VSTS. We will build a docker container image that contains our application and also push it to AWS Elastic Container Repository. Under Pipelines , click Builds . On this page, click New Pipeline Your page should look like the one below. Click Use the visual designer link under Where is your code? section In the next step, select the repo you want to connect to. Click Continue . On the Select a template screen, select Empty Job as shown in the screenshot below and click Apply Now you should be on the Tasks tab inside the Build pipeline. Here, to the right hand side for the drop down Agent Pool under Agent job , select Hosted Ubuntu 1604 . More information on Microsoft-hosted agents in Azure DevOps can be found here . Leave other fields to their default values. Add a task by click on the + sign next to Agent job 1 on the left hand side. Type Command on the search bar on the right hand side. You should see the Command Line task as shown below and click Add . Once added, name the task Build Docker Image and enter the command docker build -t hellofargate ./HelloFargate in the Script field as shown below. This command will tag the resulting docker image with 'hellofargate' (case sensitive) and provides HelloFargate folder in the repository as the build context (path to the Dockerfile). Syntax of the command is docker build [OPTIONS] PATH | URL | - . More details on docker build command can be found here . Once again click on the + symbol next to Agent job 1 . Type aws on the search field which will list all AWS tasks. Select AWS Elastic Container Registry Push task and click Add Name the image Push Image to ECR . Select the name of the AWS Credentials you setup earlier under AWS Credentials drop down. Select EU (Ireland) [eu-west-1] as Region. Select Image name with optional tag for Image identity field. Enter hellofargate for the Source Image Name field. Enter latest for Source Image tag field. Enter hellofargaterepo for the Target Repository Name field (or if you named your ECR repository differently then use that name). Enter latest for the Target Repository Tag field. Now click on Save pipeline to save your changes. The screenshot below shows all the settings for easier understanding. Its important to select the checkbox at the bottom Create repository if it does not exist You can rename the build pipeline by just clicking on HelloFargate-CI at the top and typing a name as shown below Click on Save and Queue to save the build pipeline configurations and also queue a build. It might take around 2 minutes or so for the image to be published to the repository. Go ahead and start the next step ( Create Amazon ECS Task Definition ) till this gets completed. Go to the AWS ECS console, browse to the Amazon ECR section and click on hellofargaterepo repository and make sure there is a new entry and the Pushed at column reflects the latest timestamp Build target Amazon ECS environment to deploy application In this section, we will first create the target AWS environment where the application code will be deployed, and then configure steps in Azure DevOps to configure automated deployment to this target AWS infrastructure. AWS Region Selection Login to AWS Console using an IAM identity which has admin privileges. Set the AWS region we are going to work on EU-WEST-1 (Ireland) . See picture below and ensure your setup is set right. Create Amazon ECS Task Definition In this step, we are going to create a Amazon Elastic Container Service Task Definition that will be used to create the container instances in the cluster. A task definition is the core resource within ECS. This is where you define which container images to run, CPU/Memory, ports, commands etc. Navigate to Elastic Container Service home page Go to Repositories under Amazon ECR and click on the hellofargaterepo repository Select the value of Repository URI and press Ctrl+C or Cmd+C if you're using a Mac. You will need this in the next step Select Task Definitions and click on Create new Task Definition On the next screen, select FARGATE launch type and click Next On the next screen, give the Task Definition a name. For this exercise we will call it MyNextTaskDefinition . Leave Task Role and Network Mode fields to their default values. Select 0.5 GB for Task memory and 0.25 vCPU for Task CPU dropdowns respectively Your screen should look similar to the one below Click Add Container Name the container as hellofargatecontainer Copy and paste the Repository URL from ECR and paste into Image textbox. Make sure you add the tag :latest to the end of the string. Ensure there are no white spaces at the end. Ignore the Private repository authentication and Memory Limits (MiB) fields Add 80 to Port mappings Ignore the Advanced container configuration section Click on Add button on bottom right corner Your screen should look similar to the one below Now click Create Your Task definition is created successfully Your screen should look similar to the one below Create Amazon ECS Cluster In this section we are going to create a Amazon Elastic Container Service cluster. A cluster typically has one or more nodes , which are the workermachines that run your containerized applications and other workloads. This will be deployed in a VPC. Click on Clusters on the ECS home page Click on Create Cluster Select Networking Only option and click Next step Name the cluster as hellofargatecluster Check the Create VPC checkbox Type a CIDR range to allocate to your VPC and subnet. You may use online tools to validate subnet CIDR allocation (example https://cidr.xyz/ , https://www.site24x7.com/tools/ipv4-subnetcalculator.html ) Delete Subnet 2 by clicking the x to the right and leave other values to default Click Create This step will take about a minute to complete. In the background, AWS is creating a CloudFormation stack with all the input settings. Amazon ECS cluster is created sucessfully Important: Once complete, make sure you take a note of the VPC name that we just created, since we will use it in the future steps Click on View Cluster to see the cluster Create Amazon ECS Service In this step, we will create the Amazon ECS service with Fargate Launch type & manually deploy the latest application docker image (available in Amazon ECR) to this service. On the newly created cluster page, under Services tab, click on Create . Select FARGATE for Launch type option Make sure the Task Definition and the Clusters are selected to the ones you just created Enter hellofargateservice for Service name field Enter 1 for Number of tasks field and leave everything else as is. Click Next step Make sure the VPC drop down has the name of the VPC you just created Click on the Subnets drop down and select the subnet that comes up Click on Edit for Security groups and ensure the Inboud rules for security group has Port 80 selected. Look at the image below for clarity. Click cancel to exit the Configure security groups page Select None for Load balancing Under Service Discovery (optional) section uncheck the Enable service discovery integration checkbox as this is not required for this lab and click Next Step Leave the Set Auto Scaling to Do not adjust the service's desired count as it is Do a quick review of the Review screen and click Create service The cluster service is now created successfully Click on View Service Go to the Tasks tab and check the Last status column. It will refresh its status to RUNNING and turn green once the provisioning is complete The current screen should look similar to the one below. Notice that the Last status column says PROVISIONING which means the taks is currently being executed Once the value on Last status column says RUNNING and is green, click on the task name under Task column Copy and paste the value of Public IP under Network on to a new browser tab and press Enter You should see the home page of the your new application running on Amazon ECS Fargate Create a new Azure DevOps Release pipeline In this step, we will setup automated deployment process to update the Amazon ECS Service with the docker image built by the build pipeline. Go to Releases page on your Azure DevOps project Click New pipeline Select Empty job under Select a template Enter Prod for Stage name field Change the name of the pipeline to Release to Prod . Now your screen should look similar to the below screenshot Hover your mouse over 1 job, 0 task link and click on the + sign that becomes visible. This will allow you to add new tasks to the pipeline. In the next screen, under Agent job , select Hosted 2017 for Agent pool drop down Now click on the + sign next to Agent job and type Command in the search field. Select the Command Line task and click Add Name the task Install AWS CLI and enter pip install awscli in the Script field Click on the + sign next to Agent job . Type aws in the search field. Select AWS CLI task and click Add Name the new task Update ECS Service . Select the name of the AWS credential you configured earlier. Select EU (Ireland) [eu-west-1] as region Enter ecs in the Command field. Enter update-service in the Subcommand field. Enter --cluster hellofargatecluster --service hellofargatecontainer-service --force-new-deployment in the Options and parameters field Save the changes. Deploy application to Amazon ECS Fargate & validate In this step, we will create a release using the release pipeline we created in the previous step. Doing this will deploy the container into the cluster using the task definition it is configured to. Go to Releases and select the release pipeline you created earlier. You should see a screen similar to the one below. Click Create a release Select Prod in the drop down for Stages for a trigger change from automated to manual and click Create In the next screen, click on Release-1 that appears on the green bar Your next screen will look similar to the one below. Hover the mouse arrow on Prod stage rectangle. Click on Deploy button. Simply click Deploy in the pop-up screen The job will be queued and will be picked up once an agent gets free. After its complete, your screen should look like the one below Final Steps Now you have a fully functional Build and Deploy pipeline setup for your application running on Amazon Elastic Cluster Service Go ahead and make some simple change to the project and do a git push to the repo. Queue a Build and a Release to see your changes reflecting successfully on your target environment","title":"Amazon ECS Fargate"},{"location":"reinvent2018/win313/fargate/#deploy-a-net-core-application-on-amazon-ecs-fargate-using-azure-devops-vsts","text":"In this lab we will create a CICD pipeline on Azure DevOps using AWS Tools for VSTS.","title":"Deploy a .Net Core application on Amazon ECS Fargate using Azure DevOps (VSTS)"},{"location":"reinvent2018/win313/fargate/#prerequisites","text":"Complete the Lab Setup . Validate access to the sample source code repository in Github by browsing to this url https://github.com/awsimaya/ECSFargate.git . It is necessary to follow the instructions carefully to ensure successful completion of the lab.","title":"Prerequisites"},{"location":"reinvent2018/win313/fargate/#environment-setup","text":"In this section we will configure the environment baseline to get started.","title":"Environment Setup"},{"location":"reinvent2018/win313/fargate/#create-a-new-iam-user-for-azure-devops","text":"In this section, we will create a new IAM user account that will be used by your Azure DevOps service to deploy resources in your AWS Account. If you already have Access Key ID and Secret Access Key of an IAM User account to be used with Azure DevOps then you can skip this section. Click on IAM Service In the navigation pane, choose Users and then choose Add user . Type the user name for the new user. This is the sign-in name for AWS. Select only programmatic access. This creates an access key for each new user. You can view or download the access keys when you get to the Final page. Click on Next: Permissions . Select Attach existing policies directly. Select Administrator Access. Note: This is not a best practice to provide Administrator Access. Its recommended to follow least access privilege and limit permissions to the resources that would be required from AzureDevOps account. This could vary depending upon your use case and scenarios. Example you may chose to create different IAM User account for Non-Prod Vs Prod and appropriately provide the necessary permissions following least access privilege principle. Click on Next: Review Review the settings and Click on Create User . Save the Secret Access Key and Access Key ID to be used later in this lab.","title":"Create a new IAM User for Azure DevOps"},{"location":"reinvent2018/win313/fargate/#install-aws-tools-for-vsts-aka-azure-devops","text":"In this step, we will install AWS Tools for Azure DevOps aka Visual Studio Team Services (VSTS) to your account. This will allow us to leverage the powerful features that this add-on provides to create CICD tasks on AWS Login to your Azure DevOps (formerly VSTS) account Navigate to Visual Studio Marketplace and click on Get it free button as shown in the image below If you're logged in to Azure DevOps already, you will be taken to your Azure DevOps account where you can select the Organization as shown below. Once you click, AWS Tools for VSTS will be installed on the organization successfully. You will screen like the below if the the process was successful. Click Proceed to organization","title":"Install AWS Tools for VSTS aka Azure DevOps"},{"location":"reinvent2018/win313/fargate/#import-sample-application-code-create-azure-devops-project","text":"In this step, we will clone an existing sample .Net Core Application code available in a public Github repository to Azure DevOps Git repository. We will be deploying this application code to Amazon ECS Fargate environment. On your Azure DevOps home page, go ahead and create a project. Use default settings. You can create a new Azure DevOps project and/or repository to start from scratch. To create new project, go to home page in Azure DevOps and select + Create Project . To create new repository in existing project, you can select the Import repository from the Repos screen by providing the sample application code's Github repository link https://github.com/awsimaya/ECSFargate.git . Now Azure DevOps will clone the project from GitHub into its own git repo. if you already have a project created, then navigate to Repos page on the left navigation section. The screen should look similar to this below Click on Import under or Import a repository Enter the Github URL https://github.com/awsimaya/ECSFargate.git . Click Import . Now Azure DevOps will clone the project from GitHub into its own git repo.","title":"Import Sample Application code &amp; Create Azure DevOps project"},{"location":"reinvent2018/win313/fargate/#setup-aws-service-connection-for-azure-devops-project","text":"In this step, we will create a service connection to AWS on Azure DevOps. This will allow us to easily execute AWS specific tasks by simply selecting the service connection within the task itself. If you already have this configured in your Azure DevOps account then you can skip this step and go to next step. Under Azure DevOps Project Settings > Service connections, click on New Service connections and select AWS from that list You will see a window similar to the screenshot below. Give a connection name, enter the Access Key Id and Secret Access Key of the IAM User Account to be used by Azure DevOps. Click OK You will see a screen like the one below once this process is complete","title":"Setup AWS Service Connection for Azure DevOps project"},{"location":"reinvent2018/win313/fargate/#create-azure-devops-build-pipeline","text":"In this section, we will configure build and release pipelines on Azure DevOps environment using tasks from AWS Tools for VSTS. We will build a docker container image that contains our application and also push it to AWS Elastic Container Repository. Under Pipelines , click Builds . On this page, click New Pipeline Your page should look like the one below. Click Use the visual designer link under Where is your code? section In the next step, select the repo you want to connect to. Click Continue . On the Select a template screen, select Empty Job as shown in the screenshot below and click Apply Now you should be on the Tasks tab inside the Build pipeline. Here, to the right hand side for the drop down Agent Pool under Agent job , select Hosted Ubuntu 1604 . More information on Microsoft-hosted agents in Azure DevOps can be found here . Leave other fields to their default values. Add a task by click on the + sign next to Agent job 1 on the left hand side. Type Command on the search bar on the right hand side. You should see the Command Line task as shown below and click Add . Once added, name the task Build Docker Image and enter the command docker build -t hellofargate ./HelloFargate in the Script field as shown below. This command will tag the resulting docker image with 'hellofargate' (case sensitive) and provides HelloFargate folder in the repository as the build context (path to the Dockerfile). Syntax of the command is docker build [OPTIONS] PATH | URL | - . More details on docker build command can be found here . Once again click on the + symbol next to Agent job 1 . Type aws on the search field which will list all AWS tasks. Select AWS Elastic Container Registry Push task and click Add Name the image Push Image to ECR . Select the name of the AWS Credentials you setup earlier under AWS Credentials drop down. Select EU (Ireland) [eu-west-1] as Region. Select Image name with optional tag for Image identity field. Enter hellofargate for the Source Image Name field. Enter latest for Source Image tag field. Enter hellofargaterepo for the Target Repository Name field (or if you named your ECR repository differently then use that name). Enter latest for the Target Repository Tag field. Now click on Save pipeline to save your changes. The screenshot below shows all the settings for easier understanding. Its important to select the checkbox at the bottom Create repository if it does not exist You can rename the build pipeline by just clicking on HelloFargate-CI at the top and typing a name as shown below Click on Save and Queue to save the build pipeline configurations and also queue a build. It might take around 2 minutes or so for the image to be published to the repository. Go ahead and start the next step ( Create Amazon ECS Task Definition ) till this gets completed. Go to the AWS ECS console, browse to the Amazon ECR section and click on hellofargaterepo repository and make sure there is a new entry and the Pushed at column reflects the latest timestamp","title":"Create Azure DevOps Build pipeline"},{"location":"reinvent2018/win313/fargate/#build-target-amazon-ecs-environment-to-deploy-application","text":"In this section, we will first create the target AWS environment where the application code will be deployed, and then configure steps in Azure DevOps to configure automated deployment to this target AWS infrastructure.","title":"Build target Amazon ECS environment to deploy application"},{"location":"reinvent2018/win313/fargate/#aws-region-selection","text":"Login to AWS Console using an IAM identity which has admin privileges. Set the AWS region we are going to work on EU-WEST-1 (Ireland) . See picture below and ensure your setup is set right.","title":"AWS Region Selection"},{"location":"reinvent2018/win313/fargate/#create-amazon-ecs-task-definition","text":"In this step, we are going to create a Amazon Elastic Container Service Task Definition that will be used to create the container instances in the cluster. A task definition is the core resource within ECS. This is where you define which container images to run, CPU/Memory, ports, commands etc. Navigate to Elastic Container Service home page Go to Repositories under Amazon ECR and click on the hellofargaterepo repository Select the value of Repository URI and press Ctrl+C or Cmd+C if you're using a Mac. You will need this in the next step Select Task Definitions and click on Create new Task Definition On the next screen, select FARGATE launch type and click Next On the next screen, give the Task Definition a name. For this exercise we will call it MyNextTaskDefinition . Leave Task Role and Network Mode fields to their default values. Select 0.5 GB for Task memory and 0.25 vCPU for Task CPU dropdowns respectively Your screen should look similar to the one below Click Add Container Name the container as hellofargatecontainer Copy and paste the Repository URL from ECR and paste into Image textbox. Make sure you add the tag :latest to the end of the string. Ensure there are no white spaces at the end. Ignore the Private repository authentication and Memory Limits (MiB) fields Add 80 to Port mappings Ignore the Advanced container configuration section Click on Add button on bottom right corner Your screen should look similar to the one below Now click Create Your Task definition is created successfully Your screen should look similar to the one below","title":"Create Amazon ECS Task Definition"},{"location":"reinvent2018/win313/fargate/#create-amazon-ecs-cluster","text":"In this section we are going to create a Amazon Elastic Container Service cluster. A cluster typically has one or more nodes , which are the workermachines that run your containerized applications and other workloads. This will be deployed in a VPC. Click on Clusters on the ECS home page Click on Create Cluster Select Networking Only option and click Next step Name the cluster as hellofargatecluster Check the Create VPC checkbox Type a CIDR range to allocate to your VPC and subnet. You may use online tools to validate subnet CIDR allocation (example https://cidr.xyz/ , https://www.site24x7.com/tools/ipv4-subnetcalculator.html ) Delete Subnet 2 by clicking the x to the right and leave other values to default Click Create This step will take about a minute to complete. In the background, AWS is creating a CloudFormation stack with all the input settings. Amazon ECS cluster is created sucessfully Important: Once complete, make sure you take a note of the VPC name that we just created, since we will use it in the future steps Click on View Cluster to see the cluster","title":"Create Amazon ECS Cluster"},{"location":"reinvent2018/win313/fargate/#create-amazon-ecs-service","text":"In this step, we will create the Amazon ECS service with Fargate Launch type & manually deploy the latest application docker image (available in Amazon ECR) to this service. On the newly created cluster page, under Services tab, click on Create . Select FARGATE for Launch type option Make sure the Task Definition and the Clusters are selected to the ones you just created Enter hellofargateservice for Service name field Enter 1 for Number of tasks field and leave everything else as is. Click Next step Make sure the VPC drop down has the name of the VPC you just created Click on the Subnets drop down and select the subnet that comes up Click on Edit for Security groups and ensure the Inboud rules for security group has Port 80 selected. Look at the image below for clarity. Click cancel to exit the Configure security groups page Select None for Load balancing Under Service Discovery (optional) section uncheck the Enable service discovery integration checkbox as this is not required for this lab and click Next Step Leave the Set Auto Scaling to Do not adjust the service's desired count as it is Do a quick review of the Review screen and click Create service The cluster service is now created successfully Click on View Service Go to the Tasks tab and check the Last status column. It will refresh its status to RUNNING and turn green once the provisioning is complete The current screen should look similar to the one below. Notice that the Last status column says PROVISIONING which means the taks is currently being executed Once the value on Last status column says RUNNING and is green, click on the task name under Task column Copy and paste the value of Public IP under Network on to a new browser tab and press Enter You should see the home page of the your new application running on Amazon ECS Fargate","title":"Create Amazon ECS Service"},{"location":"reinvent2018/win313/fargate/#create-a-new-azure-devops-release-pipeline","text":"In this step, we will setup automated deployment process to update the Amazon ECS Service with the docker image built by the build pipeline. Go to Releases page on your Azure DevOps project Click New pipeline Select Empty job under Select a template Enter Prod for Stage name field Change the name of the pipeline to Release to Prod . Now your screen should look similar to the below screenshot Hover your mouse over 1 job, 0 task link and click on the + sign that becomes visible. This will allow you to add new tasks to the pipeline. In the next screen, under Agent job , select Hosted 2017 for Agent pool drop down Now click on the + sign next to Agent job and type Command in the search field. Select the Command Line task and click Add Name the task Install AWS CLI and enter pip install awscli in the Script field Click on the + sign next to Agent job . Type aws in the search field. Select AWS CLI task and click Add Name the new task Update ECS Service . Select the name of the AWS credential you configured earlier. Select EU (Ireland) [eu-west-1] as region Enter ecs in the Command field. Enter update-service in the Subcommand field. Enter --cluster hellofargatecluster --service hellofargatecontainer-service --force-new-deployment in the Options and parameters field Save the changes.","title":"Create a new Azure DevOps Release pipeline"},{"location":"reinvent2018/win313/fargate/#deploy-application-to-amazon-ecs-fargate-validate","text":"In this step, we will create a release using the release pipeline we created in the previous step. Doing this will deploy the container into the cluster using the task definition it is configured to. Go to Releases and select the release pipeline you created earlier. You should see a screen similar to the one below. Click Create a release Select Prod in the drop down for Stages for a trigger change from automated to manual and click Create In the next screen, click on Release-1 that appears on the green bar Your next screen will look similar to the one below. Hover the mouse arrow on Prod stage rectangle. Click on Deploy button. Simply click Deploy in the pop-up screen The job will be queued and will be picked up once an agent gets free. After its complete, your screen should look like the one below","title":"Deploy application to Amazon ECS Fargate &amp; validate"},{"location":"reinvent2018/win313/fargate/#final-steps","text":"Now you have a fully functional Build and Deploy pipeline setup for your application running on Amazon Elastic Cluster Service Go ahead and make some simple change to the project and do a git push to the repo. Queue a Build and a Release to see your changes reflecting successfully on your target environment","title":"Final Steps"},{"location":"reinvent2018/win313/lambda/","text":"Deploy AWS Lambda with Azure DevOps (VSTS) Introduction In this lab, you will learn how to use Azure DevOps and AWS VSTS tool to deploy AWS Lambda project. Prerequisites Complete the Lab Setup . Create an IAM User using the AWS Management Console, as described here. Create an Access Key for the IAM User by following these instructions . You will need the AWS CLI installed and configured. If you don't already have it you can follow these instructions . You will need Git installed locally. If not, you can install as described here . You will need the .NET Core CLI installed. If not, install as described in these instructions . Create an IAM Role for your Lambda Function In this section we will create an IAM Role in your AWS Account for the Lambda Function. This is the permission that the lambda function can do when it assumes this role. Learn more about Lambda role here. Log in to your AWS Account and go to IAM Console. Go to Role and click Create Role . Select AWS Service for Type of trusted entity then slect Lambda as the service that will use this role. Click Next: Permission Filter and Select AWSLambdaBasicExecutionRole and click Next: Tags Click Next: Review and name the Role. Review the configuration and click Create Role Create Azure DevOps project In this section we will create an Azure DevOps project and clone the repo to your local working environment. Log in to your Azure DevOps account and create a project. It takes some time to complete. Create a Personal Access Token. Click on your avatar in the top-right of the screen and then click Security . Under the Personal Access Tokens section, click New Token . Enter a value in the Name field, select Full Access under Scope, and click Create. Copy the token to a temporary storage location and click Close . Create AWS service connection for this project. On your Summary page on the project, click Project settings on the bottom left menu bar Click Service connection under Pipelines then click + New service connection. Enter Access Key ID and Secret Access Key of your IAM user. For more information see this instruction to Add AWS service connection for this project. Click in the project and select Repos. Copy Git repo address. Select Add ReadMe file and add gitignore for VisualStudio. Click Initialize . On you command line type the command below to clone your newly created code repository to your local machine. Enter your Azure DevOps username and use your Personal Access Token as the password. use git repo you copied in step 4 in the git clone command. git clone https : //{ git repo account } @dev . azure . com /{ git repo account }/ ReInventLambda / _git / ReInventLambda Create a simple Lambda project In this session, we will create a simple dotnet Lambda project using donet CLI and push it to Azure DevOps git repository. Install AWS Lambda template. dotnet new -i Amazon.Lambda.Templates Once the install is complete, verify if the Lambda templates are listed. dotnet new -all Create a new Lambda project. Name function name ReInventLambda , choose your AWS profile and AWS region. dotnet new lambda.EmptyFunction --name ReInventLambda --profile default --region us-west-2 Browse into the folder. Examine the folder structure. cd ReInventLambda Use your favorite text editor to open ..\\ReInventLambda\\src\\ReInventLambda\\aws-lambda-tools-defaults.json and resave it as UTF-8 (with no BOM) encoding. In this example, we use Visual Studio Code. Commit the new code to local and remote (Azure DevOps) repository. git add * git commit -m \"Lambda empty function first commit\" git push In Azure DevOps, examine your repo. Create Build pipeline In this section, we will configure Build Pipeline using AWS Lambda.NET Core Deployment task. We will output the artifact to Azure DevOps. Select Pipelines, Builds , hit + button and select New build pipeline . Click Use the visual designer . Select Source, Team project, Repository and branch . Click Continue . Select a source: Azure Repos Git Team project: ReInventLambda Repository: ReInventLambda Default branch for manual and scheduled builds: master Select start with an Empty job by clicking Empty job . Under Tasks, Pipeline, name the Build pipeline and select Hosted VS2017 as Agent pool. Under Agent job 1, name Agent job and select inherit from pipeline for Agent pool. Click + button at Agent job 1 task, to Add a task. In the search box, enter PowerShell . Add PowerShell Task . Display name: Install Amazon.Lambda.Tools Type: Inline Scripts: Enter the script bellowed. Write - Host ' Installing . NET Global Tool Amazon . Lambda . Tools ' Write - Host \"dotnet tool install Amazon.Lambda.Tools --tool-path C:\\Program Files\\dotnet \\t ools\" # Using stream redirection to force hide all output from the dotnet cli call & dotnet tool install Amazon . Lambda . Tools -- tool - path \"C:\\Program Files\\dotnet \\t ools\" *>& 1 | Out - Null if ( $ LASTEXITCODE - ne 0 ) { Write - Verbose - Message ' Error installing , attempting to update Amazon . Lambda . Tools ' # When \"-Verbose\" switch was used this output was not hidden. # Using stream redirection to force hide all output from the dotnet cli call & dotnet tool update Amazon . Lambda . Tools -- tool - path \"C:\\Program Files\\dotnet \\t ools\" *>& 1 | Out - Null if ( $ LASTEXITCODE - ne 0 ) { $ msg = @\" Error configuring . NET CLI AWS Lambda deployment tools : $ LastExitCode CALLSTACK : $ ( Get - PSCallStack | Out - String ) \"@ throw $ msg } } Write - Host \"Confirm if the tool is intalled\" get - childitem \"C:\\Program Files\\dotnet \\t ools\" Write - Host \"Add path to AWS tool to env variable.\" $ env : Path += \";C:\\Program Files\\dotnet \\t ools\" Write - Host \"PATH variable is \" $ env : Path Write - Host \"##vso[task.setvariable variable=PATH;]${env:PATH};$env:Path\" ; This task is necessary if you are using the new .NET Core Global tool (.NET Core SDK 2.1.300 and later versions). With this change, the csproj file no longer include DotNetCliToolReference (see below). That means dotnet restore will not automatically download the tool during the build time. This task downloads and saves the tool in the specified location. The tool will be referenced in the next step. <ItemGroup> <DotNetCliToolReference Include= \"Amazon.Lambda.Tools\" Version= \"2.2.0\" /> </ItemGroup> Click + button at Agent job 1 task to add a task. In the search box, enter aws . This should filter only AWS related tasks. Select AWS Lambda.NET Core Deployment then click Add . Configure Deploy.NET Core to Lambda task. Display name: Deploy .NET Core to Lambda AWS Credentials: AWS Note: AWS Credential was created in prerequisites section. Region: Select AWS Region where you wanted your Lambda function to reside. Deployment Type: Function Check Create deployment package only Package-only output file: $(Build.ArtifactStagingDirectory)\\ReInventLambda.zip Path to Lambda Project: browse to the directory containing the Lambda Project file Do not need to fill Lambda Function Properties, Advanced, Control Options and Output Variables. Click + to add a task. Select Publish build Artifacts and click Add . Configure task as seen below. Display name: Publish Artifact: PackagedLambdaFunction Path to publish: $(Build.ArtifactStagingDirectory)\\ReInventLambda.zip Note: this is Package-only output file from the previous step Artifacts name: PackagedLambdaFunction Artifact publish location: Azure Pipelines/TFS Select Save & queue . Examine Build logs. If you face this error, use your favorite text editor to re-save aws-lambda-tools-defaults.json with UTF-8 with no BOM file encoding. Create Release pipeline In this section, we will use AWS Lambda Deploy Function task to deploy the output build artifact from the last section to AWS Lambda function. Select Pipelines, Releases and New Release pipeline . In the New release pipeline, select start with an Empty job . To add the artifact to the release pipeline, click Add an Artifact and configure as followed: Source type: Build Project: ReinventLambda Source (Build Pipeline): ReinventLambda-CI Default version: Latest Source alias: _ReinventLambda-CI Click Add . Click Stage and name the stage. Click 1 job, 0 task to view task. At Agent job, click + to add a task. In the search box, type AWS. Select AWS Lambda Deploy Function and click Add. Configure Deploy Lambda Function task. Display name: Deploy Lambda Function: ReInventLambda AWS Credentails: AWS Note: AWS Credential was created in prerequisites section. Region: Select the same AWS Region as in the previous step. Deployment Mode: Update code and configuration (or create a new funtion). Function name: ReInventLambda Function Handler: ReInventLambda::ReInventLambda.Function::FunctionHandler Runtime: dotnetcore2.1 Code Location: Zip file in the work area Zip File Path: $(System.DefaultWorkingDirectory)/_ReInventLambda-CI/PackagedLambdaFunction/ReInventLambda.zip Role ARN or Name: Lambda-ServiceRole-BasicExecution Note: This is the Lambda service role created in Prerquisite section. Memory Size: 256 Timeout: 30 Check Publish Leave everything else as default in other sections. Click Save (at the top) to save the pipeline. At the top, click Release and select Create a release . Review the release and click Create . Select the release to view its status. Log in to AWS Lambda console to view the Function. Testing the function using .net CLI In this section, we will use dotnet CLI to test our newly created Lambda Funtion Install dotnet Lambda Tools and test the function. dotnet tool install -g Amazon.Lambda.Tools dotnet lambda invoke-function ReInventLambda --payload \"Just checking\"","title":"AWS Lambda"},{"location":"reinvent2018/win313/lambda/#deploy-aws-lambda-with-azure-devops-vsts","text":"","title":"Deploy AWS Lambda with Azure DevOps (VSTS)"},{"location":"reinvent2018/win313/lambda/#introduction","text":"In this lab, you will learn how to use Azure DevOps and AWS VSTS tool to deploy AWS Lambda project.","title":"Introduction"},{"location":"reinvent2018/win313/lambda/#prerequisites","text":"Complete the Lab Setup . Create an IAM User using the AWS Management Console, as described here. Create an Access Key for the IAM User by following these instructions . You will need the AWS CLI installed and configured. If you don't already have it you can follow these instructions . You will need Git installed locally. If not, you can install as described here . You will need the .NET Core CLI installed. If not, install as described in these instructions .","title":"Prerequisites"},{"location":"reinvent2018/win313/lambda/#create-an-iam-role-for-your-lambda-function","text":"In this section we will create an IAM Role in your AWS Account for the Lambda Function. This is the permission that the lambda function can do when it assumes this role. Learn more about Lambda role here. Log in to your AWS Account and go to IAM Console. Go to Role and click Create Role . Select AWS Service for Type of trusted entity then slect Lambda as the service that will use this role. Click Next: Permission Filter and Select AWSLambdaBasicExecutionRole and click Next: Tags Click Next: Review and name the Role. Review the configuration and click Create Role","title":"Create an IAM Role for your Lambda Function"},{"location":"reinvent2018/win313/lambda/#create-azure-devops-project","text":"In this section we will create an Azure DevOps project and clone the repo to your local working environment. Log in to your Azure DevOps account and create a project. It takes some time to complete. Create a Personal Access Token. Click on your avatar in the top-right of the screen and then click Security . Under the Personal Access Tokens section, click New Token . Enter a value in the Name field, select Full Access under Scope, and click Create. Copy the token to a temporary storage location and click Close . Create AWS service connection for this project. On your Summary page on the project, click Project settings on the bottom left menu bar Click Service connection under Pipelines then click + New service connection. Enter Access Key ID and Secret Access Key of your IAM user. For more information see this instruction to Add AWS service connection for this project. Click in the project and select Repos. Copy Git repo address. Select Add ReadMe file and add gitignore for VisualStudio. Click Initialize . On you command line type the command below to clone your newly created code repository to your local machine. Enter your Azure DevOps username and use your Personal Access Token as the password. use git repo you copied in step 4 in the git clone command. git clone https : //{ git repo account } @dev . azure . com /{ git repo account }/ ReInventLambda / _git / ReInventLambda","title":"Create Azure DevOps project"},{"location":"reinvent2018/win313/lambda/#create-a-simple-lambda-project","text":"In this session, we will create a simple dotnet Lambda project using donet CLI and push it to Azure DevOps git repository. Install AWS Lambda template. dotnet new -i Amazon.Lambda.Templates Once the install is complete, verify if the Lambda templates are listed. dotnet new -all Create a new Lambda project. Name function name ReInventLambda , choose your AWS profile and AWS region. dotnet new lambda.EmptyFunction --name ReInventLambda --profile default --region us-west-2 Browse into the folder. Examine the folder structure. cd ReInventLambda Use your favorite text editor to open ..\\ReInventLambda\\src\\ReInventLambda\\aws-lambda-tools-defaults.json and resave it as UTF-8 (with no BOM) encoding. In this example, we use Visual Studio Code. Commit the new code to local and remote (Azure DevOps) repository. git add * git commit -m \"Lambda empty function first commit\" git push In Azure DevOps, examine your repo.","title":"Create a simple Lambda project"},{"location":"reinvent2018/win313/lambda/#create-build-pipeline","text":"In this section, we will configure Build Pipeline using AWS Lambda.NET Core Deployment task. We will output the artifact to Azure DevOps. Select Pipelines, Builds , hit + button and select New build pipeline . Click Use the visual designer . Select Source, Team project, Repository and branch . Click Continue . Select a source: Azure Repos Git Team project: ReInventLambda Repository: ReInventLambda Default branch for manual and scheduled builds: master Select start with an Empty job by clicking Empty job . Under Tasks, Pipeline, name the Build pipeline and select Hosted VS2017 as Agent pool. Under Agent job 1, name Agent job and select inherit from pipeline for Agent pool. Click + button at Agent job 1 task, to Add a task. In the search box, enter PowerShell . Add PowerShell Task . Display name: Install Amazon.Lambda.Tools Type: Inline Scripts: Enter the script bellowed. Write - Host ' Installing . NET Global Tool Amazon . Lambda . Tools ' Write - Host \"dotnet tool install Amazon.Lambda.Tools --tool-path C:\\Program Files\\dotnet \\t ools\" # Using stream redirection to force hide all output from the dotnet cli call & dotnet tool install Amazon . Lambda . Tools -- tool - path \"C:\\Program Files\\dotnet \\t ools\" *>& 1 | Out - Null if ( $ LASTEXITCODE - ne 0 ) { Write - Verbose - Message ' Error installing , attempting to update Amazon . Lambda . Tools ' # When \"-Verbose\" switch was used this output was not hidden. # Using stream redirection to force hide all output from the dotnet cli call & dotnet tool update Amazon . Lambda . Tools -- tool - path \"C:\\Program Files\\dotnet \\t ools\" *>& 1 | Out - Null if ( $ LASTEXITCODE - ne 0 ) { $ msg = @\" Error configuring . NET CLI AWS Lambda deployment tools : $ LastExitCode CALLSTACK : $ ( Get - PSCallStack | Out - String ) \"@ throw $ msg } } Write - Host \"Confirm if the tool is intalled\" get - childitem \"C:\\Program Files\\dotnet \\t ools\" Write - Host \"Add path to AWS tool to env variable.\" $ env : Path += \";C:\\Program Files\\dotnet \\t ools\" Write - Host \"PATH variable is \" $ env : Path Write - Host \"##vso[task.setvariable variable=PATH;]${env:PATH};$env:Path\" ; This task is necessary if you are using the new .NET Core Global tool (.NET Core SDK 2.1.300 and later versions). With this change, the csproj file no longer include DotNetCliToolReference (see below). That means dotnet restore will not automatically download the tool during the build time. This task downloads and saves the tool in the specified location. The tool will be referenced in the next step. <ItemGroup> <DotNetCliToolReference Include= \"Amazon.Lambda.Tools\" Version= \"2.2.0\" /> </ItemGroup> Click + button at Agent job 1 task to add a task. In the search box, enter aws . This should filter only AWS related tasks. Select AWS Lambda.NET Core Deployment then click Add . Configure Deploy.NET Core to Lambda task. Display name: Deploy .NET Core to Lambda AWS Credentials: AWS Note: AWS Credential was created in prerequisites section. Region: Select AWS Region where you wanted your Lambda function to reside. Deployment Type: Function Check Create deployment package only Package-only output file: $(Build.ArtifactStagingDirectory)\\ReInventLambda.zip Path to Lambda Project: browse to the directory containing the Lambda Project file Do not need to fill Lambda Function Properties, Advanced, Control Options and Output Variables. Click + to add a task. Select Publish build Artifacts and click Add . Configure task as seen below. Display name: Publish Artifact: PackagedLambdaFunction Path to publish: $(Build.ArtifactStagingDirectory)\\ReInventLambda.zip Note: this is Package-only output file from the previous step Artifacts name: PackagedLambdaFunction Artifact publish location: Azure Pipelines/TFS Select Save & queue . Examine Build logs. If you face this error, use your favorite text editor to re-save aws-lambda-tools-defaults.json with UTF-8 with no BOM file encoding.","title":"Create Build pipeline"},{"location":"reinvent2018/win313/lambda/#create-release-pipeline","text":"In this section, we will use AWS Lambda Deploy Function task to deploy the output build artifact from the last section to AWS Lambda function. Select Pipelines, Releases and New Release pipeline . In the New release pipeline, select start with an Empty job . To add the artifact to the release pipeline, click Add an Artifact and configure as followed: Source type: Build Project: ReinventLambda Source (Build Pipeline): ReinventLambda-CI Default version: Latest Source alias: _ReinventLambda-CI Click Add . Click Stage and name the stage. Click 1 job, 0 task to view task. At Agent job, click + to add a task. In the search box, type AWS. Select AWS Lambda Deploy Function and click Add. Configure Deploy Lambda Function task. Display name: Deploy Lambda Function: ReInventLambda AWS Credentails: AWS Note: AWS Credential was created in prerequisites section. Region: Select the same AWS Region as in the previous step. Deployment Mode: Update code and configuration (or create a new funtion). Function name: ReInventLambda Function Handler: ReInventLambda::ReInventLambda.Function::FunctionHandler Runtime: dotnetcore2.1 Code Location: Zip file in the work area Zip File Path: $(System.DefaultWorkingDirectory)/_ReInventLambda-CI/PackagedLambdaFunction/ReInventLambda.zip Role ARN or Name: Lambda-ServiceRole-BasicExecution Note: This is the Lambda service role created in Prerquisite section. Memory Size: 256 Timeout: 30 Check Publish Leave everything else as default in other sections. Click Save (at the top) to save the pipeline. At the top, click Release and select Create a release . Review the release and click Create . Select the release to view its status. Log in to AWS Lambda console to view the Function.","title":"Create Release pipeline"},{"location":"reinvent2018/win313/lambda/#testing-the-function-using-net-cli","text":"In this section, we will use dotnet CLI to test our newly created Lambda Funtion Install dotnet Lambda Tools and test the function. dotnet tool install -g Amazon.Lambda.Tools dotnet lambda invoke-function ReInventLambda --payload \"Just checking\"","title":"Testing the function using .net CLI"},{"location":"reinvent2018/win313/resources/","text":"Resources AWS Blog for VSTS Landing page of AWS Tools for Visual Studio Team Service Using Ebextensions to deploy app dependencies in the environment ex Crystal Reports which was very popular reporting tools for legacy .Net applications AWS blog for customizing Windows Server environments using eb extensions","title":"Resources"},{"location":"reinvent2018/win313/resources/#resources","text":"AWS Blog for VSTS Landing page of AWS Tools for Visual Studio Team Service Using Ebextensions to deploy app dependencies in the environment ex Crystal Reports which was very popular reporting tools for legacy .Net applications AWS blog for customizing Windows Server environments using eb extensions","title":"Resources"},{"location":"reinvent2018/win313/setup/","text":"Lab Setup Before starting the labs, there are a couple of prerequisites and setup steps you need to carry out. If you need a hand, then please ask. Prerequisites An Amazon Web Services (AWS) account . We will provide one if you need. An Azure DevOps account. The AWS Tools Extension installed in your Azure DevOps account. Otherwise follow these instructions these instructions to install the extension.","title":"Lab Setup"},{"location":"reinvent2018/win313/setup/#lab-setup","text":"Before starting the labs, there are a couple of prerequisites and setup steps you need to carry out. If you need a hand, then please ask.","title":"Lab Setup"},{"location":"reinvent2018/win313/setup/#prerequisites","text":"An Amazon Web Services (AWS) account . We will provide one if you need. An Azure DevOps account. The AWS Tools Extension installed in your Azure DevOps account. Otherwise follow these instructions these instructions to install the extension.","title":"Prerequisites"},{"location":"reinvent2018/win313/win313index/","text":"Build a .NET CI/CD Pipeline with AWS & AWS Tools for Visual Studio Team Services We have 3 different types of modules setup. You can choose to work on any one of the three below. Deploy an app to EC2 using Azure DevOps (VSTS) Deploy AWS Lambda with Azure DevOps (VSTS) Deploy a .Net Core application on Amazon ECS Fargate using Azure DevOps (VSTS) What you will learn How to target AWS resources from your Azure DevOps build and release pipelines Deployment options for targeting different types of AWS compute services Tricks and tips for enhancing your CI/CD pipelines Getting Started Before starting the lab, run through the Lab Setup , and once you've completed it you can choose any one of the challenges to start with. The challenges are all independent so you can tackle as many of them as you like, and in any order you want.","title":"Index"},{"location":"reinvent2018/win313/win313index/#build-a-net-cicd-pipeline-with-aws-aws-tools-for-visual-studio-team-services","text":"We have 3 different types of modules setup. You can choose to work on any one of the three below. Deploy an app to EC2 using Azure DevOps (VSTS) Deploy AWS Lambda with Azure DevOps (VSTS) Deploy a .Net Core application on Amazon ECS Fargate using Azure DevOps (VSTS)","title":"Build a .NET CI/CD Pipeline with AWS &amp; AWS Tools for Visual Studio Team Services"},{"location":"reinvent2018/win313/win313index/#what-you-will-learn","text":"How to target AWS resources from your Azure DevOps build and release pipelines Deployment options for targeting different types of AWS compute services Tricks and tips for enhancing your CI/CD pipelines","title":"What you will learn"},{"location":"reinvent2018/win313/win313index/#getting-started","text":"Before starting the lab, run through the Lab Setup , and once you've completed it you can choose any one of the challenges to start with. The challenges are all independent so you can tackle as many of them as you like, and in any order you want.","title":"Getting Started"},{"location":"reinvent2018/win314/build/","text":"Modernize Your First Windows Application with Windows Containers Duration: 45 minutes Task 1. Log in to your AWS Account Using the console URL and username/password information from the welcome page let's log in to your AWS account. Task 2. Build the Amazon Elastic Container Service (ECS) Cluster In this task we will build the Windows Server ECS cluster. Step 1. In the AWS console change your AWS Region to \"EU (Frankfurt)\". a Step 2. Now let's navigate to the ECS admin interface and then we'll select \"Clusters\" from the left nav. a Step 3. Click on \"Create Cluster\" and then select \"EC2 Windows + Networking\" and then click \"Next step\" a Step 4. Enter a Cluster name and then review the configuration options available (we will leave the default values as-is with the exception of the key pair). Info Because this is a new AWS account you will need to create a key pair in the Frankfurt Region. Follow the \"EC2 Console link\" to create a new key pair. a Step 5. Once the key pair has been created return to the ECS Cluster tab and click the refresh icon next to the Key pair and your new key pair should appear. a Step 6. Review the remaining configuration options and click \"Create\" to build the ECS Windows cluster. While the ECS cluster is building we'll jump over to our dev server and get started with configuring Visual Studio. a Task 3. Configure the Dev Server In this task we will RDP in to our dev server and configure Visual Studio and the AWS Toolkit for Visual Studio. Step 7. Let's RDP into your dev server. From the EC2 admin interface click on \"Running instances\" and then select the \"WIN314 Dev Server\", click on \"Connect\", then click on \"Download Remote Desktop File\" and finally launch the RDP file. a Step 8. When you are prompted for credentials first click on \"More choices\", then click on \"Use a different account\" and then enter the following credentials: Info username: .\\developer password: ILove.Net! a Step 9. Once we have established an RDP connection to the dev server let's configure the AWS CLI with your AWS account information. Open a command prompt as an administrator (right-click the command prompt, select more and the select Run as administrator) and enter the command: \"aws configure\", when prompted enter you Access Key and Secret Access Key (included in the email sent by your instructor). a Step 10. Now let's open Visual Studio (VS) 2017 Community edition and configure the AWS Toolkit for Visual Studio with your AWS account information. Launch Visual Studio and wait a few seconds for the AWS Explorer UI to appear. If it doesn't appear from the VS menu select View -> AWS Explorer and enter your AWS account information if prompted. From the AWS Explorer let's change the AWS Region to Frankfurt. Tip Once you change the Region you can expand the ECS item and double-click your ECS cluster to see it's current status a Step 11. Now let's open our sample .NET application. From the Start Page tab (or form the Visual Studio menu File -> Open -> Project/Solution) open the \"MvsMusicStore-Wed-F2017\" Solution (located at C:\\Source) a Step 12. Once the solution has loaded let's run it locally to make sure the application is working. You may receive some warnings regarding SSL, self-signed certificates, the current debug mode, etc. Simply accept those warnings and then the application should render: a Now that have confirmed we have a running application let's start the process of containerizing our application. a Task 4. Containerize .NET App in Visual Studio In this task we will containerize the application in Visual Studio using Docker compose. Step 13. Let's start by adding Docker support to our project. Right-click the project and select Add -> Container Orchestrator Support. Leave the default value for \"Docker Compose\" and click OK. a Step 14. Visual Studio has now added Docker Compose support to our project and a Docker file. If you read through the Output window you will notice that docker has also started a container and is running the application inside that container locally on the dev server. a Step 15. Let's take a quick look at the docker container that is running locally and let's also browse to the application that's running inside the container to verify that it's working as expected. Open a Command Prompt as an administrator and enter the following docker command: \"docker ps\" The docker ps command lists the running Docker processes or containers. We should see that one container is running on the dev server. Now let's take a closer look at this container by entering the following command: docker inspect ' . Tip You only need to enter enough of the CONTAINER ID value for docker to be able to uniquely identify the container. In my case the commands looked like this: a Step 16. Near the bottom of the output form the docker inspect command will list the IP address of the container. Let's grab the IP address, open a browser and verify the application is running correctly from within the docker container. a Congratulations you have containerized a .NET application using Visual Studio and Docker. In the next task we will push the container (containing our application) to your Elastic Container Registry (ECR) on AWS. a Task 5. Push the Container to ECR In this task we will push the container from Visual Studio to ECR. Step 17. Let's start by opening the docker-compose.yml file and copy the image name. a Step 18. Next let's create a new ECR repository. From the AWS Explorer menu expand the Amazon Elastic Container Service section and right-click Repositories and click on Create Repository... a Step 19. Enter the Docker compose image name from step #17 above and click OK. a Step 20. Once the new repository has been created you can expand the Repository section in the AWS Explorer and double-click on the new repository. You can see the repository metadata and that the repository is currently empty. Click on the \"View Push Commands\" link to see a list of helpful PowerShell and AWS CLI commands for interacting with this repository. Click on the \"AWS CLI\" tab. a Step 21. From the AWS CLI tab copy the first command (1. Retrieve the docker login command that you can use to authenticate your Docker client to your registry). We will use the output of this command to allow Visual Studio to authenticate and push the container to our repository. a Step 22. Copy the command from the previous step and run the command as an administrator. The output contains three values: -u for username, -p for password, and the URL of the repository. Leave this command prompt open and we'll copy these values to use in the next step. a Step 23. Now let's configure the publish action on the project. We are going to configure a publish action to a custom container registry. Right-click the project and select Publish. a Step 24. Select \"New Profile...\", then select \"Container Registry\", then \"Custom\" and click Publish. Now we can copy/paste the values from the previous command prompt window in to the Visual Studio Publish UI. a Step 25. Once the values are entered and you click publish Visual Studio will begin to publish the container to your ECR repository on AWS. A Docker prompt will appear to indicate progress of the publish action. a Step 26. Once the publish action is complete you can go back and refresh the repository view from the AWS Explorer and you should see that your repository now has one container image. a Congratulations you now have a container image in AWS that you can deploy to your ECS environment. In the next task we will deploy your container as a task within ECS. Task 6. Deploy and Run the Container on ECS In this task we will create an ECS Task to run the container that has been pushed to ECR. Step 27. Let's jump back to your AWS console and navigate to the ECS admin interface. Once there let's verify again that our container image is in the repository. From the ECS interface under Amazon ECR select Repositories and then click on the repository that we created previously. You should see the container image that we created and pushed. Tip Make a note of Repository URI and the image tag. We will use those values in a future step to identify the container image. a Step 28. Now let's create a Task Definition. Select Task Definitions from the left nav and click on Create a new Task Definition. Select EC2 and click next. a Step 29. Enter a name for the task, review the configuration options available (we'll leave the default values as-is) and then scroll down and click on \"Add container\". a Step 30. In the \"Add container\" UI let's enter a Container name, and in the image field we'll use the values from our repository noted in step #TBD. Pay close attention to the format required here: repository-url/image:tag Info In my example the value for the Image field would be: 353497669637.dkr.ecr.ca-central-1.amazonaws.com/mvcmusicstorewedf2017:latest Let's enter 1024 (1GB) for a Memory Limit, and for Port mappings let's enter 8080 for the Host port and 80 for the Container port. So far your form should look like the following: a Let's scroll down and enter 1024 for \"CPU units\" and enter the following value for \"Entry point\": C:\\ServiceMonitor.exe, w3svc and then click Add a Step 31. You should now see your container under Container Definitions in the Task Definitions form, click on Create and you should see the \"Created Task Definition Successfully\" message. a Step 32. The final step in the sequence is to run the task we just created on our ECS host. From the Task Definitions interface mark/select the Task Definition that we just created and click on the Actions button and select Run Task. a Step 33. Select a launch type of EC2, enter a Task Group name and then click on Run Task. a Step 34. You should receive a \"Created tasks successfully\" message and now let's refresh the ECS Cluster UI until we see that our task status is no longer PENDING and has turned to RUNNING. a Step 35. At this point our container (and application) have been successfully deployed to our ECS Cluster. Now let's test the application to make sure it's running correctly. We'll first test the application locally from the ECS host server and then we'll test the application from the Internet. To do that we'll first need to modify the Security Group rules to allow us to RDP into our ECS host server. Click on the link for \"Container\" and then click on the EC2 Instance ID. a Step 36. From the EC2 admin interface note down the public IP address for use later and then under \"Security groups\" click on the one security group link. a Step 37. First select the \"Inbound\" tab, then select \"Edit\", then select \"Add Rule\", then select \"RDP from the dropdown, then enter \"0.0.0.0/0\" for the CIDR and finally click Save. a Step 38. Let's RDP into our ECS Host and run a docker ps command to verify that our task/container is running. a Step 39. Now lets run a dicker inspect command on our running container and grab it's IP address. a Step 40. And using the container's IP let's browse the site locally. a Step 41. We've now confirmed that our ECS task/container is running correctly on our ECS host. The final step will be to browse to the site from the Internet using port 8080 per the task port mapping we created earlier. From your laptop open a browser and using the public IP address of the ECS host recorded in step #36 append port 8080 to the IP address and browse to the site.","title":"Let's Build"},{"location":"reinvent2018/win314/build/#modernize-your-first-windows-application-with-windows-containers","text":"Duration: 45 minutes","title":"Modernize Your First Windows Application with Windows Containers"},{"location":"reinvent2018/win314/build/#task-1-log-in-to-your-aws-account","text":"Using the console URL and username/password information from the welcome page let's log in to your AWS account.","title":"Task 1. Log in to your AWS Account"},{"location":"reinvent2018/win314/build/#task-2-build-the-amazon-elastic-container-service-ecs-cluster","text":"In this task we will build the Windows Server ECS cluster. Step 1. In the AWS console change your AWS Region to \"EU (Frankfurt)\". a Step 2. Now let's navigate to the ECS admin interface and then we'll select \"Clusters\" from the left nav. a Step 3. Click on \"Create Cluster\" and then select \"EC2 Windows + Networking\" and then click \"Next step\" a Step 4. Enter a Cluster name and then review the configuration options available (we will leave the default values as-is with the exception of the key pair). Info Because this is a new AWS account you will need to create a key pair in the Frankfurt Region. Follow the \"EC2 Console link\" to create a new key pair. a Step 5. Once the key pair has been created return to the ECS Cluster tab and click the refresh icon next to the Key pair and your new key pair should appear. a Step 6. Review the remaining configuration options and click \"Create\" to build the ECS Windows cluster. While the ECS cluster is building we'll jump over to our dev server and get started with configuring Visual Studio. a","title":"Task 2. Build the Amazon Elastic Container Service (ECS) Cluster"},{"location":"reinvent2018/win314/build/#task-3-configure-the-dev-server","text":"In this task we will RDP in to our dev server and configure Visual Studio and the AWS Toolkit for Visual Studio. Step 7. Let's RDP into your dev server. From the EC2 admin interface click on \"Running instances\" and then select the \"WIN314 Dev Server\", click on \"Connect\", then click on \"Download Remote Desktop File\" and finally launch the RDP file. a Step 8. When you are prompted for credentials first click on \"More choices\", then click on \"Use a different account\" and then enter the following credentials: Info username: .\\developer password: ILove.Net! a Step 9. Once we have established an RDP connection to the dev server let's configure the AWS CLI with your AWS account information. Open a command prompt as an administrator (right-click the command prompt, select more and the select Run as administrator) and enter the command: \"aws configure\", when prompted enter you Access Key and Secret Access Key (included in the email sent by your instructor). a Step 10. Now let's open Visual Studio (VS) 2017 Community edition and configure the AWS Toolkit for Visual Studio with your AWS account information. Launch Visual Studio and wait a few seconds for the AWS Explorer UI to appear. If it doesn't appear from the VS menu select View -> AWS Explorer and enter your AWS account information if prompted. From the AWS Explorer let's change the AWS Region to Frankfurt. Tip Once you change the Region you can expand the ECS item and double-click your ECS cluster to see it's current status a Step 11. Now let's open our sample .NET application. From the Start Page tab (or form the Visual Studio menu File -> Open -> Project/Solution) open the \"MvsMusicStore-Wed-F2017\" Solution (located at C:\\Source) a Step 12. Once the solution has loaded let's run it locally to make sure the application is working. You may receive some warnings regarding SSL, self-signed certificates, the current debug mode, etc. Simply accept those warnings and then the application should render: a Now that have confirmed we have a running application let's start the process of containerizing our application. a","title":"Task 3. Configure the Dev Server"},{"location":"reinvent2018/win314/build/#task-4-containerize-net-app-in-visual-studio","text":"In this task we will containerize the application in Visual Studio using Docker compose. Step 13. Let's start by adding Docker support to our project. Right-click the project and select Add -> Container Orchestrator Support. Leave the default value for \"Docker Compose\" and click OK. a Step 14. Visual Studio has now added Docker Compose support to our project and a Docker file. If you read through the Output window you will notice that docker has also started a container and is running the application inside that container locally on the dev server. a Step 15. Let's take a quick look at the docker container that is running locally and let's also browse to the application that's running inside the container to verify that it's working as expected. Open a Command Prompt as an administrator and enter the following docker command: \"docker ps\" The docker ps command lists the running Docker processes or containers. We should see that one container is running on the dev server. Now let's take a closer look at this container by entering the following command: docker inspect ' . Tip You only need to enter enough of the CONTAINER ID value for docker to be able to uniquely identify the container. In my case the commands looked like this: a Step 16. Near the bottom of the output form the docker inspect command will list the IP address of the container. Let's grab the IP address, open a browser and verify the application is running correctly from within the docker container. a Congratulations you have containerized a .NET application using Visual Studio and Docker. In the next task we will push the container (containing our application) to your Elastic Container Registry (ECR) on AWS. a","title":"Task 4. Containerize .NET App in Visual Studio"},{"location":"reinvent2018/win314/build/#task-5-push-the-container-to-ecr","text":"In this task we will push the container from Visual Studio to ECR. Step 17. Let's start by opening the docker-compose.yml file and copy the image name. a Step 18. Next let's create a new ECR repository. From the AWS Explorer menu expand the Amazon Elastic Container Service section and right-click Repositories and click on Create Repository... a Step 19. Enter the Docker compose image name from step #17 above and click OK. a Step 20. Once the new repository has been created you can expand the Repository section in the AWS Explorer and double-click on the new repository. You can see the repository metadata and that the repository is currently empty. Click on the \"View Push Commands\" link to see a list of helpful PowerShell and AWS CLI commands for interacting with this repository. Click on the \"AWS CLI\" tab. a Step 21. From the AWS CLI tab copy the first command (1. Retrieve the docker login command that you can use to authenticate your Docker client to your registry). We will use the output of this command to allow Visual Studio to authenticate and push the container to our repository. a Step 22. Copy the command from the previous step and run the command as an administrator. The output contains three values: -u for username, -p for password, and the URL of the repository. Leave this command prompt open and we'll copy these values to use in the next step. a Step 23. Now let's configure the publish action on the project. We are going to configure a publish action to a custom container registry. Right-click the project and select Publish. a Step 24. Select \"New Profile...\", then select \"Container Registry\", then \"Custom\" and click Publish. Now we can copy/paste the values from the previous command prompt window in to the Visual Studio Publish UI. a Step 25. Once the values are entered and you click publish Visual Studio will begin to publish the container to your ECR repository on AWS. A Docker prompt will appear to indicate progress of the publish action. a Step 26. Once the publish action is complete you can go back and refresh the repository view from the AWS Explorer and you should see that your repository now has one container image. a Congratulations you now have a container image in AWS that you can deploy to your ECS environment. In the next task we will deploy your container as a task within ECS.","title":"Task 5. Push the Container to ECR"},{"location":"reinvent2018/win314/build/#task-6-deploy-and-run-the-container-on-ecs","text":"In this task we will create an ECS Task to run the container that has been pushed to ECR. Step 27. Let's jump back to your AWS console and navigate to the ECS admin interface. Once there let's verify again that our container image is in the repository. From the ECS interface under Amazon ECR select Repositories and then click on the repository that we created previously. You should see the container image that we created and pushed. Tip Make a note of Repository URI and the image tag. We will use those values in a future step to identify the container image. a Step 28. Now let's create a Task Definition. Select Task Definitions from the left nav and click on Create a new Task Definition. Select EC2 and click next. a Step 29. Enter a name for the task, review the configuration options available (we'll leave the default values as-is) and then scroll down and click on \"Add container\". a Step 30. In the \"Add container\" UI let's enter a Container name, and in the image field we'll use the values from our repository noted in step #TBD. Pay close attention to the format required here: repository-url/image:tag Info In my example the value for the Image field would be: 353497669637.dkr.ecr.ca-central-1.amazonaws.com/mvcmusicstorewedf2017:latest Let's enter 1024 (1GB) for a Memory Limit, and for Port mappings let's enter 8080 for the Host port and 80 for the Container port. So far your form should look like the following: a Let's scroll down and enter 1024 for \"CPU units\" and enter the following value for \"Entry point\": C:\\ServiceMonitor.exe, w3svc and then click Add a Step 31. You should now see your container under Container Definitions in the Task Definitions form, click on Create and you should see the \"Created Task Definition Successfully\" message. a Step 32. The final step in the sequence is to run the task we just created on our ECS host. From the Task Definitions interface mark/select the Task Definition that we just created and click on the Actions button and select Run Task. a Step 33. Select a launch type of EC2, enter a Task Group name and then click on Run Task. a Step 34. You should receive a \"Created tasks successfully\" message and now let's refresh the ECS Cluster UI until we see that our task status is no longer PENDING and has turned to RUNNING. a Step 35. At this point our container (and application) have been successfully deployed to our ECS Cluster. Now let's test the application to make sure it's running correctly. We'll first test the application locally from the ECS host server and then we'll test the application from the Internet. To do that we'll first need to modify the Security Group rules to allow us to RDP into our ECS host server. Click on the link for \"Container\" and then click on the EC2 Instance ID. a Step 36. From the EC2 admin interface note down the public IP address for use later and then under \"Security groups\" click on the one security group link. a Step 37. First select the \"Inbound\" tab, then select \"Edit\", then select \"Add Rule\", then select \"RDP from the dropdown, then enter \"0.0.0.0/0\" for the CIDR and finally click Save. a Step 38. Let's RDP into our ECS Host and run a docker ps command to verify that our task/container is running. a Step 39. Now lets run a dicker inspect command on our running container and grab it's IP address. a Step 40. And using the container's IP let's browse the site locally. a Step 41. We've now confirmed that our ECS task/container is running correctly on our ECS host. The final step will be to browse to the site from the Internet using port 8080 per the task port mapping we created earlier. From your laptop open a browser and using the public IP address of the ECS host recorded in step #36 append port 8080 to the IP address and browse to the site.","title":"Task 6. Deploy and Run the Container on ECS"},{"location":"reinvent2018/win314/win314index/","text":"Welcome to WIN314 - Modernize Your First Windows Application with Windows Containers Our builder session will begin with a short presentation on how to containerize .NET applications on AWS. You will then have the opportunity to build a Docker container for a sample .NET application and deploy that container/application to Amazon Elastic Container Service (ECS). We have created a temporary AWS account for you to use during this builder session. The account has the running EC2 Windows Server already configured with the tools needed to run this session. Enter the code given to you by your instructor to retrieve your console access URL, username/password and AWS keys. We'll use these to log in to your temporary AWS account. Code: function handleKey(event) { event.stopPropagation(); } function showCredentials(el) { var codeInput = document.getElementById('code'); var code = codeInput.value; var win = window.open(\"https://cj31tpwxr8.execute-api.us-east-2.amazonaws.com/Prod/api/values/\" + code, '_blank'); win.focus(); } Let's get started! (click next below)","title":"Home"},{"location":"reinvent2018/win314/win314index/#welcome-to-win314-modernize-your-first-windows-application-with-windows-containers","text":"Our builder session will begin with a short presentation on how to containerize .NET applications on AWS. You will then have the opportunity to build a Docker container for a sample .NET application and deploy that container/application to Amazon Elastic Container Service (ECS). We have created a temporary AWS account for you to use during this builder session. The account has the running EC2 Windows Server already configured with the tools needed to run this session. Enter the code given to you by your instructor to retrieve your console access URL, username/password and AWS keys. We'll use these to log in to your temporary AWS account. Code: function handleKey(event) { event.stopPropagation(); } function showCredentials(el) { var codeInput = document.getElementById('code'); var code = codeInput.value; var win = window.open(\"https://cj31tpwxr8.execute-api.us-east-2.amazonaws.com/Prod/api/values/\" + code, '_blank'); win.focus(); } Let's get started! (click next below)","title":"Welcome to WIN314 - Modernize Your First Windows Application with Windows Containers"},{"location":"reinvent2018/win319/1_DeployAd/","text":"Step 1: Deploying AD-DS on EC2 using Active Directory (AD) Quick Start Log in to your AWS account at https://aws.amazon.com/ Select My Account, AWS Management Console. Enter account provided, select \"Next\", and then enter Username and password which will be provided separately. Create a keypair for use during this lab, Instructions are located here: https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair On a new browser tab, open the Active Directory Quick Start: https://docs.aws.amazon.com/quickstart/latest/active-directory-ds/step2.html . Click Launch Quickstart for scenario 1 . Make sure you select US East (N. Virginia) as your AWS region. Click Next . Provide the below inputs during the CloudFormation Stack creation: Choose at least two availability zones, preferably us-east-1e and us-east-1f. VPC CIDR: 10.0.0.0/16 Private Subnet 1 CIDR: 10.0.1.0/24 Private Subnet 2 CIDR: 10.0.2.0/24 Public Subnet 1 CIDR: 10.0.10.0/24 Public Subnet 2 CIDR: 10.0.20.0/24 Allowed Remote Desktop Gateway Access CIDR: Provide your public IP address in CIDR notation. (Example: 54.239.6.185/32). You can find your public IP address by going to http://myipaddress.com using your browser. Select the keypair you created during the pre-requisite steps. Create a new keypair if you don't have the private key for an existing keypair. Select m4.xlarge for both domain controller size. Domain Controller 1 Name: onpremdc1 Domain Controller 1 IP Address: 10.0.1.10 Domain Controller 2 Name: onpremdc2 Domain Controller 2 IP Address: 10.0.2.10 .Select t2.large for Remote Desktop Gateway Instance Type Domain DNS name: Onprem.local Domain NetBIOS name: ONPREM Supply a domain admin & restore mode password and make sure to remember it. Change domain admin name from 'StackAdmin' to 'admin'. Leave Number of RDGW Hosts to 1. Leave the AWS Quick Start Configuration values for S3 Bucket Name and S3 Key Prefix to the default. Click Next to go to the next page twice and click to create the stack after selecting the check box against \"I acknowledge that AWS CloudFormation might create IAM resources with custom names\". CloudFormation will take some time to create this stack. To save time, you can proceed to Step 2 and come back to check the status of the CloudFormation stack. You must see CREATE_COMPLETE in the Status when the stack has finished creating.","title":"Create Directory"},{"location":"reinvent2018/win319/1_DeployAd/#step-1-deploying-ad-ds-on-ec2-using-active-directory-ad-quick-start","text":"Log in to your AWS account at https://aws.amazon.com/ Select My Account, AWS Management Console. Enter account provided, select \"Next\", and then enter Username and password which will be provided separately. Create a keypair for use during this lab, Instructions are located here: https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair On a new browser tab, open the Active Directory Quick Start: https://docs.aws.amazon.com/quickstart/latest/active-directory-ds/step2.html . Click Launch Quickstart for scenario 1 . Make sure you select US East (N. Virginia) as your AWS region. Click Next . Provide the below inputs during the CloudFormation Stack creation: Choose at least two availability zones, preferably us-east-1e and us-east-1f. VPC CIDR: 10.0.0.0/16 Private Subnet 1 CIDR: 10.0.1.0/24 Private Subnet 2 CIDR: 10.0.2.0/24 Public Subnet 1 CIDR: 10.0.10.0/24 Public Subnet 2 CIDR: 10.0.20.0/24 Allowed Remote Desktop Gateway Access CIDR: Provide your public IP address in CIDR notation. (Example: 54.239.6.185/32). You can find your public IP address by going to http://myipaddress.com using your browser. Select the keypair you created during the pre-requisite steps. Create a new keypair if you don't have the private key for an existing keypair. Select m4.xlarge for both domain controller size. Domain Controller 1 Name: onpremdc1 Domain Controller 1 IP Address: 10.0.1.10 Domain Controller 2 Name: onpremdc2 Domain Controller 2 IP Address: 10.0.2.10 .Select t2.large for Remote Desktop Gateway Instance Type Domain DNS name: Onprem.local Domain NetBIOS name: ONPREM Supply a domain admin & restore mode password and make sure to remember it. Change domain admin name from 'StackAdmin' to 'admin'. Leave Number of RDGW Hosts to 1. Leave the AWS Quick Start Configuration values for S3 Bucket Name and S3 Key Prefix to the default. Click Next to go to the next page twice and click to create the stack after selecting the check box against \"I acknowledge that AWS CloudFormation might create IAM resources with custom names\". CloudFormation will take some time to create this stack. To save time, you can proceed to Step 2 and come back to check the status of the CloudFormation stack. You must see CREATE_COMPLETE in the Status when the stack has finished creating.","title":"Step 1: Deploying AD-DS on EC2 using Active Directory (AD) Quick Start"},{"location":"reinvent2018/win319/2_DeployMad/","text":"Step 2: Deploying AWS Managed Active Directory through the console Go to the VPC service in the AWS console. Click on Subnets on the left. Create two private subnets with the following parameters: Name tag: ManagedAD-Subnet1, CIDR: 10.0.100.0/24 Name tag: ManagedAD-Subnet2, CIDR: 10.0.200.0/24 Select the correct VPC that was created by the AD Quickstart. The VPC name starts with \"Active-Directory-DS-VPCStack-\u2026\" Pick different availability zone for each of the above private subnets. On the AWS Management Console, click Services and choose Directory Service under Security, Identity & Compliance. Click Set up Directory to create a new directory and choose to create an AWS Managed Microsoft AD. Provide the below inputs during the creation of the Directory Service: Edition: Standard Directory DNS Name: cloud.local Directory NetBIOS Name: CLOUD Directory Description: Provide a suitable description Admin Password: Provide a password for the CLOUD\\Admin account. Make sure to remember the password. Provide the VPC and subnets your prepared in step 6. Select Create Directory for the Managed AD to begin to creation. This will take a while. Go back to CloudFormation and view the progress of the stack creation for the environment that will serve as your on-premises AD. You must see CREATE_COMPLETE in the Status when the stack has finished creating.","title":"Deploy Managed AD"},{"location":"reinvent2018/win319/2_DeployMad/#step-2-deploying-aws-managed-active-directory-through-the-console","text":"Go to the VPC service in the AWS console. Click on Subnets on the left. Create two private subnets with the following parameters: Name tag: ManagedAD-Subnet1, CIDR: 10.0.100.0/24 Name tag: ManagedAD-Subnet2, CIDR: 10.0.200.0/24 Select the correct VPC that was created by the AD Quickstart. The VPC name starts with \"Active-Directory-DS-VPCStack-\u2026\" Pick different availability zone for each of the above private subnets. On the AWS Management Console, click Services and choose Directory Service under Security, Identity & Compliance. Click Set up Directory to create a new directory and choose to create an AWS Managed Microsoft AD. Provide the below inputs during the creation of the Directory Service: Edition: Standard Directory DNS Name: cloud.local Directory NetBIOS Name: CLOUD Directory Description: Provide a suitable description Admin Password: Provide a password for the CLOUD\\Admin account. Make sure to remember the password. Provide the VPC and subnets your prepared in step 6. Select Create Directory for the Managed AD to begin to creation. This will take a while. Go back to CloudFormation and view the progress of the stack creation for the environment that will serve as your on-premises AD. You must see CREATE_COMPLETE in the Status when the stack has finished creating.","title":"Step 2: Deploying AWS Managed Active Directory through the console"},{"location":"reinvent2018/win319/3_Trust/","text":"Step 3: Setup 2-way trust between AWS Managed AD & on-premises AD Part 1: Allow communication between AWS Managed AD & on-prem AD Log in to your AWS account and access the Security Group page under EC2. https://console.aws.amazon.com/vpc/home?region=us-east-1#securityGroups: In the Filter/Search bar type Controller and hit enter. This will show you the AD/AWS Directory Service Security groups: Look at the descriptions, you will see two \"Domain Controller\" Security group. These correspond to the two on-prem domain controllers networks that was built as a part of AD quickstart. You will also see the Security group that was created for your AWS Directory Service \"AWS created security group for xxx directory controllers\". Take note of these security group ID's. Click on the Security Group named \"d-xxxxxxx_controllers\" Click the Outbound Rules tab and click Edit You will need to create two Allow All Outbound rules, one for each Domain Controller Security group set as the destination as follows: Click Save Now you will create rules to allow AWS Directory Service traffic to each of the on-prem Domain Controller Security Groups Click on one of the \"Domain Controller\" Security Group Click Inbound Rules and click Edit Create an Allow All inbound rule and specify the AWS Directory Service Security Group as the source: Click Save Repeat these steps for the other On-Premise Domain Controller Security Group Part 2: Enable Kerberos pre-authentication on both user accounts User accounts in both directories must have Kerberos preauthentication enabled. This is the default, but it is important to verify: First, we'll update the security group for the Remote Desktop Gateway Security Group. In the Filter/Search bar type RDGW and hit enter. This will show you the Remote Desktop Gateway Security group. Select Inbound Rules, Edit Rules, and change the \"Source IP\" to the correct IP, which can be found at http://myipaddress.com/ . Ensure the IP has the /32 at the end in CIDR notation. Connect to your on-premise domain controller via the RDGW. You can find the public IP address or DNS name in the EC2 console. The name of the instance will be \"RDGW\" Note: You can login to the RDGW using \"ONPREM\\admin\" as username and password you set during the AD quickstart. Once you login to RDGW, you can RDP again from here to one of the domain controllers (10.0.1.10 or 10.0.2.10). Once you login to the domain controller, open Server Manager. On the Tools menu, choose Active Directory Users and Computers. Choose the Users folder and open the context (right-click) menu for the admin account listed in the right pane. Choose Properties. Choose the Account tab. In the Account options list, scroll down and ensure that Do not require Kerberos preauthentication is not checked. Part 3: Configure Conditional Forwarder on On-Prem Domain You will need to configure conditional forwarders on both domain (on-prem & Managed AD) so domain members in each domain can resolve name in the other domain. This is also a pre-requisite before you can setup trust between the domains. Sign into the AWS Management Console and open the AWS Directory Service console at https://console.aws.amazon.com/directoryservicev2/. In the navigation pane, select Directories. Click on the directory ID of your AWS Managed Microsoft AD. On the Details page, take note of the values in Directory name and the DNS address of your directory. Using Microsoft Remote Desktop, connect to the \"RDGW\" using your ONPREM\\admin credentials. Once connected to the RDGW, use Microsoft Remote Desktop to connect to DC1/DC2 using the ONPREM\\admin credentials. Open Server Manager. On the Tools menu, choose DNS. In the console tree, expand the DNS server of the domain for which you are setting up the trust. In our case it is DC1.onprem.local In the console tree, choose Conditional Forwarders. Right-click and choose New conditional forwarder. In DNS domain, type the fully qualified domain name (FQDN) of your AWS Managed Microsoft AD (cloud.local). Under IP addresses of the master servers, add the IP's of your AWS Managed Microsoft AD directory, which you noted earlier. Note: After entering the DNS addresses, you may get a \"timeout\" or \"unable to resolve\" error. You can ignore these errors. Select Store this conditional forwarder in Active Directory, and replicate it as follows. Select All DNS servers in this domain, and then choose OK. Part 3: Configure on-premises trust relationship In this section, you configure a two-way trust. However, if you create a one-way trust, be aware that the trust directions on each of your domains must be complementary. For example, if you create a one-way, outgoing trust on your on-premises domain, you need to create a one-way, incoming trust on your AWS Managed Microsoft AD. Log onto RDGW through Remote Desktop then login to DC1/DC2. Open Server Manager and on the Tools menu, click on Active Directory Domains and Trusts . Open the context (right-click) menu of your domain and choose Properties. Choose the Trusts tab and choose New trust. Type the name of your AWS Managed AD cloud.local and click Next. Choose Forest trust . Click Next. Choose Two-way . Click Next. Choose This domain only . Click Next . Choose Forest-wide authentication . Click Next . Type a Trust password. Make sure to remember this password as you will need it when setting up the trust for your AWS Managed Microsoft AD. In the next dialog box, confirm your settings and choose Next. Confirm that the trust was created successfully and again choose Next. Choose No, do not confirm the outgoing trust. Click Next. Choose No, do not confirm the incoming trust. Click Next. To close this wizard, select \"Finish\". Part 5: Configure AWS Managed AD trust relationship As a last step, you will need to configure the trust relationship for your AWS Managed Microsoft AD. Because you created a two-way trust on the on-premises domain, you will also create a two-way trust on your AWS Managed Microsoft AD. Sign into the AWS Management Console and open the AWS Directory Service console at https://console.aws.amazon.com/directoryservicev2/. On the Directories page, click on your AWS Managed Microsoft AD. On the Directory details page, select the Networking & security tab. In the Trust relationships section, choose Actions, and then select Add trust relationship. On the Add a trust relationship page, select Forest Trust, and type the FQDN of your on-premises domain, onprem.local. (Type the same trust password that you used when creating the trust on your on-premises domain. Specify the direction, in this case we choose Two-way. In the Conditional forwarder field, enter the IP address of your on-premises DNS server. In this case, enter 10.0.1.10. (Optional) Choose Add another IP address and enter a second IP address for your on-premises DNS server. In this example it will be 10.0.2.10. Click Add to continue. You will find the status of the trust changes to Creating. After a few minutes, this should change to Verified. You now have a trust relationship between your on-premises domain (onprem.local) and your AWS Managed Microsoft AD (cloud.local).","title":"Create Domain Trust"},{"location":"reinvent2018/win319/3_Trust/#step-3-setup-2-way-trust-between-aws-managed-ad-on-premises-ad","text":"","title":"Step 3: Setup 2-way trust between AWS Managed AD &amp; on-premises AD"},{"location":"reinvent2018/win319/3_Trust/#part-1-allow-communication-between-aws-managed-ad-on-prem-ad","text":"Log in to your AWS account and access the Security Group page under EC2. https://console.aws.amazon.com/vpc/home?region=us-east-1#securityGroups: In the Filter/Search bar type Controller and hit enter. This will show you the AD/AWS Directory Service Security groups: Look at the descriptions, you will see two \"Domain Controller\" Security group. These correspond to the two on-prem domain controllers networks that was built as a part of AD quickstart. You will also see the Security group that was created for your AWS Directory Service \"AWS created security group for xxx directory controllers\". Take note of these security group ID's. Click on the Security Group named \"d-xxxxxxx_controllers\" Click the Outbound Rules tab and click Edit You will need to create two Allow All Outbound rules, one for each Domain Controller Security group set as the destination as follows: Click Save Now you will create rules to allow AWS Directory Service traffic to each of the on-prem Domain Controller Security Groups Click on one of the \"Domain Controller\" Security Group Click Inbound Rules and click Edit Create an Allow All inbound rule and specify the AWS Directory Service Security Group as the source: Click Save Repeat these steps for the other On-Premise Domain Controller Security Group","title":"Part 1: Allow communication between AWS Managed AD &amp; on-prem AD"},{"location":"reinvent2018/win319/3_Trust/#part-2-enable-kerberos-pre-authentication-on-both-user-accounts","text":"User accounts in both directories must have Kerberos preauthentication enabled. This is the default, but it is important to verify: First, we'll update the security group for the Remote Desktop Gateway Security Group. In the Filter/Search bar type RDGW and hit enter. This will show you the Remote Desktop Gateway Security group. Select Inbound Rules, Edit Rules, and change the \"Source IP\" to the correct IP, which can be found at http://myipaddress.com/ . Ensure the IP has the /32 at the end in CIDR notation. Connect to your on-premise domain controller via the RDGW. You can find the public IP address or DNS name in the EC2 console. The name of the instance will be \"RDGW\" Note: You can login to the RDGW using \"ONPREM\\admin\" as username and password you set during the AD quickstart. Once you login to RDGW, you can RDP again from here to one of the domain controllers (10.0.1.10 or 10.0.2.10). Once you login to the domain controller, open Server Manager. On the Tools menu, choose Active Directory Users and Computers. Choose the Users folder and open the context (right-click) menu for the admin account listed in the right pane. Choose Properties. Choose the Account tab. In the Account options list, scroll down and ensure that Do not require Kerberos preauthentication is not checked.","title":"Part 2: Enable Kerberos pre-authentication on both user accounts"},{"location":"reinvent2018/win319/3_Trust/#part-3-configure-conditional-forwarder-on-on-prem-domain","text":"You will need to configure conditional forwarders on both domain (on-prem & Managed AD) so domain members in each domain can resolve name in the other domain. This is also a pre-requisite before you can setup trust between the domains. Sign into the AWS Management Console and open the AWS Directory Service console at https://console.aws.amazon.com/directoryservicev2/. In the navigation pane, select Directories. Click on the directory ID of your AWS Managed Microsoft AD. On the Details page, take note of the values in Directory name and the DNS address of your directory. Using Microsoft Remote Desktop, connect to the \"RDGW\" using your ONPREM\\admin credentials. Once connected to the RDGW, use Microsoft Remote Desktop to connect to DC1/DC2 using the ONPREM\\admin credentials. Open Server Manager. On the Tools menu, choose DNS. In the console tree, expand the DNS server of the domain for which you are setting up the trust. In our case it is DC1.onprem.local In the console tree, choose Conditional Forwarders. Right-click and choose New conditional forwarder. In DNS domain, type the fully qualified domain name (FQDN) of your AWS Managed Microsoft AD (cloud.local). Under IP addresses of the master servers, add the IP's of your AWS Managed Microsoft AD directory, which you noted earlier. Note: After entering the DNS addresses, you may get a \"timeout\" or \"unable to resolve\" error. You can ignore these errors. Select Store this conditional forwarder in Active Directory, and replicate it as follows. Select All DNS servers in this domain, and then choose OK.","title":"Part 3: Configure Conditional Forwarder on On-Prem Domain"},{"location":"reinvent2018/win319/3_Trust/#part-3-configure-on-premises-trust-relationship","text":"In this section, you configure a two-way trust. However, if you create a one-way trust, be aware that the trust directions on each of your domains must be complementary. For example, if you create a one-way, outgoing trust on your on-premises domain, you need to create a one-way, incoming trust on your AWS Managed Microsoft AD. Log onto RDGW through Remote Desktop then login to DC1/DC2. Open Server Manager and on the Tools menu, click on Active Directory Domains and Trusts . Open the context (right-click) menu of your domain and choose Properties. Choose the Trusts tab and choose New trust. Type the name of your AWS Managed AD cloud.local and click Next. Choose Forest trust . Click Next. Choose Two-way . Click Next. Choose This domain only . Click Next . Choose Forest-wide authentication . Click Next . Type a Trust password. Make sure to remember this password as you will need it when setting up the trust for your AWS Managed Microsoft AD. In the next dialog box, confirm your settings and choose Next. Confirm that the trust was created successfully and again choose Next. Choose No, do not confirm the outgoing trust. Click Next. Choose No, do not confirm the incoming trust. Click Next. To close this wizard, select \"Finish\".","title":"Part 3: Configure on-premises trust relationship"},{"location":"reinvent2018/win319/3_Trust/#part-5-configure-aws-managed-ad-trust-relationship","text":"As a last step, you will need to configure the trust relationship for your AWS Managed Microsoft AD. Because you created a two-way trust on the on-premises domain, you will also create a two-way trust on your AWS Managed Microsoft AD. Sign into the AWS Management Console and open the AWS Directory Service console at https://console.aws.amazon.com/directoryservicev2/. On the Directories page, click on your AWS Managed Microsoft AD. On the Directory details page, select the Networking & security tab. In the Trust relationships section, choose Actions, and then select Add trust relationship. On the Add a trust relationship page, select Forest Trust, and type the FQDN of your on-premises domain, onprem.local. (Type the same trust password that you used when creating the trust on your on-premises domain. Specify the direction, in this case we choose Two-way. In the Conditional forwarder field, enter the IP address of your on-premises DNS server. In this case, enter 10.0.1.10. (Optional) Choose Add another IP address and enter a second IP address for your on-premises DNS server. In this example it will be 10.0.2.10. Click Add to continue. You will find the status of the trust changes to Creating. After a few minutes, this should change to Verified. You now have a trust relationship between your on-premises domain (onprem.local) and your AWS Managed Microsoft AD (cloud.local).","title":"Part 5: Configure AWS Managed AD trust relationship"},{"location":"reinvent2018/win319/4_Createorg/","text":"Step 4: Connect AWS Managed Microsoft AD to AWS SSO Create an AWS Organization with all features enabled On the AWS Organizations console: https://console.aws.amazon.com/organizations/ . Select \"Create Organization\", then in the pop-up box, \"Create Organization\" again.","title":"Create AWS Organization"},{"location":"reinvent2018/win319/5_ConnectADtoSSO/","text":"Step 5: Connect your directory to AWS SSO Open the AWS SSO console at https://console.aws.amazon.com/singlesignon . (On the AWS Management Console, click Services \\> Security, Identity & Compliance \\> AWS Single Sign On ) Click Enable SSO . This process will take approximately 30 seconds. From the Dashboard , choose Manage your directory On the Directory page, do the following: Select \"Change Directory\" where it currently reads \"Directory: AWS SSO directory\" Under Existing directories , select the AWS Managed Microsoft AD directory you want AWS SSO to connect to. Type \"CONFIRM\" to confirm the change. Once the change is complete, select \"Proceed to the directory\". Next to User portal URL , select Customize, and enter a friendly name for the User Portal URL. a. I recommend reinvent2018_birthday as an option.","title":"Connect AD to SSO"},{"location":"reinvent2018/win319/5_ConnectADtoSSO/#step-5-connect-your-directory-to-aws-sso","text":"Open the AWS SSO console at https://console.aws.amazon.com/singlesignon . (On the AWS Management Console, click Services \\> Security, Identity & Compliance \\> AWS Single Sign On ) Click Enable SSO . This process will take approximately 30 seconds. From the Dashboard , choose Manage your directory On the Directory page, do the following: Select \"Change Directory\" where it currently reads \"Directory: AWS SSO directory\" Under Existing directories , select the AWS Managed Microsoft AD directory you want AWS SSO to connect to. Type \"CONFIRM\" to confirm the change. Once the change is complete, select \"Proceed to the directory\". Next to User portal URL , select Customize, and enter a friendly name for the User Portal URL. a. I recommend reinvent2018_birthday as an option.","title":"Step 5: Connect your directory to AWS SSO"},{"location":"reinvent2018/win319/6_VerifyADSSO_MAD/","text":"Part 6: Connect your directory to AWS SSO Open the AWS SSO console at https://console.aws.amazon.com/singlesignon . (On the AWS Management Console, click Services \\> Security, Identity & Compliance \\> AWS Single Sign On ) Select \"AWS Accounts\" on the left navigation bar. Select the AWS Account to setup SSO for. Select \"Assign Users\". you created is selected, cloud.local. Enter the name of a group (For this exercise we will use the AWS Delegated Administrators account), then \"Search connected directory\". Select the group (AWS Delegated Administrators), then select Permission sets\". Create new permission set to define the level of access users in this group will have. We'll use an existing job function policy, but you can create a custom policy. Select \"AdministratorAccess\", then \"Create\". Select the permission set we created, AdministratorAccess, and select \"Finish\" to configure. Once complete, Proeed to AWS Accounts. Return to the \"Dashboard\" from the left navigation bar, and open the User portal URL in a new window. Sign-in with the Admin account you created in Step 2, Deploying AWS Managed Active Directory. You will see \"AWS Account\" as one of Your applications, if you select the AWS account you will see the one account you have access to, the specific permission set (In this case, AdministratorAccess), and then links to the Management Console. Select \"Management Console\". This should log you into the console with the appropriate access rights. Congratulations you setup AWS SSO to Managed AD!","title":"Verify SSO"},{"location":"reinvent2018/win319/6_VerifyADSSO_MAD/#part-6-connect-your-directory-to-aws-sso","text":"Open the AWS SSO console at https://console.aws.amazon.com/singlesignon . (On the AWS Management Console, click Services \\> Security, Identity & Compliance \\> AWS Single Sign On ) Select \"AWS Accounts\" on the left navigation bar. Select the AWS Account to setup SSO for. Select \"Assign Users\". you created is selected, cloud.local. Enter the name of a group (For this exercise we will use the AWS Delegated Administrators account), then \"Search connected directory\". Select the group (AWS Delegated Administrators), then select Permission sets\". Create new permission set to define the level of access users in this group will have. We'll use an existing job function policy, but you can create a custom policy. Select \"AdministratorAccess\", then \"Create\". Select the permission set we created, AdministratorAccess, and select \"Finish\" to configure. Once complete, Proeed to AWS Accounts. Return to the \"Dashboard\" from the left navigation bar, and open the User portal URL in a new window. Sign-in with the Admin account you created in Step 2, Deploying AWS Managed Active Directory. You will see \"AWS Account\" as one of Your applications, if you select the AWS account you will see the one account you have access to, the specific permission set (In this case, AdministratorAccess), and then links to the Management Console. Select \"Management Console\". This should log you into the console with the appropriate access rights. Congratulations you setup AWS SSO to Managed AD!","title":"Part 6: Connect your directory to AWS SSO"},{"location":"reinvent2018/win319/prereq/","text":"Prerequisites for attendees Attendees need access to an AWS account with access to provision resources. We will be using the us-east-1 (N. Virginia) region. The following resources will be provisioned during the lab: VPC (1), EC2 instances (3), AWS Managed AD (1), Elastic IP (1). If you are not using a Windows computer, please make sure you have the latest RDP client installed on your computer.","title":"Pre-Requisite"},{"location":"reinvent2018/win319/win319index/","text":"Active Directory Domain Trusts and AWS SSO Jumpstart Join us for this builders session where you and a small group work with an AWS subject matter expert to build a domain trust between on-premises Active Directory and AWS Managed AD in AWS. As a bonus, learn how to connect AWS Managed AD to AWS SSO.","title":"Index"},{"location":"reinvent2019/index2019/","text":"re:Invent 2019 Title Description","title":"Directory"},{"location":"reinvent2019/index2019/#reinvent-2019","text":"Title Description","title":"re:Invent 2019"},{"location":"workshops/","text":"Full-Day Workshops Title Description **Coming Soon - Windows Immersion Day** \"Windows Immersion Day\" The AWS for Microsoft Workloads Immersion Day is a customizable AWS experience designed to help customers build, deploy, and manage Microsoft applications quickly, easily and cost-effectively. During this Immersion Day customers are presented with AWS solutions to challenging real world Microsoft use cases that relate to their business and IT needs. This Immersion Day is a powerful tool for showing the art of what\u2019s possible on AWS for Microsoft workloads. We have several topics that cover a broad range of Microsoft topics including Windows Server, SQL Server, and more. Level : 200 Duration : Each sub-section is one hour, totaling eight hours. **Coming Soon - .Net Immersion Day** \".Net Immersion Day\" The AWS for .Net Workloads Immersion Day is a customizable AWS experience designed to help developers understand how to use AWS .Net tools to build and deploy on AWS. This Immersion Day allows hands on time with AWS services specific to developer workflows. We have several topics that cover a broad range of .Net development topics including Containers, Visual Studio Toolkit, .Net SDK, and more. Level : 200 Duration : Each sub-section is one hour, totaling eight hours. **Managing Windows Workloads at Scale** \"Managing Windows Workloads at Scale\" Level : 200 Duration : Each sub-section is one hour, totaling eight hours.","title":"Directory"},{"location":"workshops/#full-day-workshops","text":"Title Description **Coming Soon - Windows Immersion Day** \"Windows Immersion Day\" The AWS for Microsoft Workloads Immersion Day is a customizable AWS experience designed to help customers build, deploy, and manage Microsoft applications quickly, easily and cost-effectively. During this Immersion Day customers are presented with AWS solutions to challenging real world Microsoft use cases that relate to their business and IT needs. This Immersion Day is a powerful tool for showing the art of what\u2019s possible on AWS for Microsoft workloads. We have several topics that cover a broad range of Microsoft topics including Windows Server, SQL Server, and more. Level : 200 Duration : Each sub-section is one hour, totaling eight hours. **Coming Soon - .Net Immersion Day** \".Net Immersion Day\" The AWS for .Net Workloads Immersion Day is a customizable AWS experience designed to help developers understand how to use AWS .Net tools to build and deploy on AWS. This Immersion Day allows hands on time with AWS services specific to developer workflows. We have several topics that cover a broad range of .Net development topics including Containers, Visual Studio Toolkit, .Net SDK, and more. Level : 200 Duration : Each sub-section is one hour, totaling eight hours. **Managing Windows Workloads at Scale** \"Managing Windows Workloads at Scale\" Level : 200 Duration : Each sub-section is one hour, totaling eight hours.","title":"Full-Day Workshops"},{"location":"workshops/AMIPipelineAutomation/","text":"Windows Golden AMI Pipeline This lab will demonstrate creating a Windows AMI Pipeline using AWS Systems Manager (SSM) Automation. Most of this lab will be copying and pasting blocks of YAML code and explaining what each block does, a copy of the entire document will be included in step #7, if you would like to fast forward. In the end we will import the document to SSM Automation and fire it off to see what happens. This will help to drive the concepts of SSM Automation, and allow you to get comfortable building your own SSM Automation Worklflows. While this lab will focus on the AMI Pipeline use case, there are many other use cases and scenarios for SSM Automation. The pipeline we will build will do the following: Get Current AMI ID Launch Current AMI Install any Windows OS Updates Update any AWS Agent Components Install Additional Software Sysprep Image Create New Windows AMI Lab Pre-Reqs Click here To Deploy Lab Pre-Reqs into your Account This CloudFormation Template will deploy the following resources: S3 Bucket to store files IAM Instance Role that allows Instances to use the AWS Systems Manager Service Please examine the the cloudformation template below in the resources section. Once deployed go to the Output section and note the LabBucketName and InstanceProfile, we will need this info for later steps. You will need a text or code editor of choice installed on the machine you will be using for this lab. Please ensure that the text\\code editor can do syntax highlighting for YAML as that is the language we will be using in this lab. AWS Systems Manager Automation also supports JSON format. Set Automation Document Scaffolding Create a new text file and call it WindowsAMIPipeline.yaml, and copy in the scaffolding from the next code block. This represents the beginning of our Automation document. A Systems Manager Automation document defines the actions that Systems Manager performs on your managed instances and AWS resources. Documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. Steps run in sequential order. Automation documents are Systems Manager documents of type Automation, as opposed to Command documents. Automation documents currently support schema version 0.3. Command documents use schema version 1.2, 2.0, or 2.2. There are four sections that make up a Automation document, description (optional), schemaVersion (required), parameters (optional), mainSteps (required). description : 'Sample Automation to Create a Windows AMI' schemaVersion : '0.3' parameters : mainSteps : Adding in Parameters Let's Add in some Parameters before we get started on our mainSteps section. Parameters in AWS Systems Manager Automation documents are used in a basic string replace. However, using parameters allows us to re-use Automation Documents instead of building them for single-use. description : 'Sample Automation to Create a Windows AMI' schemaVersion : '0.3' parameters : SourceAmiId : type : String description : '(Required) SSM AMI Parameter.' default : /aws/service/ami-windows-latest/Windows_Server-2019-English-Full-Base IamInstanceProfileName : type : String description : '(Required) The name of the role that enables Systems Manager to manage the instance.' default : IAMInstanceProfile InstanceType : type : String description : '(Optional) Select the instance type.' default : m5.large SubnetId : type : String description : '(Optional) Specify the SubnetId if you want to launch into a specific subnet.' default : '' NewImageName : type : String description : '(Optional) The name of the new AMI that is created.' default : 'NewAMI_Created_On_{{global:DATE_TIME}}' NewImageDescription : type : String description : '(Optional) The description of the new AMI that is created.' default : 'NewAMI_Created_On_{{global:DATE}}' OutputS3BucketName : type : String description : 'The S3 bucket to store logs and grab scripts.' default : 'samples-us-east-1' mainSteps : mainSteps Section - where all the actions happens. Systems Manager Automation runs steps defined in Automation documents. Each step is associated with a particular action. The action determines the inputs, behavior, and outputs of the step. Steps are defined in the mainSteps section of your Automation document. You don't need to specify the outputs of an action or step. The outputs are predetermined by the action associated with the step. When you specify step inputs in your Automation documents, you can reference one or more outputs from an earlier step. For example, you can make the output of aws:runInstances available for a subsequent aws:runCommand action. You can also reference outputs from earlier steps in the Output section of the Automation document. Resources Lab Pre-Req CloudFormation Template AWSTemplateFormatVersion : \"2010-09-09\" Description : This is a Cloudformation that setups Lab Components for PowerShell DSC Lab Resources : LabBucket : Type : AWS::S3::Bucket DeletionPolicy : Delete LogonMessageParam : Type : AWS::SSM::Parameter Properties : Description : Logon Message for Interactive Logon Name : LogonMessage Type : String Value : \"'This is a Test System.,Testing how to Set a Logon Message with.,PowerShell DSC and AWS Systems Manager.,Parameter Store'\" SSMLabRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject Resource : - !Sub 'arn:aws:s3:::aws-ssm-${AWS::Region}/*' - !Sub 'arn:aws:s3:::aws-windows-downloads-${AWS::Region}/*' - !Sub 'arn:aws:s3:::amazon-ssm-${AWS::Region}/*' - !Sub 'arn:aws:s3:::amazon-ssm-packages-${AWS::Region}/*' - !Sub 'arn:aws:s3:::${AWS::Region}-birdwatcher-prod/*' - !Sub 'arn:aws:s3:::patch-baseline-snapshot-${AWS::Region}/*' Effect : Allow PolicyName : ssm-custom-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Action : - ssm:GetParameter Resource : - !Sub 'arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${LogonMessageParam}' Effect : Allow PolicyName : ssm-param-policy - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject - s3:PutObject - s3:PutObjectAcl Resource : - !Sub 'arn:${AWS::Partition}:s3:::${LabBucket}/*' Effect : Allow PolicyName : ssm-mof-bucket-policy AssumeRolePolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Principal : Service : ec2.amazonaws.com Action : sts:AssumeRole ManagedPolicyArns : - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/CloudWatchAgentServerPolicy' SSMLabProfile : Type : AWS::IAM::InstanceProfile Properties : Roles : - !Ref 'PsDscSSMLabRole' Path : / Outputs : LabBucketName : Description : S3 Bucket where MOF File will be uploaded to. Value : !Ref LabBucket InstanceProfile : Value : !Ref SSMLabProfile InstanceRoleArn : Value : !GetAtt SSMLabRole.Arn InstanceRoleName : Value : !Ref SSMLabRole","title":"**Windows Golden AMI Pipeline**"},{"location":"workshops/AMIPipelineAutomation/#windows-golden-ami-pipeline","text":"This lab will demonstrate creating a Windows AMI Pipeline using AWS Systems Manager (SSM) Automation. Most of this lab will be copying and pasting blocks of YAML code and explaining what each block does, a copy of the entire document will be included in step #7, if you would like to fast forward. In the end we will import the document to SSM Automation and fire it off to see what happens. This will help to drive the concepts of SSM Automation, and allow you to get comfortable building your own SSM Automation Worklflows. While this lab will focus on the AMI Pipeline use case, there are many other use cases and scenarios for SSM Automation. The pipeline we will build will do the following: Get Current AMI ID Launch Current AMI Install any Windows OS Updates Update any AWS Agent Components Install Additional Software Sysprep Image Create New Windows AMI","title":"Windows Golden AMI Pipeline"},{"location":"workshops/AMIPipelineAutomation/#lab-pre-reqs","text":"Click here To Deploy Lab Pre-Reqs into your Account This CloudFormation Template will deploy the following resources: S3 Bucket to store files IAM Instance Role that allows Instances to use the AWS Systems Manager Service Please examine the the cloudformation template below in the resources section. Once deployed go to the Output section and note the LabBucketName and InstanceProfile, we will need this info for later steps. You will need a text or code editor of choice installed on the machine you will be using for this lab. Please ensure that the text\\code editor can do syntax highlighting for YAML as that is the language we will be using in this lab. AWS Systems Manager Automation also supports JSON format.","title":"Lab Pre-Reqs"},{"location":"workshops/AMIPipelineAutomation/#set-automation-document-scaffolding","text":"Create a new text file and call it WindowsAMIPipeline.yaml, and copy in the scaffolding from the next code block. This represents the beginning of our Automation document. A Systems Manager Automation document defines the actions that Systems Manager performs on your managed instances and AWS resources. Documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. Steps run in sequential order. Automation documents are Systems Manager documents of type Automation, as opposed to Command documents. Automation documents currently support schema version 0.3. Command documents use schema version 1.2, 2.0, or 2.2. There are four sections that make up a Automation document, description (optional), schemaVersion (required), parameters (optional), mainSteps (required). description : 'Sample Automation to Create a Windows AMI' schemaVersion : '0.3' parameters : mainSteps :","title":"Set Automation Document Scaffolding"},{"location":"workshops/AMIPipelineAutomation/#adding-in-parameters","text":"Let's Add in some Parameters before we get started on our mainSteps section. Parameters in AWS Systems Manager Automation documents are used in a basic string replace. However, using parameters allows us to re-use Automation Documents instead of building them for single-use. description : 'Sample Automation to Create a Windows AMI' schemaVersion : '0.3' parameters : SourceAmiId : type : String description : '(Required) SSM AMI Parameter.' default : /aws/service/ami-windows-latest/Windows_Server-2019-English-Full-Base IamInstanceProfileName : type : String description : '(Required) The name of the role that enables Systems Manager to manage the instance.' default : IAMInstanceProfile InstanceType : type : String description : '(Optional) Select the instance type.' default : m5.large SubnetId : type : String description : '(Optional) Specify the SubnetId if you want to launch into a specific subnet.' default : '' NewImageName : type : String description : '(Optional) The name of the new AMI that is created.' default : 'NewAMI_Created_On_{{global:DATE_TIME}}' NewImageDescription : type : String description : '(Optional) The description of the new AMI that is created.' default : 'NewAMI_Created_On_{{global:DATE}}' OutputS3BucketName : type : String description : 'The S3 bucket to store logs and grab scripts.' default : 'samples-us-east-1' mainSteps :","title":"Adding in Parameters"},{"location":"workshops/AMIPipelineAutomation/#mainsteps-section-where-all-the-actions-happens","text":"Systems Manager Automation runs steps defined in Automation documents. Each step is associated with a particular action. The action determines the inputs, behavior, and outputs of the step. Steps are defined in the mainSteps section of your Automation document. You don't need to specify the outputs of an action or step. The outputs are predetermined by the action associated with the step. When you specify step inputs in your Automation documents, you can reference one or more outputs from an earlier step. For example, you can make the output of aws:runInstances available for a subsequent aws:runCommand action. You can also reference outputs from earlier steps in the Output section of the Automation document.","title":"mainSteps Section - where all the actions happens."},{"location":"workshops/AMIPipelineAutomation/#resources","text":"Lab Pre-Req CloudFormation Template AWSTemplateFormatVersion : \"2010-09-09\" Description : This is a Cloudformation that setups Lab Components for PowerShell DSC Lab Resources : LabBucket : Type : AWS::S3::Bucket DeletionPolicy : Delete LogonMessageParam : Type : AWS::SSM::Parameter Properties : Description : Logon Message for Interactive Logon Name : LogonMessage Type : String Value : \"'This is a Test System.,Testing how to Set a Logon Message with.,PowerShell DSC and AWS Systems Manager.,Parameter Store'\" SSMLabRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject Resource : - !Sub 'arn:aws:s3:::aws-ssm-${AWS::Region}/*' - !Sub 'arn:aws:s3:::aws-windows-downloads-${AWS::Region}/*' - !Sub 'arn:aws:s3:::amazon-ssm-${AWS::Region}/*' - !Sub 'arn:aws:s3:::amazon-ssm-packages-${AWS::Region}/*' - !Sub 'arn:aws:s3:::${AWS::Region}-birdwatcher-prod/*' - !Sub 'arn:aws:s3:::patch-baseline-snapshot-${AWS::Region}/*' Effect : Allow PolicyName : ssm-custom-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Action : - ssm:GetParameter Resource : - !Sub 'arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${LogonMessageParam}' Effect : Allow PolicyName : ssm-param-policy - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject - s3:PutObject - s3:PutObjectAcl Resource : - !Sub 'arn:${AWS::Partition}:s3:::${LabBucket}/*' Effect : Allow PolicyName : ssm-mof-bucket-policy AssumeRolePolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Principal : Service : ec2.amazonaws.com Action : sts:AssumeRole ManagedPolicyArns : - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/CloudWatchAgentServerPolicy' SSMLabProfile : Type : AWS::IAM::InstanceProfile Properties : Roles : - !Ref 'PsDscSSMLabRole' Path : / Outputs : LabBucketName : Description : S3 Bucket where MOF File will be uploaded to. Value : !Ref LabBucket InstanceProfile : Value : !Ref SSMLabProfile InstanceRoleArn : Value : !GetAtt SSMLabRole.Arn InstanceRoleName : Value : !Ref SSMLabRole","title":"Resources"},{"location":"workshops/PowerShellDSCinAWS/","text":"Managing Windows Workloads at Scale with PowerShell DSC and AWS Systems Manager This lab will demonstrate one way to use PowerShell Desired State Configuration with AWS Systems Manager (SSM) to configure Windows workloads and maintain compliance. It's goal is to share a pattern and concepts that you can utilize within your own AWS Environment. In this lab we will do the following: Generate a MOF File Create an AWS Systems Manager Association Use AWS Systems Manager Parameter Store To provide Config Data to our MOF Configuration To provide latest AMI IDs to deploy instances with CloudFormation and CLI Verify the configuration of our EC2 Instances Check and update complaince information Step 1 - Deploy Lab Pre-Req Components Click here To Deploy Lab into your Account This CloudFormation Template will deploy the following resources: S3 Bucket to store MOF Files SSM Parameter for Logon Message IAM Instance Role that allows Instances to use the AWS Systems Manager Service Please examine the the cloudformation template below. Once deployed go to the Output section and note the MofBucketName and InstanceProfile, we will need this info for later steps. AWSTemplateFormatVersion : \"2010-09-09\" Description : This is a Cloudformation that setups Lab Components for PowerShell DSC Lab Resources : LabBucket : Type : AWS::S3::Bucket DeletionPolicy : Delete LogonMessageParam : Type : AWS::SSM::Parameter Properties : Description : Logon Message for Interactive Logon Name : LogonMessage Type : String Value : \"'This is a Test System.,Testing how to Set a Logon Message with.,PowerShell DSC and AWS Systems Manager.,Parameter Store'\" SSMLabRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject Resource : - !Sub 'arn:aws:s3:::aws-ssm-${AWS::Region}/*' - !Sub 'arn:aws:s3:::aws-windows-downloads-${AWS::Region}/*' - !Sub 'arn:aws:s3:::amazon-ssm-${AWS::Region}/*' - !Sub 'arn:aws:s3:::amazon-ssm-packages-${AWS::Region}/*' - !Sub 'arn:aws:s3:::${AWS::Region}-birdwatcher-prod/*' - !Sub 'arn:aws:s3:::patch-baseline-snapshot-${AWS::Region}/*' Effect : Allow PolicyName : ssm-custom-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Action : - ssm:GetParameter Resource : - !Sub 'arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${LogonMessageParam}' Effect : Allow PolicyName : ssm-param-policy - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject - s3:PutObject - s3:PutObjectAcl Resource : - !Sub 'arn:${AWS::Partition}:s3:::${LabBucket}/*' Effect : Allow PolicyName : ssm-bucket-policy AssumeRolePolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Principal : Service : ec2.amazonaws.com Action : sts:AssumeRole ManagedPolicyArns : - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/CloudWatchAgentServerPolicy' SSMLabProfile : Type : AWS::IAM::InstanceProfile Properties : Roles : - !Ref 'PsDscSSMLabRole' Path : / Outputs : LabBucketName : Description : S3 Bucket where MOF File will be uploaded to. Value : !Ref LabBucket InstanceProfile : Value : !Ref SSMLabProfile InstanceRoleArn : Value : !GetAtt SSMLabRole.Arn InstanceRoleName : Value : !Ref SSMLabRole Step 2 - Create Powershell Script that generates MOF Files Copy the script below and run it on your local machine. This will generate a MOF File in C:\\MofFiles directory and is named localhost.mof, we will rename it to winworkshop.mof. This MOF File will configure the following items: Sets the Interative Logon Message Installs IIS Web Services Before running the script be sure to install the DSC modules on the machine you execute the script. Below are the commands that will install these from the PowerShell Gallery. There are many DSC Resources that can be used to perform things like Domain Join, or ensure local administrator groups have specific AD Users or Groups. Install-Module -Name PSDscResources Install-Module -Name SecurityPolicyDsc Take Note of the use of SSM Parameter Tokens {ssm:LogonMessage} in the script. In this lab we will be using the AWS-ApplyDSCMofs command document. When using this command document we can use tokens in the MOF file to grab config data from AWS Systems Manager Parameter Store, information from Tags assigned to the instance or a combination of both. In this case we are pulling the Logon Message string from SSM Parameter Store. This script will grab the Logon Message text from the parameter deployed in the Cloudformation template. [ CmdletBinding ()] param () $ConfigurationData = @{ AllNodes = @( @{ NodeName = \"*\" PSDscAllowPlainTextPassword = $true PSDscAllowDomainUser = $true }, @{ NodeName = 'localhost' } ) } # PowerShell DSC Configuration Function configuration ssmlab { # Importing DSC Resource used in the Configuration Import-DscResource -ModuleName PSDscResources Import-DscResource -ModuleName SecurityPolicyDsc Node 'localhost' { # Configures the Interactive Logon Message SecurityOption LogonMessage { Name = \"LogonMessage\" Interactive_logon_Message_title_for_users_attempting_to_log_on = 'Logon policy From SSM' Interactive_logon_Message_text_for_users_attempting_to_log_on = '{ssm:LogonMessage}' } # Installs IIS WindowsFeature WebServer { Ensure = \"Present\" Name = \"Web-Server\" } } } # Create the MOF File from the Configuration Function ssmlab -OutputPath 'C:\\MofFiles' -ConfigurationData $ConfigurationData Rename the MOF File to winworkshop.mof and upload it to the S3 Bucket that was created by the CloudFormation template, the name you can find in the output section of cloudformation. Step 3 - Create a State Manager Association using AWS-ApplyDSCMofs Document Now that we have our pre-reqs and our MOF File let's configure a State Manager Association. You can use the AWS-ApplyDSCMofs Document with Run Command or Automation. However, using it with state manager allows us to report on compliance regularly and allows us to automatically pick up and configure new instances to our desired state. Here we are demonstrating State Manager Association creation via the web console, but this can be done via SDK, CLI or CloudFormation. We will only be focusing on the parameters we will we be changing, or important to note when setting up an association for this document. First Let's navigate to the AWS Systems Manager Web Console, and Click on State Manager on the left side of the web page under Nodes and Instances. Once we are in the State Manager Console, we click on the Create Association Button We want to name the Association as well as select the AWS-ApplyDSCMof Document. We want to point the document to the location of our MOF Files, this can be a file location, HTTP Location or S3 Bucket. In this example we are going to use the MOF File we uploaded to our S3 Bucket. Note the way we are specifying the S3 Bucket, this naming style if unique to this document. Take notice of the *MOF Operation mode , here we are leaving it to apply which will apply configuration if our instance does not match the desired state. This can also be set to Report Only, which will not apply configuration but only report when an instance has drifted from a desired state. Next we will review the PS Gallery Setting and the Reboot Behavior. With the PS Gallery parameter set to true, the AWS-ApplyDSCMof document will automatically install the DSC modules specified in our MOF File. This combines what would be multiple steps into one. The Reboot Behavor setting has three option, After Mof, Immediately or Never. Reboot will occur if it is required for certain configurations such as joining a Domain. After MOF will apply all configurations in the MOF and the Reboot, Immediately will perform the reboot after applying the config that requires it, and Never will surpress reboot completely. Under Compliance Type we are going to create a Custom Compliance Type for this association. Targets are the instances we want to target with this association. We can manually select instances, select all instances managed in this region, or select by tag. In this case we want to target a specific Tag with the Name Build and the value of Base . In a subsquent step we will be deploying instance with these tag values and ensuring they meet our configuration. Now lets specify a schedule we want the Association to evaluate compliance. Since we are doing this during a workshop, lets set this to 30 Minutes. The compliance severity parameter controls what level we want the reflected for this association in the dashboard. Finally, we can specify an S3 Bucket to write all command output to S3, if we do not output in the console is truncated to 2500 characters. You can also leave this unchecked and click on Create Association. We will leave this association perculating and come back to it once we have deployed a few instances. Step 4 - Deploy a Windows EC2 Instances Let's test out what our MOF File and Association does. If you have the AWS Cli installed and configured you can copy the command below, other wise use the web console to launch an instance. In the command below substitute the InstanceProfile after --iam-instance-profile with the name in the output of PsDscSSMInstanceProfile in the CloudFormation we deployed in Step 1. If you deploy via the web console be sure to select the Instance Profile with PsDSC in the name from the drop down as demonstrated in the next screenshot. This will allow the instance to communicate with AWS Systems Manager service. Note the use of the of AWS Systems Parameter Store in the CLI command to get the latest AMI ID for Windows 2019. AWS Systems Manager provides these parameters so you can get the latest AMI IDs in every region. aws ec2 run-instances --image-id $(aws ssm get-parameters --names /aws/service/ami-windows-latest/Windows_Server-2019-English-Full-Base --query 'Parameters[0].[Value]' --output text) --count 1 --instance-type t3.large --iam-instance-profile InstanceProfile --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=WinWorkShop},{Key=Build,Value=Base}]' Once we have the Instance deployed lets go back to our Association in State Manager and click into it. Then click the resources tab, we can see that our instance was picked up by the association based on the Build tag. Once the association applies successfully, let's click into Compliance. When the association first applies, it reports that the instance is non-compliant and the applies the configuration. We will need to go back to the association, and click on Apply Association Now to run it manually before the next cron schedule. Once we do so and it completes we can go back to Compliance, and see that it is now reporting as compliant. Let's test this by RDPing to the Instance, we should see the following interactive message before we login. Then grab the IP or Public DNS Name of the instance and throw into a Web Browser and see the IIS Splash Screen. Here we demonstrated how we can Generate a MOF file, use it with AWS Systems Manager to configure instances and report on compliance. Next we will see how we can do this at scale with an auto-scaling group. Step 5 - Let's Try an Auto-Scaling Group Click here To Deploy a Windows Auto-Scaling Group into your Account Note the use of the of AWS Systems Parameter Store in the LatestAmiId Parameter and the Parameter type. You can use SSM Parameter Store integration with Cloudformation and not use AMI Mappings to get the latest AMI ID for Windows 2019. AWS Systems Manager provides these parameters so you can get the latest AMI IDs in every region. Please examine the the cloudformation template below. Once deployed test as you did in Step 5. AWSTemplateFormatVersion : '2010-09-09' Description : This Template Deploys a Simple Windows Auto-Scaling Group Parameters : KeyPairName : Description : Public/private key pairs allow you to securely connect to your instance after it launches Type : AWS::EC2::KeyPair::KeyName LatestAmiId : Type : AWS::SSM::Parameter::Value<AWS::EC2::Image::Id> Default : /aws/service/ami-windows-latest/Windows_Server-2019-English-Full-Base HostInstanceProfile : Description : Grab the String from the Output name PsDscSSMInstanceProfile Type : String NumberOfHosts : AllowedValues : - '1' - '2' - '3' - '4' Default : '2' Description : Enter the number of hosts to create Type : String PublicSubnet1ID : Description : ID of the public subnet 1 that you want to provision into (e.g., subnet-a0246dcd) Type : AWS::EC2::Subnet::Id PublicSubnet2ID : Description : ID of the public subnet 2 you want to provision into (e.g., subnet-e3246d8e) Type : AWS::EC2::Subnet::Id MyInstanceType : Description : Amazon EC2 instance type for the first Remote Desktop Gateway instance Type : String Default : t3.large AllowedValues : - t3.small - t3.medium - t3.large - m5.large RDPCIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/([0-9]|[1-2][0-9]|3[0-2]))$ Description : Allowed CIDR Block for external access to the Deployed Instances Type : String VPCID : Description : ID of the VPC (e.g., vpc-0343606e) Type : AWS::EC2::VPC::Id Rules : SubnetsInVPC : Assertions : - Assert : !EachMemberIn - !ValueOfAll - AWS::EC2::Subnet::Id - VpcId - !RefAll 'AWS::EC2::VPC::Id' AssertDescription : All subnets must in the VPC Resources : WinAutoScalingGroup : Type : AWS::AutoScaling::AutoScalingGroup Properties : LaunchConfigurationName : !Ref 'WinLaunchConfiguration' VPCZoneIdentifier : - !Ref 'PublicSubnet1ID' - !Ref 'PublicSubnet2ID' MinSize : !Ref 'NumberOfHosts' MaxSize : !Ref 'NumberOfHosts' Cooldown : '300' DesiredCapacity : !Ref 'NumberOfHosts' Tags : - Key : Name Value : WinWorkshop PropagateAtLaunch : 'true' - Key : Build Value : Base PropagateAtLaunch : 'true' WinLaunchConfiguration : Type : AWS::AutoScaling::LaunchConfiguration Properties : ImageId : !Ref 'LatestAmiId' SecurityGroups : - !Ref 'WinWorkshopSG' IamInstanceProfile : !Ref 'HostInstanceProfile' InstanceType : !Ref 'MyInstanceType' BlockDeviceMappings : - DeviceName : /dev/sda1 Ebs : VolumeSize : '50' VolumeType : gp2 KeyName : !Ref 'KeyPairName' WinWorkshopSG : Type : AWS::EC2::SecurityGroup Properties : GroupDescription : Enable RDP access from the Internet VpcId : !Ref 'VPCID' SecurityGroupIngress : - IpProtocol : tcp FromPort : '3389' ToPort : '3389' CidrIp : !Ref 'RDPCIDR' - IpProtocol : tcp FromPort : '80' ToPort : '80' CidrIp : !Ref 'RDPCIDR' - IpProtocol : tcp FromPort : '443' ToPort : '443' CidrIp : !Ref 'RDPCIDR' Once the Auto-Scaling Group is deployed using CloudFormation, lets run through the same steps as in Step 4, to review and test our configuration. Using this method we can configure Windows Workloads at scale, using tags, SSM State Manager, SSM Parameter Store and PowerShell DSC.","title":"**Managing Windows Workloads at Scale with PowerShell DSC and AWS Systems Manager**"},{"location":"workshops/PowerShellDSCinAWS/#managing-windows-workloads-at-scale-with-powershell-dsc-and-aws-systems-manager","text":"This lab will demonstrate one way to use PowerShell Desired State Configuration with AWS Systems Manager (SSM) to configure Windows workloads and maintain compliance. It's goal is to share a pattern and concepts that you can utilize within your own AWS Environment. In this lab we will do the following: Generate a MOF File Create an AWS Systems Manager Association Use AWS Systems Manager Parameter Store To provide Config Data to our MOF Configuration To provide latest AMI IDs to deploy instances with CloudFormation and CLI Verify the configuration of our EC2 Instances Check and update complaince information","title":"Managing Windows Workloads at Scale with PowerShell DSC and AWS Systems Manager"},{"location":"workshops/PowerShellDSCinAWS/#step-1-deploy-lab-pre-req-components","text":"Click here To Deploy Lab into your Account This CloudFormation Template will deploy the following resources: S3 Bucket to store MOF Files SSM Parameter for Logon Message IAM Instance Role that allows Instances to use the AWS Systems Manager Service Please examine the the cloudformation template below. Once deployed go to the Output section and note the MofBucketName and InstanceProfile, we will need this info for later steps. AWSTemplateFormatVersion : \"2010-09-09\" Description : This is a Cloudformation that setups Lab Components for PowerShell DSC Lab Resources : LabBucket : Type : AWS::S3::Bucket DeletionPolicy : Delete LogonMessageParam : Type : AWS::SSM::Parameter Properties : Description : Logon Message for Interactive Logon Name : LogonMessage Type : String Value : \"'This is a Test System.,Testing how to Set a Logon Message with.,PowerShell DSC and AWS Systems Manager.,Parameter Store'\" SSMLabRole : Type : AWS::IAM::Role Properties : Policies : - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject Resource : - !Sub 'arn:aws:s3:::aws-ssm-${AWS::Region}/*' - !Sub 'arn:aws:s3:::aws-windows-downloads-${AWS::Region}/*' - !Sub 'arn:aws:s3:::amazon-ssm-${AWS::Region}/*' - !Sub 'arn:aws:s3:::amazon-ssm-packages-${AWS::Region}/*' - !Sub 'arn:aws:s3:::${AWS::Region}-birdwatcher-prod/*' - !Sub 'arn:aws:s3:::patch-baseline-snapshot-${AWS::Region}/*' Effect : Allow PolicyName : ssm-custom-s3-policy - PolicyDocument : Version : '2012-10-17' Statement : - Action : - ssm:GetParameter Resource : - !Sub 'arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/${LogonMessageParam}' Effect : Allow PolicyName : ssm-param-policy - PolicyDocument : Version : '2012-10-17' Statement : - Action : - s3:GetObject - s3:PutObject - s3:PutObjectAcl Resource : - !Sub 'arn:${AWS::Partition}:s3:::${LabBucket}/*' Effect : Allow PolicyName : ssm-bucket-policy AssumeRolePolicyDocument : Version : '2012-10-17' Statement : - Effect : Allow Principal : Service : ec2.amazonaws.com Action : sts:AssumeRole ManagedPolicyArns : - !Sub 'arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore' - !Sub 'arn:${AWS::Partition}:iam::aws:policy/CloudWatchAgentServerPolicy' SSMLabProfile : Type : AWS::IAM::InstanceProfile Properties : Roles : - !Ref 'PsDscSSMLabRole' Path : / Outputs : LabBucketName : Description : S3 Bucket where MOF File will be uploaded to. Value : !Ref LabBucket InstanceProfile : Value : !Ref SSMLabProfile InstanceRoleArn : Value : !GetAtt SSMLabRole.Arn InstanceRoleName : Value : !Ref SSMLabRole","title":"Step 1 - Deploy Lab Pre-Req Components"},{"location":"workshops/PowerShellDSCinAWS/#step-2-create-powershell-script-that-generates-mof-files","text":"Copy the script below and run it on your local machine. This will generate a MOF File in C:\\MofFiles directory and is named localhost.mof, we will rename it to winworkshop.mof. This MOF File will configure the following items: Sets the Interative Logon Message Installs IIS Web Services Before running the script be sure to install the DSC modules on the machine you execute the script. Below are the commands that will install these from the PowerShell Gallery. There are many DSC Resources that can be used to perform things like Domain Join, or ensure local administrator groups have specific AD Users or Groups. Install-Module -Name PSDscResources Install-Module -Name SecurityPolicyDsc Take Note of the use of SSM Parameter Tokens {ssm:LogonMessage} in the script. In this lab we will be using the AWS-ApplyDSCMofs command document. When using this command document we can use tokens in the MOF file to grab config data from AWS Systems Manager Parameter Store, information from Tags assigned to the instance or a combination of both. In this case we are pulling the Logon Message string from SSM Parameter Store. This script will grab the Logon Message text from the parameter deployed in the Cloudformation template. [ CmdletBinding ()] param () $ConfigurationData = @{ AllNodes = @( @{ NodeName = \"*\" PSDscAllowPlainTextPassword = $true PSDscAllowDomainUser = $true }, @{ NodeName = 'localhost' } ) } # PowerShell DSC Configuration Function configuration ssmlab { # Importing DSC Resource used in the Configuration Import-DscResource -ModuleName PSDscResources Import-DscResource -ModuleName SecurityPolicyDsc Node 'localhost' { # Configures the Interactive Logon Message SecurityOption LogonMessage { Name = \"LogonMessage\" Interactive_logon_Message_title_for_users_attempting_to_log_on = 'Logon policy From SSM' Interactive_logon_Message_text_for_users_attempting_to_log_on = '{ssm:LogonMessage}' } # Installs IIS WindowsFeature WebServer { Ensure = \"Present\" Name = \"Web-Server\" } } } # Create the MOF File from the Configuration Function ssmlab -OutputPath 'C:\\MofFiles' -ConfigurationData $ConfigurationData Rename the MOF File to winworkshop.mof and upload it to the S3 Bucket that was created by the CloudFormation template, the name you can find in the output section of cloudformation.","title":"Step 2 - Create Powershell Script that generates MOF Files"},{"location":"workshops/PowerShellDSCinAWS/#step-3-create-a-state-manager-association-using-aws-applydscmofs-document","text":"Now that we have our pre-reqs and our MOF File let's configure a State Manager Association. You can use the AWS-ApplyDSCMofs Document with Run Command or Automation. However, using it with state manager allows us to report on compliance regularly and allows us to automatically pick up and configure new instances to our desired state. Here we are demonstrating State Manager Association creation via the web console, but this can be done via SDK, CLI or CloudFormation. We will only be focusing on the parameters we will we be changing, or important to note when setting up an association for this document. First Let's navigate to the AWS Systems Manager Web Console, and Click on State Manager on the left side of the web page under Nodes and Instances. Once we are in the State Manager Console, we click on the Create Association Button We want to name the Association as well as select the AWS-ApplyDSCMof Document. We want to point the document to the location of our MOF Files, this can be a file location, HTTP Location or S3 Bucket. In this example we are going to use the MOF File we uploaded to our S3 Bucket. Note the way we are specifying the S3 Bucket, this naming style if unique to this document. Take notice of the *MOF Operation mode , here we are leaving it to apply which will apply configuration if our instance does not match the desired state. This can also be set to Report Only, which will not apply configuration but only report when an instance has drifted from a desired state. Next we will review the PS Gallery Setting and the Reboot Behavior. With the PS Gallery parameter set to true, the AWS-ApplyDSCMof document will automatically install the DSC modules specified in our MOF File. This combines what would be multiple steps into one. The Reboot Behavor setting has three option, After Mof, Immediately or Never. Reboot will occur if it is required for certain configurations such as joining a Domain. After MOF will apply all configurations in the MOF and the Reboot, Immediately will perform the reboot after applying the config that requires it, and Never will surpress reboot completely. Under Compliance Type we are going to create a Custom Compliance Type for this association. Targets are the instances we want to target with this association. We can manually select instances, select all instances managed in this region, or select by tag. In this case we want to target a specific Tag with the Name Build and the value of Base . In a subsquent step we will be deploying instance with these tag values and ensuring they meet our configuration. Now lets specify a schedule we want the Association to evaluate compliance. Since we are doing this during a workshop, lets set this to 30 Minutes. The compliance severity parameter controls what level we want the reflected for this association in the dashboard. Finally, we can specify an S3 Bucket to write all command output to S3, if we do not output in the console is truncated to 2500 characters. You can also leave this unchecked and click on Create Association. We will leave this association perculating and come back to it once we have deployed a few instances.","title":"Step 3 - Create a State Manager Association using AWS-ApplyDSCMofs Document"},{"location":"workshops/PowerShellDSCinAWS/#step-4-deploy-a-windows-ec2-instances","text":"Let's test out what our MOF File and Association does. If you have the AWS Cli installed and configured you can copy the command below, other wise use the web console to launch an instance. In the command below substitute the InstanceProfile after --iam-instance-profile with the name in the output of PsDscSSMInstanceProfile in the CloudFormation we deployed in Step 1. If you deploy via the web console be sure to select the Instance Profile with PsDSC in the name from the drop down as demonstrated in the next screenshot. This will allow the instance to communicate with AWS Systems Manager service. Note the use of the of AWS Systems Parameter Store in the CLI command to get the latest AMI ID for Windows 2019. AWS Systems Manager provides these parameters so you can get the latest AMI IDs in every region. aws ec2 run-instances --image-id $(aws ssm get-parameters --names /aws/service/ami-windows-latest/Windows_Server-2019-English-Full-Base --query 'Parameters[0].[Value]' --output text) --count 1 --instance-type t3.large --iam-instance-profile InstanceProfile --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=WinWorkShop},{Key=Build,Value=Base}]' Once we have the Instance deployed lets go back to our Association in State Manager and click into it. Then click the resources tab, we can see that our instance was picked up by the association based on the Build tag. Once the association applies successfully, let's click into Compliance. When the association first applies, it reports that the instance is non-compliant and the applies the configuration. We will need to go back to the association, and click on Apply Association Now to run it manually before the next cron schedule. Once we do so and it completes we can go back to Compliance, and see that it is now reporting as compliant. Let's test this by RDPing to the Instance, we should see the following interactive message before we login. Then grab the IP or Public DNS Name of the instance and throw into a Web Browser and see the IIS Splash Screen. Here we demonstrated how we can Generate a MOF file, use it with AWS Systems Manager to configure instances and report on compliance. Next we will see how we can do this at scale with an auto-scaling group.","title":"Step 4 - Deploy a Windows EC2 Instances"},{"location":"workshops/PowerShellDSCinAWS/#step-5-lets-try-an-auto-scaling-group","text":"Click here To Deploy a Windows Auto-Scaling Group into your Account Note the use of the of AWS Systems Parameter Store in the LatestAmiId Parameter and the Parameter type. You can use SSM Parameter Store integration with Cloudformation and not use AMI Mappings to get the latest AMI ID for Windows 2019. AWS Systems Manager provides these parameters so you can get the latest AMI IDs in every region. Please examine the the cloudformation template below. Once deployed test as you did in Step 5. AWSTemplateFormatVersion : '2010-09-09' Description : This Template Deploys a Simple Windows Auto-Scaling Group Parameters : KeyPairName : Description : Public/private key pairs allow you to securely connect to your instance after it launches Type : AWS::EC2::KeyPair::KeyName LatestAmiId : Type : AWS::SSM::Parameter::Value<AWS::EC2::Image::Id> Default : /aws/service/ami-windows-latest/Windows_Server-2019-English-Full-Base HostInstanceProfile : Description : Grab the String from the Output name PsDscSSMInstanceProfile Type : String NumberOfHosts : AllowedValues : - '1' - '2' - '3' - '4' Default : '2' Description : Enter the number of hosts to create Type : String PublicSubnet1ID : Description : ID of the public subnet 1 that you want to provision into (e.g., subnet-a0246dcd) Type : AWS::EC2::Subnet::Id PublicSubnet2ID : Description : ID of the public subnet 2 you want to provision into (e.g., subnet-e3246d8e) Type : AWS::EC2::Subnet::Id MyInstanceType : Description : Amazon EC2 instance type for the first Remote Desktop Gateway instance Type : String Default : t3.large AllowedValues : - t3.small - t3.medium - t3.large - m5.large RDPCIDR : AllowedPattern : ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\\/([0-9]|[1-2][0-9]|3[0-2]))$ Description : Allowed CIDR Block for external access to the Deployed Instances Type : String VPCID : Description : ID of the VPC (e.g., vpc-0343606e) Type : AWS::EC2::VPC::Id Rules : SubnetsInVPC : Assertions : - Assert : !EachMemberIn - !ValueOfAll - AWS::EC2::Subnet::Id - VpcId - !RefAll 'AWS::EC2::VPC::Id' AssertDescription : All subnets must in the VPC Resources : WinAutoScalingGroup : Type : AWS::AutoScaling::AutoScalingGroup Properties : LaunchConfigurationName : !Ref 'WinLaunchConfiguration' VPCZoneIdentifier : - !Ref 'PublicSubnet1ID' - !Ref 'PublicSubnet2ID' MinSize : !Ref 'NumberOfHosts' MaxSize : !Ref 'NumberOfHosts' Cooldown : '300' DesiredCapacity : !Ref 'NumberOfHosts' Tags : - Key : Name Value : WinWorkshop PropagateAtLaunch : 'true' - Key : Build Value : Base PropagateAtLaunch : 'true' WinLaunchConfiguration : Type : AWS::AutoScaling::LaunchConfiguration Properties : ImageId : !Ref 'LatestAmiId' SecurityGroups : - !Ref 'WinWorkshopSG' IamInstanceProfile : !Ref 'HostInstanceProfile' InstanceType : !Ref 'MyInstanceType' BlockDeviceMappings : - DeviceName : /dev/sda1 Ebs : VolumeSize : '50' VolumeType : gp2 KeyName : !Ref 'KeyPairName' WinWorkshopSG : Type : AWS::EC2::SecurityGroup Properties : GroupDescription : Enable RDP access from the Internet VpcId : !Ref 'VPCID' SecurityGroupIngress : - IpProtocol : tcp FromPort : '3389' ToPort : '3389' CidrIp : !Ref 'RDPCIDR' - IpProtocol : tcp FromPort : '80' ToPort : '80' CidrIp : !Ref 'RDPCIDR' - IpProtocol : tcp FromPort : '443' ToPort : '443' CidrIp : !Ref 'RDPCIDR' Once the Auto-Scaling Group is deployed using CloudFormation, lets run through the same steps as in Step 4, to review and test our configuration. Using this method we can configure Windows Workloads at scale, using tags, SSM State Manager, SSM Parameter Store and PowerShell DSC.","title":"Step 5 - Let's Try an Auto-Scaling Group"}]}